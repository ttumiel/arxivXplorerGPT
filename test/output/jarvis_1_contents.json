{
  "title": null,
  "content": "\n\n  \n\n           \n\n numbers, compressnatbib \n\n    \n\n   \n\n   \n\n                     \n\n \n\n    \n\n          \n\n       \n\n  \n\n \n\n         \n\nlet@token onedot  onedot let@token .. e.g E.g i.e I.e c.f C.f etc vs w.r.t d.o.f et al \n\nllms[LLMs]Large Language Models mlms[MLMs]Multimodal Language Models  \n\n       \n\n \n\n  \n\nJARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models \n\nNovember 17, 2023 [ JARVIS-1: Open-world Multi-task Agents \n with Memory-Augmented Multimodal Language Models \n\n\n   Zihao Wang^1,2, Shaofei Cai^1,2, Anji Liu^3, Yonggang Jin^4, Jinbing Hou^4, Bowei Zhang^5, Haowei Lin^1,2, \n Zhaofeng He^4, Zilong Zheng^6, Yaodong Yang^1, Xiaojian Ma^6^†, Yitao Liang^1^†  \n\n\n Team CraftJarvis \n \n\n^1Institute for Artificial Intelligence, Peking University\n ^2School of Intelligence Science and Technology, Peking University \n ^3Computer Science Department, University of California, Los Angeles \n ^4Beijing University of Posts and Telecommunications \n ^5School of Electronics Engineering and Computer Science, Peking University \n ^6Beijing Institute for General Artificial Intelligence (BIGAI) \n \n\n\n \n\n{ zhwang,caishaofei} @stu.pku.edu.cn,liuanji@cs.ucla.edu,xiaojian.ma@ucla.edu,yitaol@pku.edu.cn \n\n\n craftjarvis-jarvis1.github.io \n\n\n \n\n21.680999999999997pt\n\n  Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. In our experiments, JARVIS-1exhibits nearly perfect performances across over 200 varying tasks from the Minecraft Universe Benchmark, ranging from entry to intermediate levels. JARVIS-1has achieved a completion rate of 12.5%  in the long-horizon diamond pickaxe task. This represents a significant increase up to 5 times compared to previous records. Furthermore, we show that JARVIS-1is able to self-improve following a life-long learning paradigm thanks to multimodal memory, sparking a more general intelligence and improved autonomy.  \n\nOpen-World, Foundation Agents, Minecraft, Multimodal Language Model \n\n28.908pt\n\n] † Corresponding author(s) \n\nIntroduction\n============\n<figure. fig:tech_tree -  How does JARVIS-1unlonk the technology tree of the Minecraft universe. JARVIS-1can consistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond, redstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1not only outperforms the previous state-of-the-art VPT  <cit. vpt> (6%  vs. 2.5%  reliablity) on diamond pickaxe, but also can craft almost all diamond items in overworld including diamond chestplate.>Creating sophisticated agents that can accomplish myriad of tasks in complex domains remains a pivotal milestone towards generally capable artificial intelligence  <cit. gato,gpt3,flamingo,rt1,jxma_vl1>. Recent advancements have shown a trend towards employing a hierarchical goal execution architecture  <cit. deps,innermonologue,huang2022language>, and leveraging large language models (LLMs) as the high-level planner to generate action plans that will be ultimately executed by low-level instruction-following controllers. Albeit the fruitful progress they have yielded in many robotics  <cit. innermonologue> and even open-world environments like Minecraft  <cit. minedojo,minerl>, today’s agents built with these approaches are still struggling with three major issues: 1) perceive the world from multimodal sensory observations, such as images, videos in addition to natural language instructions and feedback for planning; This is mostly due to the inability of LLM-based planners on processing multimodal data  <cit. huang2022language,react>; 2) perform consistent and accurate long-term planning. This requires multi-round, knowledge, and reasoning-intensive dialogues, which remain great challenges to LLMs  <cit. innermonologue>; 3) learn and evolve in a life-long fashion. This calls out the need for agents to propose their own tasks and self-improve. Addressing these issues will unleash the full planning potential of LLM-based agents, and expedite the development of more generalist agents. In this work, we introduce JARVIS-1, a brand new agent that can robustly produce plans for long-horizon tasks from multimodal user and environment inputs, and translate them into motor control in Minecraft, a popular yet challenging open-world testbed for generalist agents. To be specific, we chain a multimodal foundation model MineCLIP   <cit. minedojo> and an LLM   <cit. gpt3> together, the resulting multimodal language model (MLM) allows our agent to better understand the tasks, situations, and environmental feedback. To further enhance the correctness and consistency of planning, especially on long-horizon tasks, we propose to augment the agent with a multimodal memory, which stores both the scenarios and actual plans of the successful planning experiences in the past. By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be strengthened from the agent’s own interactions with the environment in an in-context manner. Finally, JARVIS-1is able to evolve throughout the gameplay by proposing tasks on its own (i.elet@token .self-instruct) as a means of exploration and saving the obtained experiences in the multimodal memory, therefore facilitating better reasoning and planning. This self-improving ability sparks its potential for a higher level of autonomy. Our main evaluations are conducted in Minecraft, with more than 200 tasks selected from the Minecraft Universe Benchmark  <cit. mcu>, with no demonstration provided. The tasks cover a broad spectrum from the early game (e.glet@token .ObtainCraftingTable) to intermediate and even challenging long-horizon tasks (e.glet@token .ObtainDiamondPickaxe). A glimpse of what JARVIS-1is able to achieve can be found in Figure  <ref. fig:tech_tree>. JARVIS-1exhibits strong performances on these tasks, representing an up to 5× increase to the previous records. Our ablative analysis then offers a detailed account of how JARVIS-1approaches this significant progress and becomes the first agent that can robustly obtain the diamond pickaxe with up to 12.5%  success rate. What is even more surprising is that, without the need for additional training, JARVIS-1demonstrates a continuous increase in performance as game time increases in long-horizon tasks. Moreover, JARVIS-1has demonstrated its potential of self-improve in an exploratory life-long learning experiment, where it needs to propose tasks to progressively explore the world, collect experiences, and sharpen its planning skill using these experiences stored in the multimodal memory. <figure. fig:challenge - Challenges in open-world environments and how does JARVIS-1tackle them. (Left) With situation-aware planning, JARVIS-1substantially improves the success rate on the challenging  task, compared to the baseline (GPT) without it. Note: Due to resource constraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases (STONE→IRON→DIAMOND), JARVIS-1exhibits more significant advantages thanks to interactive planning; (Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences on other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1can utilize prior experiences on relevant tasks for better planning. >In summary, JARVIS-1pilots the effort towards a human-like multi-task and autonomous agent in an open-world, embodied environment like Minecraft. We would like to share the key takeaways of what we have learned during its development as follows:  \n  - From LLMs to MLMs.    The capability of perceiving multimodal sensory input is critical to planning in a dynamic and open-world world. JARVIS-1enables this by chaining a multimodal foundation model together with an LLM. Compared to LLM “blindly” produces plans, MLM is able to natively understand the current situation and plan accordingly. Further, rich environmental feedback can be obtained through multimodal perception, therefore helping the self-check and self-explain of the planner spot and fix possible bugs in the plans, enabling stronger interactive planning. \n\n- Multimodal memory.    Early research has suggested the crucial role that memory mechanisms can serve in the functioning of generalist agents. By outfitting JARVIS-1with a multimodal memory, we effectively allow it to plan with both pretrained knowledge and its actual experiences in the world, therefore bringing significant improvement to planning correctness and consistency. Compared to canonical RL or planning agents with exploration, no additional model update is needed as the MLM in JARVIS-1makes it possible to leverage these experiences in an in-context manner. \n\n- Self-instruct and self-improve.    A sign of generalist agents is the capacity to proactively acquire new experiences and continuously improve themselves. We have demonstrated how JARVIS-1effectively traverses the environment by executing tasks autonomously generated through its self-instruct mechanism. With multimodal memory teaming up with experiences from the explorations, we have observed consistent improvement, especially in accomplishing more complicated tasks. Ultimately, this aspect of autonomous learning in JARVIS-1signifies an evolutionary step towards generalist agents that can learn, adapt, and improve over time with minimal external intervention. \n\n<figure. fig:pipeline -  Architecture of JARVIS-1and its self-improving mechanism. (a) JARVIS-1comprises a memory-augmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1also utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1can strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a growing memory that helps with better planning on tasks that has been (partially) visited before. >\n\nChallenges for Agents in open-world Environments\n================================================\nCompared to canonical scenarios with relatively small scale, simple dynamics, and limited tasks, open-world environments impose substantial challenges to building agents that can accomplish a diverse set of tasks  <cit. minedojo,minerl-rel-1,minerl-rel-2,minerl-rel-3,shaofei,deps,groot>. In this section, we will review three major challenges we’ve identified during the development of JARVIS-1. \n\nChallenge I: Situation-Aware Planning\n-------------------------------------\n<label. sec:challenge_sit_planning> In an open world, there could be various possible paths towards an open-world goal. However, not all of them are plausible or equally efficient given a certain situation (location, inventory status, etclet@token .). For example, building a bed \n    < g r a p h i c s >\n can be done through collecting wool from sheeps \n    < g r a p h i c s >\n, haunting spiders for strings \n    < g r a p h i c s >\n, or trading with villagers \n    < g r a p h i c s >\n. Depending on the current location and its proximity to these subjects, some options can be more viable and more efficient than others. Further, the agent’s own situation can also change throughout the episode, e.glet@token .day and night shifts, weather conditions (bringing different types of danger), tool usage (it can be broken). To this end, the plan needs to be constantly updated based off the current situation. Figure  <ref. fig:challenge> (left) shows that when attempting the \"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning without looking at the current situation, the agent failed to complete the task as opposed to human players and JARVIS-1, which perform situation-aware planning from time to time. We’ve observed that many failures coming from this were attributed to the agent’s inability to adapt to the changing situations including entering a new biome, the tool being used becoming broken, etclet@token .. \n\nChallenge II: Task Complexity\n-----------------------------\n<label. sec:challenge_task_complexity> The second challenge comes from the higher task complexity in open-world environments. Due to the richness of terrains, objects, and action space, tasks in open-world domains usually require substantially long planning horizons as well as good accuracy and precision. For example, the task ObtainEnchantingTable \n    < g r a p h i c s >\n includes more than 20 different sub-goals and therefore demand significantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved precisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with diamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks on crafting table; otherwise, the subsequent sub-goals won’t be executed due to unfulfilled preconditions. To tackle this, we may refer to some approaches in LLM reasoning, e.glet@token .self-debugging  <cit. selfdebug> and turning the planning into an interactive fashion. In <ref> (Middle), we’ve shown that as the complexity of the task increases, our JARVIS-1, which uses interactive planning  <cit. deps> to mitigate the aforementioned issues (details can be found in <ref>), elicits more significant advantages over the baseline (GPT) planner. \n\nChallenge III: Life-long Learning\n---------------------------------\n<label. sec:challenge3> Finally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult for an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this end, agents in an open world should be able to learn novel tasks while completing existing tasks, i.elet@token .life-long learning. Furthermore, as many open-world agents employ large models  <cit. deps,plan4mc,voyager,gitm>, canonical gradient-based learning could be extremely inefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1tackles this by adopting a memory to save all the experiences on past tasks. By retrieving memory entries relevant to the newly-coming task and putting them into the context as a reference, JARVIS-1is able to accumulate more experiences as the game continues and strengthen its own planning skills without gradient update. As illustrated in <ref> (Right), for instance, both ObtainDiamondPickaxe\n    < g r a p h i c s >\n and ObtainDiamondAxe\n    < g r a p h i c s >\n require gathering almost identical materials. Therefore, they can help each other by using the experiences from the other task. Compared to completing these challenging tasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1can bring significant advantages. \n\nMulti-task Agent with Memory-Augmented MLM\n==========================================\nThis section details the architecture of the proposed JARVIS-1agent. We begin with an overview of the modular agent design in <ref>. Next, we elaborate on how to implement an interactive planning scheme with a multimodal language model, which helps with more accurate plans, especially on complex and long-horizon tasks in <ref>. Finally, we show how to augment this planning framework with a multimodal memory to allow JARVIS-1to strengthen its planning skill throughout the episode by in-context life-long learning in <ref> and <ref>. <figure. fig:planner -  Interactive planning in JARVIS-1. After receiving the current task instruction and observation, JARVIS-1will produce an initial plan, which will go through self-check to get possible bugs (marked in colorred) fixed. Further, in case any error (also marked in colorred) occurs during the execution of the refined plan, JARVIS-1will try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check and self-explain significantly boosts the correctness and robustness of JARVIS-1planning. >\n\nOverview\n--------\n<label. sec:overview> We aim to develop an agent capable of solving long-horizon instruction-following tasks using image observations and human-aligned actions. To accomplish this, we propose a multi-modal agent including an interactive planner, a goal-conditioned controller, and a multimodal memory of multimodal experiences. Upon receiving a task and the current observation, JARVIS-1first utilizes the MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences from the memory. These experiences will then be used along with the planning instruction to prompt the MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference plans, the planner will ultimately produce a series of K short-horizon goals g_1,… ,g_K to be executed by the controller. Once the plan is successfully executed, it will be stored in the memory along with the task and the agent situation when it was planned. We also empower JARVIS-1with life-long learning by combining self-instruct, where JARVIS-1will propose some tasks for itself to complete as a means of exploration; and self-improve, where multiple JARVIS-1agents will be running in parallel to gather experiences, therefore helping with better planning later. We provide an illustration in <ref>. \n\nInteractive Planning with MLM\n-----------------------------\n<label. sec:interactive_planning> As we have mentioned in <ref> and <ref>, the primary challenges for planning in Minecraft come from the requirement of being able to plan for long-horizon tasks under dynamic observations. Confirmed by many prior arts  <cit. deps,voyager,plan4mc>, this makes it exceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this end, we take a multimodal language model (MLM) as zero-shot planner and combine it with an interactive planning framework to tackle these challenges. <figure. fig:memory_retrieval - Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1will first think backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited depth. The sub-goal that is present in the memory will join the current visual observation to form the final query. Entries that match the text query will be ranked by the perceiving distance of their states to the obs query and only the top entry of each sub-goal will be retrieved. >Situation-aware planning with MLM.    To achieve situation-aware planning, the planner must take the current observation into account, in addition to the task instruction  <cit. huang2022language,react>. Specifically, we begin with translating the multimodal observation into text descriptions. As opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft items (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT  <cit. gpt3> to generate sentences that describe these observations. For example, a generated sentence could be \"I can see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according to current visual observation during planning. Additional situation details including biome and inventory status are also converted into text using templates. Finally, we prompt the MLM again (the language part only) into a plan given the task instruction and all the aforementioned textual situation descriptions. Compared to end-to-end alternatives  <cit. brohan2023rt,leo>, we find our composable usage of MLM provides higher quality situation descriptions and ultimately, plans with much less hallucination. Planning with self-check.    Our first layer of shield to ensure the correctness of plans involves self-check. Similar to self-debugging   <cit. selfdebug>, given an initial plan, we ask JARVIS-1to progressively simulate the plan execution, predict the resulting state after each step (primarily the state of inventory), and evaluate them. By verifying if these states satisfy the goal’s precondition, JARVIS-1can proactively identify potential plan flaws. Compared to the canonical planner where the agent has to encounter the error first before making a remedy, this upfront plan verification could mitigate the need for the agent to recover (re-plan) from more challenging situations due to plan failure. For instance, if an agent starts digging underground without sufficient wood, it would typically have to return to the surface, which substantially lowers the chance of completing the task. Planning with environment feedback.    Next, our interactive planning framework ventures into allowing JARVIS-1to quickly recover from failure by leveraging environment feedback in a closed-loop fashion. The process is illustrated in <ref>. During plan execution, we feed the feedback to the MLM of JARVIS-1in case there is any execution failure (possibly due to a flawed plan) and utilize its self-explain mechanism  <cit. reflexion> to explain the error and locate the bugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1will produce an improved plan based on both the outside environment feedback and the inside retrospective. Compared to other agents that rely on human intervention or privileged environment information  <cit. innermonologue,gitm>, JARVIS-1has the ability to speculate about the reasons why current goals cannot be achieved, without the need for additional information or design. \n\nPlanning with Multimodal Memory in the Loop\n-------------------------------------------\n<label. sec:planning_with_memory> To address the life-long learning challenge mentioned in <ref>, we equip JARVIS-1with multimodal memory to allow learning from its own past experiences. We will detail the formulation of the retrieval-augmented planning, query generation, and memory layout below. Retrival-augmented planning. Retrieval-augmented generation (RAG)  <cit. lewis2020retrieval,mao2020generation> enhances the quality of responses generated by LLMs by incorporating external sources of knowledge to complement the model’s internal representation. We also utilize RAG to enhance JARVIS-1’s long-term planning capability. Compared to official RAG methods leveraging the external knowledge library, we take the collected multimodal memory as the knowledge library and retrieve the interactive experiences as the demonstration prompt to augment the planning results. The formulation is as follows: \n    p(y | x) ≈∑ _z∈top-k(p(·| x))p_η (z | x) p_θ (y | x,z),\n where x, y, and z denote instruction, plans, and retrieved memory entries respectively, and p_η and p_θ are denoted as retrieval and planning models. Such retrieval-augmented planning method helps JARVIS-1ground the internal knowledge into the open-ended environments efficiently and leverage the historical interaction feedback to solve the hallucination within LLMs and produce more accurate plans. Multimodal memory.    We have demonstrated the layout of our multimodal memory on the right side of Figure  <ref. fig:memory_retrieval>. From a high level, it is a key-value memory where the keys are multimodal, comprising both the task and the observation (or situation) made when this memory entry was created. The values are the plans that were successfully executed. Note that, since the plans in an open-world environment like Minecraft are situated (see <ref>), there could be multiple entries that are with the same task but different observations and plans. As a result, JARVIS-1needs to produce multimodal queries based on the current task and situations to retrieve the relevant memory entries. Query generation via reasoning.     When presented with an instruction as a task, we employ query generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks, which will then be used as textual queries to retrieve relevant planning experiences as references for solving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty inventory\" as shown in Figure  <ref. fig:memory_retrieval>. JARVIS-1queries the MLMs to identify the tasks that are required for achieving the main task in a backward search fashion, e.g., “obtain book \n    < g r a p h i c s >\n/diamond \n    < g r a p h i c s >\n/obsidian \n    < g r a p h i c s >\n with empty inventory”. The search depth is bounded for efficiency. Further, instead of relying solely on retrieval based on the text query  <cit. voyager,gitm>, we also propose to append the agent’s current visual observation to the textual query, resulting in a multimodal query to take the situation into account during memory retrieval. Multimodal retrieval. After obtaining the textual and visual query, we compute the alignment between the query and each trajectory in multimodal memory. We first use the text encoder of CLIP model to compute the embedding of the query and task key of each entry in memory. We select the memory entries with similarity higher than the confidence threshold as the candidate entries. Then we will compute the visual state embedding of query and states in candidate entires. Then we sort the candidate entries with the visual embedding similarities, which can be formed as: \n    p_η (z| x) ∝   CLIP_v(s_z)^⊤CLIP_v(s_x),\n where s_z and s_x are the visual key of memory entries and visual query, respectively. Finally, we retrieve the plan of top-k candidate entries as reference prompt z.  Characteristics of 11 task groups encompassing over 200 minecraft tasks. <label. tab:mc_task_set> 0.950.0pt!   Group Task Num.Max.StepsInitial Inventory Biome  Language Instruction Wood  34  12k  null  Plains/Forest  Pick up a wooden_ pickaxe. Wood-Variants  43  12k  null Savanna/Jungle/Taiga Pick up a acacia_ boat. Stone  10  12k  iron_ axe  Plains/Forest  Craft a furnace given an iron axe. Iron  22  12k  iron_ axe  Plains/Forest  Smelt and craft an iron_ door given an iron axe. Gold  9  36k  iron_ axe  Plains/Forest  Smelt and craft an golden_ axe given an iron axe. Diamond  7  36k  iron_ axe  Plains/Forest Dig down to mine diamond and craft diamond_ pickaxe.Redstone  7  36k  iron_ axe  Plains/Forest  Mine redstone and make dropper given an iron axe. Blocks  15  12-36k  iron_ axe  Plains/Forest  Dig down to mine lapis_ lazuli block. Armor  17  12-36k  iron_ axe  Plains/Forest  Craft diamond_ boots given an iron axe and equip it. Decoration  17  12k  iron_ axe  Flower Forest  Obtain the bed and dye it red. Food  9  12k  iron_ axe  Plains Kill sheep to obtain mutton and cook it. \n\nSelf-improving Agents\n---------------------\n<label. sec:learning> Learning in Minecraft with memory.    The remaining issue now is where the aforementioned multimodal memory comes from. Inspired by the life-long learning scheme in many close-world and open-world reinforcement learning problems  <cit. lifelong1,lifelong2,voyager>, we propose the following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set of tasks, which form some curricula for the agents to complete as means of exploration of the world. During this process, JARVIS-1produces plans, interacts with the environment, embraces the errors, and stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1on various tasks. Therefore, JARVIS-1is able to produce better plans with the memory teaming up with the planning experiences. In our experiments, we use this as the default setting for all tasks. Exploration using self-instruct.    The key issue to the success of learning with memory is how to effectively acquire useful experiences given a limited amount of time. We propose to use self-instruct  <cit. selfinstruct> to generate the dynamic curriculum and guide JARVIS-1to learn from the interactions with environments. In each round, we prompt the MLM to consider how capable JARVIS-1is at this point and subsequently select tasks from a task pool to explore. We find that the curriculum almost follows the technical tree-growing direction. To accelerate the learning process, we augment the linear self-instruct to distributed learning in distributed environments with shared memory, i.elet@token .speculative execution  <cit. leviathan2023fast>. Specifically, we generate multiple executable tasks as candidate task batches and provide them to agents with the same memory for verification and execution in various different environments. Meanwhile, experiences are collected into a shared centralized memory. When all exploration tasks have been accomplished, we move to the next round, until the memory reaches a certain capacity. Life-long learning.    We’ve also observed that the aforementioned learning (where the memory is being filled) can be extended throughout the whole gameplay, where the agent gradually acquires more and more skills. As the gameplay continues, more and more experiences are pouring in, therefore JARVIS-1can find better references for challenging tasks like ObtainDiamondPickaxe, resulting in an improved success rate on these tasks. Further, there is no gradient update in this thanks to the memory-augmented MLM, i.elet@token .we can do in-context life-long learning. In Section  <ref. sec:long_horizon_experiments>, we offer exploratory experiments to show the potential of such capability of JARVIS-1.  Results of JARVIS-1and baselines on Minecraft. The detailed task instructions, settings and results can be found in the Appendix. <label. tab:main_results> 0.80.0pt!   Group  Task  GPT  ReAct  Inner Monologue  DEPS  JARVIS-1  \n    < g r a p h i c s >\n  26.67  45.00  36.67  75.00  91.55 -2*Wood  AVG  27.30±14.86  40.31±13.30  60.15±19.41  80.23±17.32  88.84±16.82  \n    < g r a p h i c s >\n  6.67  36.67  30.00  36.67  60.47 -2*WoodVar  AVG  [HTML]FFFFFF24.39±11.08  [HTML]FFFFFF38.13±12.81  [HTML]FFFFFF53.39±12.86  [HTML]FFFFFF68.75±12.32  76.78±12.27  \n    < g r a p h i c s >\n  20.00  20.00  66.67  75.00  94.20 -2*Stone  AVG  [HTML]FFFFFF20.21±12.32  [HTML]FFFFFF39.00±12.15  [HTML]FFFFFF52.86±16.90  [HTML]FFFFFF69.27±7.78  88.69±4.87  \n    < g r a p h i c s >\n  0.00  0.00  3.33  20.00  33.82  \n    < g r a p h i c s >\n  3.33  6.67  0.00  20.00  38.10 -3*Iron  AVG  [HTML]FFFFFF3.27±2.85  [HTML]FFFFFF4.61±3.63  [HTML]FFFFFF5.20±5.17  [HTML]FFFFFF16.92±4.69  34.63±10.61  \n    < g r a p h i c s >\n  0.00  2.00  2.00  6.00  14.49 -2*Gold  AVG  [HTML]FFFFFF0.00±0.00  [HTML]FFFFFF0.45±0.60  [HTML]FFFFFF0.59±0.64  [HTML]FFFFFF2.20±1.55  6.85±4.71  \n    < g r a p h i c s >\n  0.00  0.00  1.00  2.00  9.20  \n    < g r a p h i c s >\n  0.00  0.00  0.00  2.50  6.22 -3*Diamond  AVG  [HTML]FFFFFF0.00±0.00  [HTML]FFFFFF0.35±0.48  [HTML]FFFFFF0.96±0.67  [HTML]FFFFFF2.42±1.01  8.99±2.68  \n    < g r a p h i c s >\n  0.00  2.00  0.00  10.00  22.78 -2*Redstone  AVG  [HTML]FFFFFF1.04±1.30  [HTML]FFFFFF1.14±1.18  [HTML]FFFFFF0.69±1.68  [HTML]FFFFFF6.02±3.61  17.51±9.34  \n    < g r a p h i c s >\n  16.67  33.33  43.33  53.33  86.67 -2*Blocks  AVG  [HTML]FFFFFF45.64±33.88  [HTML]FFFFFF49.35±30.51  [HTML]FFFFFF55.71±29.43  [HTML]FFFFFF58.02±27.68  80.34±21.09  \n    < g r a p h i c s >\n  6.67  0.00  10.00  10.00  30.30 -2*Armor  AVG  [HTML]FFFFFF1.36±2.25  [HTML]FFFFFF0.50±0.88  [HTML]FFFFFF3.10±4.71  [HTML]FFFFFF3.71±3.78  13.44±14.62  \n    < g r a p h i c s >\n  15.00  15.00  15.00  25.00  50.00 -2*Decoration  AVG  [HTML]FFFFFF17.12±11.59  [HTML]FFFFFF17.13±9.19  [HTML]FFFFFF12.03±10.19  [HTML]FFFFFF29.59±15.94  46.67±23.39  \n    < g r a p h i c s >\n  13.33  16.67  25.00  16.67  43.55 -2*Food  AVG  [HTML]FFFFFF9.40±4.29  [HTML]FFFFFF15.56±6.83  [HTML]FFFFFF20.78±11.99  [HTML]FFFFFF22.85±8.15  46.75±11.16  \n\nExperiments\n===========\nIn the experiments, our goal is to 1) evaluate the general performances of JARVIS-1on the challenging Minecraft tasks, especially on its advantages over baselines that do not (fully) address the aforementioned issues in open-world agents; 2) understand the factors that contributes to the general results; 3) explore the potential of JARVIS-1in terms of life-long learning and its benefits to long-horizon tasks. To this end, we will first briefly introduce the evaluation settings, then cover the main comparative results and ablation studies, and conclude with an exploratory trial on long-horizon tasks. \n\nExperimental Setups\n-------------------\n<label. sec:task_setting> We evaluate JARVIS-1in Minecraft, with tasks selected from the recently introduced Minecraft Universe Benchmark  <cit. mcu>. For the reader’s convenience, we provide details on the basic setups below. Environment setting.    To ensure realistic gameplay, the agent needs to utilize observation and action spaces that are similar to those used by humans. Instead of manually designing a custom interface for models to interact with the environment, as done in previous methods such as MineDojo<cit. minedojo>, GITM<cit. gitm>, and Voyager<cit. voyager>, we opt for using the native human interface provided by Minecraft. This applies to both the observation and action space. The model operates at a speed of 20 frames per second and is required to use a mouse and keyboard interface when interacting with human GUIs. For more information on the detailed descriptions of the observation and action spaces, please refer to the Appendix. Task setting.     In Minecraft, players have access to thousands of items, each with specific acquisition requirements or recipes. For example, stone-type items can only be obtained using a pickaxe, and two planks can be crafted into four sticks (these requirements are available on the Minecraft Wikihttps://minecraft.fandom.com/wiki/Minecraft_ Wiki). In survival mode, players must obtain each type of item from the environment or craft/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe Benchmark  <cit. mcu> for evaluation. These tasks are related to items that can be obtained in the Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups according to recommended categorieshttps://minecraft.fandom.com/wiki/Tutorials/Organization# Categories in Minecraft (see Table<ref. tab:mc_task_set>). Due to the varying complexity of these tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is determined by the average time the human players need to accomplish the corresponding task. Other details about each task, such as language instruction, maximum steps, evaluation times, biome, and initial inventory when the agent is born into the world can be found in Appendix Table  <ref. tab:wood_results>-<ref. tab:food_results>. Evaluation metrics.     By default, the agent always starts in survival mode, with an empty inventory. A task is considered a success when the target object is obtained within a specified time. Due to the open-world nature of Minecraft, the world and initial position that the agent is spawned at could vary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported the average success rate to ensure a thorough assessment. Further, since we categorize the tasks into groups, we also report mean and variance values for each group for ease of presentation. \n\nMain Results\n------------\nWe compare JARVIS-1with other multi-task instruction-following agents based on LLM, including Instruct GPT   <cit. huang2022language,instructGPT>, ReAct   <cit. react>, Inner Monologue   <cit. innermonologue>, DEPS   <cit. deps>. Since some methods are not originally experimented in Minecraft, we reproduce them to conform to the Minecraft specification based on prompt and feedback template design. All LLM-based methods access the LLM model through OpenAI API. And all hyper-parameters of LLM including temperature are kept as default. The average success rates for every task group are listed in Table  <ref. tab:main_results>. JARVIS-1achieves the best performance with all meta tasks. It is important to note that in Minecraft, the technology tree can be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly difficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or a diamond, the agents typically need to perform more actions and longer goal sequences in order to complete the task. As a result, the success rate of all agents decreases as the difficulty level increases. It is evident that reasoning methods (ReAct   <cit. react> vs. GPT   <cit. instructGPT,huang2022language>) and interactive re-planning with feedback (Inner Monologue   <cit. innermonologue> vs. GPT) effectively enhance the agent’s task performance in an open world. However, these approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron and Diamond group. DEPS   <cit. deps>, on the other hand, enables agents to accomplish diamond-related tasks through interactive long-horizon planning accompanied by descriptions and explanations. Nevertheless, its reliability remains very low at approximately 2.5% . In comparison to DEPS   <cit. deps> without memory, JARVIS-1demonstrates superior performance even in challenging tasks due to its extensive experience. In diamond-related tasks specifically, the success rate has increased by nearly 3 times (8.99%  vs 2.42% ). And JARVIS-1usually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas DEPS requires more than 6 rounds. This means that JARVIS-1saves a significant amount of LLM tokens and thinking time, enabling more efficient plan execution and providing additional steps and tokens for handling uncertainty in the environment. Based on our observations, we have found that the bottleneck for JARVIS-1in tasks involving diamonds often lies with the Controller’s inability to perfectly execute short-horizon text instructions generated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for the controller to execute or improving the controller’s ability to follow instructions. \n\nAblation Studies\n----------------\n<figure. fig:memory_size -  Success rate by memory size for different items. We evaluated the performance of JARVIS-1at different memory sizes (representing different learning stages) by measuring the success rate (%  Episodes) of completing key items on the Minecraft technology tree. As the learning progressed, we observed an improvement in completion rates for all items, with an increasing number of successful trajectories being included in memory. After 4 epochs of learning, JARVIS-1had accumulated a total of 425 successful trajectories in its memory. >\n\nJARVIS-1based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT  <cit. instructGPT> and GPT-4  <cit. gpt4>. Among these models, GPT-4 has more parameters and has been proven to outperform ChatGPT in extensive research  <cit. voyager>. We also select the open-source pre-trained LLaMA2 70B (LLaMA2 PT) model  <cit. llama2>. Additionally, we gathered a substantial amount of Minecraft-related text from the internet as training data and further fine-tuned LLaMA2 13B (LLaMA FT). The experiments were conducted on a subset of Minecraft tasks using different language models. Each JARVIS-1learns for 4 epochs of interaction with all task sets and evaluates on task subset across at least 20 seeds. The experimental results are presented in Table  <ref. tab:ablation_lm>. \n\n\n\n===================================================\n\n\n\n\n\n\n\n\n\n\nTable: Success rates for different LLMs on Minecraft tasks.\n\n\n\n\n\n=====================================================================================================================================\n\n\n\n\nTable: Success rates for memory ablation on Minecraft tasks.\n\n<figure. fig:long_horizon -  (Left) The success rate of different models in the  challenge over gameplay time. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-1agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it takes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of obtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1. This task has been evaluated over 300 times on different seeds. These curves indicate that as the game progresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1is constantly improving its skills. >Table  <ref. tab:ablation_lm> demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical success rates as GPT-4. This suggests that language models equipped with memory can significantly enhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B exhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks. However, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves substantially. This indicates that the open-source model lacks knowledge specific to Minecraft and requires further finetuning for successful completion of such tasks. \n\nAblation on Memory\n <label. sec:ablation_memory> We also conduct ablation experiments on the multimodality memory and retrieval methods. We set JARVIS-1w/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance with different memory size (representing different learning stages) as shown in Figure  <ref. fig:memory_size>, which demonstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on a subset of Minecraft tasks using three different retrieval methods: retrieval with instruction embedding only (T), reasoning + retrieval with text embedding (T+R), and reasoning + retrieval with multimodality embedding (M+R). Except the memory and retrieval methods, all others are kept same. The results are listed in Table  <ref. tab:ablation_memory>. The experiments show that reasoning before retrieval can effectively improve retrieval accuracy. And retrieval based on multimodal state including vision observation and symbolic information (e.g., inventory, location etc) is better than only considering the text embedding. \n\nLong-Horizon Challenges\n-----------------------\n<label. sec:long_horizon_experiments> Most concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-horizon tasks like CraftingDiamondPickaxe. The VPT foundation model   <cit. vpt> is capable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions. To address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model for specific task completion. However, after fine-tuning, VPT may experience a decline in performance for other tasks while focusing on the specified task. In contrast, Steve-1   <cit. steve1> has implemented goal-conditioned fine-tuning on VPT-earlygame, enabling it to follow human text instructions while maintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like obtaining dirt, collecting flowers and chopping trees. When it comes to long-horizon tasks such as starting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties. DEPS   <cit. deps> also utilizes LLM as a planner, but it lacks the ability to learn from experience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in its re-planning rounds due to the LM’s context constraints. The experiments reveal that DEPS has a success rate of less than 50%  in generating accurate and executable plans for acquiring diamonds. The probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59% . Consequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within the Minecraft world. Even human players who have mastered the distribution pattern of diamonds achieve success rates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds) within 10 minutes at approximately 15%  and 12% , respectively. JARVIS-1performs better in the ObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone RL-finetuned VPT, JARVIS-1has more than doubled the success rate of obtaining a diamond pickaxe (6.22%  vs 2.5%  within 20 minutes). To increase the chances of obtaining diamonds, we extended the game playing time to 60 minutes (72000 game-playing steps, as shown in Figure  <ref. fig:long_horizon>). As a result, JARVIS-1’s success rate in acquiring a diamond pickaxe improved from 6.2%  to 12.5% . The graph on the right side of Figure 7 illustrates how the success rate of intermediate milestone items changes over time, indicating that JARVIS-1tends to improve with longer game-playing time. We also conduct two variants of JARVIS-1with different self-improving curriculum: human-written and random-generated. All three JARVIS-1have collect experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes. The results show that JARVIS-1with GPT-generated curriculum can finish the task within the shortest game-playing steps and achieve the best performance in 60 minutes. In contrast, VPT’s success rate barely changed when we increased the time from 20 minutes to 60 minutes (from 2.5%  to 3% ). This can be attributed to Minecraft’s durability system where prolonged underground exploration often leads to pickaxe damage. When JARVIS-1’s pickaxe breaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL exhibits perplexing behaviors at this stage by using inappropriate tools for mining stone or crafting unnecessary items. This comparison demonstrates that JARVIS-1possesses superior generalization and planning abilities for long-horizon tasks. Note that our method is designed to be multi-task in its nature and not finetuned through imitation learning on specific dataset or reinforcement learning. \n\nRelated Works\n=============\n\n\nPlanning with LLM\n-----------------\nThere have been some methods leveraging the large language model to generate action plans for high-level tasks in embodied environments  <cit. socraticmodels,reporter,mai2023llm,liu2023llm+,proagent,rladapter,jxma_llm1>. <cit. huang2022language> decompose natural language commands into sequences of executable actions by text completion and semantic translation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances from value functions <cit. saycan>. Some methods also leverage the LLM to produce the program code as plan for better executation  <cit. progprompt,codeaspolicies,text2motion>. However, the above methods assume that the initial plan from the LLM is correct. When there are bugs in the initial plan, it’s difficult for the agent to finish the task successfully. Recent research frequently employs LLM as an interactive planner, harnessing its self-updating capabilities to enhance the plan’s executability over time  <cit. deps,reflexion,sun2023adaplanner>. Inner Monologue  <cit. innermonologue> pilots the front of interactive planning with LLMs, which introduces the feedback (including success detection and scene description) to the planner. However, we found it could still suffer from accumulative planning errors, especially in long-horizon open-world tasks. ReAct  <cit. react> will reason about the agent state before acting, which indicates that various reasoning methods  <cit. chainofthought,treeofthought,spring> are benefitial for planning. LLM-based planning methods often use the fixed pretrained LLM as the agent, while we focus more on life-long and continual learning for agents in open-world environments  <cit. continual1,continual2,voyager>. For better leveraging historical interaction between agent and environments, an explicit memory  <cit. generativeagents,gitm> for more historical chatting has been leveraged for bigger storage of agent experiences. However, the above methods usually rely only on a text-based environment and struggle to execute plans in partial-observed visual open-world environments. \n\nMinecraft Agents\n----------------\n Developing generally capable agents in Minecraft to solve open-world tasks has gained increasing interests  <cit. clip4mc,minedojo,vpt,shaofei,groot,rladapter,plan4mc,gitm>. As an early attempt, <cit. oh2017zero> studied task generalization in a simple Minecraft environment variant. It designed a two-stage pipeline, first mastering the prerequisite skills with parameterization trick, and then learning a meta controller to execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works <cit. oh2017zero,mao2022seihai,lin2021juewu> explored the hierarchical architecture. In recent years, influenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged, who are utilizing vast amounts of internet knowledge to train intelligent agents. <cit. minedojo> trained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles and video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy learning. <cit. vpt> utilizes a pre-trained inverse dynamics model to label actions in YouTube videos which are used to learn a foundation policy VPT through imitation learning. By bridging MineCLIP and VPT, <cit. steve1> creates a performant instruction-following policy Steve-1 to solve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1 can not solve complicated process-oriented tasks due to the expressive capability of its goal space. <cit. groot> learns to follow reference videos as the instruction by merely watching gameplay videos, which improves the capacity of goal space and reduces the cost of policy training. All of these methods focus on improving the smoothness and robustness of interaction between policy and environment. Inspired by the powerful language understanding and reasoning capabilities of large language models, researchers have begun to build Minecraft agents based on LLMs. <cit. voyager> used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills, making novel discoveries, and generating goal proposals. <cit. gitm> integrated LLM with text-based knowledge and memory to equip the agent with common sense and past experiences for higher reasoning efficiency. <cit. plan4mc> used LLM to guide the agent to explore the Minecraft world and interact with the environment with reinforcement learning control policies. \n\nConclusion\n==========\n We propose a multi-task agent JARVIS-1designed for the complex environment of Minecraft, marks a significant advancement in achieving human-like planning within an open-world setting. By leveraging pre-trained Multi-modal Language Models, JARVIS-1not only effectively interprets multimodal inputs but also adeptly translates them into actions. Its integration of a multimodal memory, which draws from both ingrained knowledge and real-time game experiences, enhances its decision-making capabilities. The empirical evidence of its prowess is evident in its impressive performance across a wide array of tasks in Minecraft. Notably, its achievement in the long-horizon diamond pickaxe task, where it achieved a completion rate that surpasses VPT by up to five times, underscores its potential and the strides made in this domain. This breakthrough sets the stage for the future of more versatile and adaptable agents in complex virtual environments. \n\nAcknowledgments\n===============\n This work is funded in part by the National Key R& D Program of China # 2022ZD0160301, a grant from CCF-Tencent Rhino-Bird Open Research Fund, NSF grants # IIS-1943641, # IIS-1956441, # CCF-1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI. The authors sincerely thank Dr. Rita Zhang, Zhixiang Dai at NVIDIA for the valuable technical support of GPU computing. gong2023mindagent,mialon2023augmented unsrtnat    \n\nImplementation Details\n======================\n<label. sec:implementation_details> \n\nController\n----------\n<label. sec:controller> Tasks in Minecraft are usually related to mine and craft goals. The mine goals require the agent to collect raw materials from the environment using the appropriate tools. The craft goals ask the agent to use the recipe to generate new items with existing materials in inventory. The mine goals are achieved through STEVE-1<cit. steve1> with text condition during implementation. The environment can directly executes the craft and smelt actions (craft/smelt with argument), which are same as MineDojo  <cit. minedojo> . \n\nInteractive Planner\n-------------------\nJARVIS-1relies on the Multi-modal Language Model for planning, self-checking, and self-explaining, and can accept three types of inputs: visual images, language, and symbolic information (including inventory, located position, home, current life statistics, etc.). Specifically, this is a hybrid model with language processing capabilities derived from the GPT model  <cit. gpt4>. The visual ability comes from MineCLIP  <cit. minedojo>. We collected approximately 1000 Minecraft text data from the internet and calculated the similarity between the current vision observation and these text data. Text above the similarity threshold will be selected into the GPT model’s prompt. Symbolic information is converted into natural language text through a designed template. All modalities are ultimately captured as language and processed by the GPT model. Different modules in JARVIS-1(e.g. self-check and self-explain) are completed through MLM based on different prompts. The specific prompt design are shown below.     backgroundcolor=, caption=Planning prompt in JARVIS-1, label=lst:planning_prompt   \n\nSystem: You are a helper agent in Minecraft. You need to generate the sequences of goals for a certain task in Minecraft. Just refer the history dialogue to give the plan consist of template. Do not explain or give any other instruction. \n\n==========\nUser: My current inventory has nothing. I current locate in plains. How to obtain 1 wooden_pickaxe in Minecraft step-by-step?\nAssistant: The code for obtaining 1 wooden_pickaxe is as follows:\ndef craft_wooden_axe(initial_inventory=):\n    mine(obj = \"log\":3, tool = None)  # step 1: chop down trees to mine logs\n    craft(obj = \"planks\":12, materials = \"log\":3, tool = None)   # step 2: craft 12 planks from 3 logs\n    craft(obj = \"stick\":4, materials = \"planks\":2, tool = None) # step 3: craft 4 sticks from 2 planks\n    craft(obj = \"crafting_table\":1, materials = \"planks\":4, tool = None) # step 4: craft 1 crafting_table from 4 planks\n    craft(obj = \"wooden_axe\":1, \"planks\": 3, \"stick\": 2, \"crafting_table\") # step 5: craft 1 wooden_axe from 3 planks and 2 sticks on crafting table\n    return \"wooden_axe\"\nUser: [Description] I succeed in step 1, 2, 3, 4, 5. \nI finish all steps and I obtain 1 wooden_axe successfully.\n\n==========\nUser: My current inventory has <inventory>. <visual observation>. How to obtain 1 stone_sword in Minecraft step-by-step?\nAssistant: \n\n    backgroundcolor=, caption=Goal parsing prompt in JARVIS-1, label=lst:parser_prompt   \n\nSystem: Extract the action name, action type, goal object, tool and action rank from the input text.\n\nUser: mine(\"log\":3, null); # step 1: chop down trees to mine logs\nAssistant:\nname: mine_log\ntext condition: chop down trees to mine logs\naction: mine\nobject_item: log\nobject_number: 3\ntool: null\nrank: 1\n###\n\ninput: craft(\"planks\":12, \"log\":3, null); # step 2: craft 12 planks from 3 log\nAssistant:\nname: craft_planks\ncondition: craft 12 planks from 3 log\naction: craft\nobject_item: planks\nobject_number: 12\nmaterials: \"log\":3\ntool: null\nrank: 2\n###\n\n\n\n\n    backgroundcolor=, caption=Self-explaining prompt in JARVIS-1, label=lst:explainer_prompt   \n\nSystem: Here are some actions that the agent fails to perform in Minecraft. Please give the explanation of action execution failure according to the current inventory information of the agent.\n\n###\nUser:\nFailed Action: mine(\"iron_ore\":1, null); # step 5: dig down to mine iron blocks\nCurrent Inventory: null\nAssistant: Because mining iron_ore needs to use the tool stone_pickaxe, but my inventory does not have stone_pickaxe. So I need to craft stone_pickaxe first.\n\n###\nUser:\nFailed Action: craft(\"stone_pickaxe\":1, \"cobblestone\":3, \"stick\":2, \"crafting_table\"); # step 1: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on crafting_table\nCurrent Inventory: null\nAssistant: Because crafting stone_pickaxe needs to have 3 cobblestone and 2 stick in inventory, but my inventory does not have cobblestone and stick. So I need to mine cobblestone and craft stick first.\n\n### \nUser:\nFailed Action: craft(\"stick\":4, \"planks\":2, null); # step 3: craft 4 stick from 2 planks first\nCurrent Inventory: null\nAssistant: Because crafting stick needs to have planks in inventory, but my inventory does not have planks. So I need to craft planks first.\n\n###\n\n\n    backgroundcolor=, caption=Self-checking prompt in JARVIS-1, label=lst:self_check_prompt   \n\nSystem: You are a helper agent in Minecraft. Check the plan whether can be finished.\n\n==========\nUser: My current inventory has nothing. I current locate in plains. My task is to obtain 1 wooden axe in Minecraft step-by-step. This is my plan:\ndef craft_wooden_axe(initial_inventory=):\n    mine(obj = \"log\":3, tool = None)  # step 1: chop down trees to mine logs\n    craft(obj = \"planks\":12, materials = \"log\":3, tool = None)   # step 2: craft 12 planks from 3 logs\n    craft(obj = \"stick\":4, materials = \"planks\":2, tool = None) # step 3: craft 4 sticks from 2 planks\n    craft(obj = \"wooden_axe\":1, \"planks\": 3, \"stick\": 2, null) # step 4: craft 1 wooden_axe from 3 planks and 2 sticks\n    return \"wooden_axe\"\n\nAssistant:\nMy current inventory is empty.\n1. I want to mine 3 logs. Mining logs do not require tools. My inventory will has 3 logs.\n2. I want to craft 12 planks from 3 logs. 1 log can produce 4 planks. My inventory will add 12 planks, and delete 3 logs. My inventory will has 12 planks.\n3. I want to craft 4 stick from 2 planks. 2 planks can produce 4 planks. My inventory will add 4 stick and delete 2 planks. My inventory will has 10 planks, 4 stick.\n4. I want to craft 1 wooden pickaxe from 3 planks and 2 sticks. Crafting wooden pickaxe requires crafting_table. But I do not have crafting_table in inventory. This action will failed. \nReturn: Step 4 will failed because of lacking of crafting_table.\n\n\n\nMemory\n------\nOur memory records every successful trajectory experience of JARVIS-1, including the task goals that the agent needs to execute, the actual goal sequence (plan) executed by the agent, and the state (visual observation and symbolic information returned from the environment) when the agent completes the task. In specific implementation, memory is a list where each trajectory experience is encoded as a dictionary, including the keys task, state, and plan. \n\nEnvironment Setting\n===================\nOur Minecraft environment is a hybrid between MineRL   <cit. minerl> and the MCP-Reborn (github.com/Hexeption/MCP-Reborn) Minecraft modding package. Unlike the regular Minecraft game, in which the server (or the \"world\") always runs at 20Hz and the client runs as fast as rendering \n\nObservation Space\n-----------------\nThe environment observations include two parts. One are simply the raw pixels from the Minecraft game that player would see. The overlays including the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or “use” actions are not removed, which are same with the human-playing GUI. Another part is some auxiliary information about the current environment of the agent, including the agent’s current location and current weather. These pieces of information can be obtained by human players by pressing F3. The specific observation information we include are shown in Table  <ref. tab:observation_space>. \n\n\n============================================================================================\n\n\n\n\nTable: The observation space we use in Minecraft.\n\nNote that no high-level observations like voxels and lidar information in Minedojo   <cit. minedojo> can be accessed by agents. During the actual inference process, controller only perceive the raw pixels and interact with the environment, which is same with VPT   <cit. vpt> models. The agent will access information from the environment to generate the text condition of the controller. The environment observations are simply the raw pixels from the Minecraft game that a human would see. Unlike MineRL, we do not remove overlays like the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or “use” actions. The field of view is 70 degrees, which corresponds to the Minecraft default. GUI scale (a parameter controlling the size of the in-game GUI) is set to 2, and brightness is set to 2 (which is not a Minecraft default, but is very frequently used in online videos). The rendering resolution is 640x360, which is downsampled to 128x128 before being input to the models. We empirically found 128x128 to be the smallest resolution for which in-game GUI elements are still discernible, and then chose that to minimize compute costs. Whenever an in-game GUI is open, we additionally render an image of a mouse cursor at the appropriate mouse position to match what a human player’s operating system does (Fig. 12). \n\nAction Space\n------------\n We design a hybrid action space. Some are directly available to human players, including keypresses, mouse movements, and clicks, which are similar to VPT  <cit. vpt>. The keypresses and clicks are binary functional actions, including forward, jump, use and attack etc. In addition to the binary (on/off) keypress actions, our action space also includes mouse movements, as with human gameplay. As with human gameplay, when in-game GUIs are not open, mouse X and Y actions change the agent’s yaw and pitch, respectively. When a GUI is open, camera actions move the mouse cursor. Mouse movements are relative (i.e. they move the mouse or camera relative to the current position, and thus their effect depends on the current position). In Minecraft, interacting with the inventory requires precise mouse movements for tasks like crafting and smelting, while mining and navigating the world can be accomplished with broader mouse actions. To be enable to achieve both the same action space, we abstract the craft and smelt action with GUI into functional binary actions, which are same as MineDojo  <cit. minedojo>. The detailed action space are described in Table  <ref. tab:action_space>. \n\n\n========================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: The action space we use in Minecraft.\n\n\n\nRules\n-----\nWe choose to conduct the test in survival mode of Minecraft 1.16.5. For each environment reset, we have added the following rules: \n - : Set the difficulty of the environment to peaceful mode. \n\n- : Set the environment to daytime forever. \n\n- : Set agent to not drop items upon death. We have added a time limit for each task, within which if the player dies, they will respawn at the spawn point and retain their previous inventory contents. \n\n- : In order to facilitate the display of agent behavior, we have added night vision effects to the agent. \n\n\n\nResults and Details of 200+ tasks in Minecraft Universe Benchmark\n=================================================================\n<label. sec:detailed_results> We list the evaluation task set belows with details including task name, maximum steps, initial inventory, biome, and language instructions. We also show the evaluation times across different seeds and successful episodes rate. Note that all tasks are evaluated in Minecraft 1.16.5 Survival Mode. \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the  ",
  "subsections": [
    {
      "title": "Introduction",
      "content": "<figure. fig:tech_tree -  How does JARVIS-1unlonk the technology tree of the Minecraft universe. JARVIS-1can consistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond, redstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1not only outperforms the previous state-of-the-art VPT  <cit. vpt> (6%  vs. 2.5%  reliablity) on diamond pickaxe, but also can craft almost all diamond items in overworld including diamond chestplate.>Creating sophisticated agents that can accomplish myriad of tasks in complex domains remains a pivotal milestone towards generally capable artificial intelligence  <cit. gato,gpt3,flamingo,rt1,jxma_vl1>. Recent advancements have shown a trend towards employing a hierarchical goal execution architecture  <cit. deps,innermonologue,huang2022language>, and leveraging large language models (LLMs) as the high-level planner to generate action plans that will be ultimately executed by low-level instruction-following controllers. Albeit the fruitful progress they have yielded in many robotics  <cit. innermonologue> and even open-world environments like Minecraft  <cit. minedojo,minerl>, today’s agents built with these approaches are still struggling with three major issues: 1) perceive the world from multimodal sensory observations, such as images, videos in addition to natural language instructions and feedback for planning; This is mostly due to the inability of LLM-based planners on processing multimodal data  <cit. huang2022language,react>; 2) perform consistent and accurate long-term planning. This requires multi-round, knowledge, and reasoning-intensive dialogues, which remain great challenges to LLMs  <cit. innermonologue>; 3) learn and evolve in a life-long fashion. This calls out the need for agents to propose their own tasks and self-improve. Addressing these issues will unleash the full planning potential of LLM-based agents, and expedite the development of more generalist agents. In this work, we introduce JARVIS-1, a brand new agent that can robustly produce plans for long-horizon tasks from multimodal user and environment inputs, and translate them into motor control in Minecraft, a popular yet challenging open-world testbed for generalist agents. To be specific, we chain a multimodal foundation model MineCLIP   <cit. minedojo> and an LLM   <cit. gpt3> together, the resulting multimodal language model (MLM) allows our agent to better understand the tasks, situations, and environmental feedback. To further enhance the correctness and consistency of planning, especially on long-horizon tasks, we propose to augment the agent with a multimodal memory, which stores both the scenarios and actual plans of the successful planning experiences in the past. By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be strengthened from the agent’s own interactions with the environment in an in-context manner. Finally, JARVIS-1is able to evolve throughout the gameplay by proposing tasks on its own (i.elet@token .self-instruct) as a means of exploration and saving the obtained experiences in the multimodal memory, therefore facilitating better reasoning and planning. This self-improving ability sparks its potential for a higher level of autonomy. Our main evaluations are conducted in Minecraft, with more than 200 tasks selected from the Minecraft Universe Benchmark  <cit. mcu>, with no demonstration provided. The tasks cover a broad spectrum from the early game (e.glet@token .ObtainCraftingTable) to intermediate and even challenging long-horizon tasks (e.glet@token .ObtainDiamondPickaxe). A glimpse of what JARVIS-1is able to achieve can be found in Figure  <ref. fig:tech_tree>. JARVIS-1exhibits strong performances on these tasks, representing an up to 5× increase to the previous records. Our ablative analysis then offers a detailed account of how JARVIS-1approaches this significant progress and becomes the first agent that can robustly obtain the diamond pickaxe with up to 12.5%  success rate. What is even more surprising is that, without the need for additional training, JARVIS-1demonstrates a continuous increase in performance as game time increases in long-horizon tasks. Moreover, JARVIS-1has demonstrated its potential of self-improve in an exploratory life-long learning experiment, where it needs to propose tasks to progressively explore the world, collect experiences, and sharpen its planning skill using these experiences stored in the multimodal memory. <figure. fig:challenge - Challenges in open-world environments and how does JARVIS-1tackle them. (Left) With situation-aware planning, JARVIS-1substantially improves the success rate on the challenging  task, compared to the baseline (GPT) without it. Note: Due to resource constraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases (STONE→IRON→DIAMOND), JARVIS-1exhibits more significant advantages thanks to interactive planning; (Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences on other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1can utilize prior experiences on relevant tasks for better planning. >In summary, JARVIS-1pilots the effort towards a human-like multi-task and autonomous agent in an open-world, embodied environment like Minecraft. We would like to share the key takeaways of what we have learned during its development as follows:  \n  - From LLMs to MLMs.    The capability of perceiving multimodal sensory input is critical to planning in a dynamic and open-world world. JARVIS-1enables this by chaining a multimodal foundation model together with an LLM. Compared to LLM “blindly” produces plans, MLM is able to natively understand the current situation and plan accordingly. Further, rich environmental feedback can be obtained through multimodal perception, therefore helping the self-check and self-explain of the planner spot and fix possible bugs in the plans, enabling stronger interactive planning. \n\n- Multimodal memory.    Early research has suggested the crucial role that memory mechanisms can serve in the functioning of generalist agents. By outfitting JARVIS-1with a multimodal memory, we effectively allow it to plan with both pretrained knowledge and its actual experiences in the world, therefore bringing significant improvement to planning correctness and consistency. Compared to canonical RL or planning agents with exploration, no additional model update is needed as the MLM in JARVIS-1makes it possible to leverage these experiences in an in-context manner. \n\n- Self-instruct and self-improve.    A sign of generalist agents is the capacity to proactively acquire new experiences and continuously improve themselves. We have demonstrated how JARVIS-1effectively traverses the environment by executing tasks autonomously generated through its self-instruct mechanism. With multimodal memory teaming up with experiences from the explorations, we have observed consistent improvement, especially in accomplishing more complicated tasks. Ultimately, this aspect of autonomous learning in JARVIS-1signifies an evolutionary step towards generalist agents that can learn, adapt, and improve over time with minimal external intervention. \n\n<figure. fig:pipeline -  Architecture of JARVIS-1and its self-improving mechanism. (a) JARVIS-1comprises a memory-augmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1also utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1can strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a growing memory that helps with better planning on tasks that has been (partially) visited before. >",
      "subsections": [],
      "figures": {
        "fig:tech_tree": {
          "label": "fig:tech_tree",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/tech_tree_1021.pdf"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": null,
              "width": 626
            }
          ],
          "caption": " How does JARVIS-1unlonk the technology tree of the Minecraft universe. JARVIS-1can consistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond, redstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1not only outperforms the previous state-of-the-art VPT  <cit. vpt> (6%  vs. 2.5%  reliablity) on diamond pickaxe, but also can craft almost all diamond items in overworld including diamond chestplate.",
          "section": "1. Introduction"
        },
        "fig:challenge": {
          "label": "fig:challenge",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/challenge_1020.pdf"
          ],
          "size": [
            {
              "scale": 0.55,
              "height": null,
              "width": null
            }
          ],
          "caption": "Challenges in open-world environments and how does JARVIS-1tackle them. (Left) With situation-aware planning, JARVIS-1substantially improves the success rate on the challenging  task, compared to the baseline (GPT) without it. Note: Due to resource constraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases (STONE→IRON→DIAMOND), JARVIS-1exhibits more significant advantages thanks to interactive planning; (Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences on other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1can utilize prior experiences on relevant tasks for better planning. ",
          "section": "1. Introduction"
        },
        "fig:pipeline": {
          "label": "fig:pipeline",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/architecture_1021.pdf"
          ],
          "size": [
            {
              "scale": 0.45,
              "height": null,
              "width": null
            }
          ],
          "caption": " Architecture of JARVIS-1and its self-improving mechanism. (a) JARVIS-1comprises a memory-augmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1also utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1can strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a growing memory that helps with better planning on tasks that has been (partially) visited before. ",
          "section": "1. Introduction"
        }
      }
    },
    {
      "title": "Challenges for Agents in open-world Environments",
      "content": "Compared to canonical scenarios with relatively small scale, simple dynamics, and limited tasks, open-world environments impose substantial challenges to building agents that can accomplish a diverse set of tasks  <cit. minedojo,minerl-rel-1,minerl-rel-2,minerl-rel-3,shaofei,deps,groot>. In this section, we will review three major challenges we’ve identified during the development of JARVIS-1. \n\nChallenge I: Situation-Aware Planning\n-------------------------------------\n<label. sec:challenge_sit_planning> In an open world, there could be various possible paths towards an open-world goal. However, not all of them are plausible or equally efficient given a certain situation (location, inventory status, etclet@token .). For example, building a bed \n    < g r a p h i c s >\n can be done through collecting wool from sheeps \n    < g r a p h i c s >\n, haunting spiders for strings \n    < g r a p h i c s >\n, or trading with villagers \n    < g r a p h i c s >\n. Depending on the current location and its proximity to these subjects, some options can be more viable and more efficient than others. Further, the agent’s own situation can also change throughout the episode, e.glet@token .day and night shifts, weather conditions (bringing different types of danger), tool usage (it can be broken). To this end, the plan needs to be constantly updated based off the current situation. Figure  <ref. fig:challenge> (left) shows that when attempting the \"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning without looking at the current situation, the agent failed to complete the task as opposed to human players and JARVIS-1, which perform situation-aware planning from time to time. We’ve observed that many failures coming from this were attributed to the agent’s inability to adapt to the changing situations including entering a new biome, the tool being used becoming broken, etclet@token .. \n\nChallenge II: Task Complexity\n-----------------------------\n<label. sec:challenge_task_complexity> The second challenge comes from the higher task complexity in open-world environments. Due to the richness of terrains, objects, and action space, tasks in open-world domains usually require substantially long planning horizons as well as good accuracy and precision. For example, the task ObtainEnchantingTable \n    < g r a p h i c s >\n includes more than 20 different sub-goals and therefore demand significantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved precisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with diamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks on crafting table; otherwise, the subsequent sub-goals won’t be executed due to unfulfilled preconditions. To tackle this, we may refer to some approaches in LLM reasoning, e.glet@token .self-debugging  <cit. selfdebug> and turning the planning into an interactive fashion. In <ref> (Middle), we’ve shown that as the complexity of the task increases, our JARVIS-1, which uses interactive planning  <cit. deps> to mitigate the aforementioned issues (details can be found in <ref>), elicits more significant advantages over the baseline (GPT) planner. \n\nChallenge III: Life-long Learning\n---------------------------------\n<label. sec:challenge3> Finally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult for an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this end, agents in an open world should be able to learn novel tasks while completing existing tasks, i.elet@token .life-long learning. Furthermore, as many open-world agents employ large models  <cit. deps,plan4mc,voyager,gitm>, canonical gradient-based learning could be extremely inefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1tackles this by adopting a memory to save all the experiences on past tasks. By retrieving memory entries relevant to the newly-coming task and putting them into the context as a reference, JARVIS-1is able to accumulate more experiences as the game continues and strengthen its own planning skills without gradient update. As illustrated in <ref> (Right), for instance, both ObtainDiamondPickaxe\n    < g r a p h i c s >\n and ObtainDiamondAxe\n    < g r a p h i c s >\n require gathering almost identical materials. Therefore, they can help each other by using the experiences from the other task. Compared to completing these challenging tasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1can bring significant advantages. ",
      "subsections": [
        {
          "title": "Challenge I: Situation-Aware Planning",
          "content": "<label. sec:challenge_sit_planning> In an open world, there could be various possible paths towards an open-world goal. However, not all of them are plausible or equally efficient given a certain situation (location, inventory status, etclet@token .). For example, building a bed \n    < g r a p h i c s >\n can be done through collecting wool from sheeps \n    < g r a p h i c s >\n, haunting spiders for strings \n    < g r a p h i c s >\n, or trading with villagers \n    < g r a p h i c s >\n. Depending on the current location and its proximity to these subjects, some options can be more viable and more efficient than others. Further, the agent’s own situation can also change throughout the episode, e.glet@token .day and night shifts, weather conditions (bringing different types of danger), tool usage (it can be broken). To this end, the plan needs to be constantly updated based off the current situation. Figure  <ref. fig:challenge> (left) shows that when attempting the \"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning without looking at the current situation, the agent failed to complete the task as opposed to human players and JARVIS-1, which perform situation-aware planning from time to time. We’ve observed that many failures coming from this were attributed to the agent’s inability to adapt to the changing situations including entering a new biome, the tool being used becoming broken, etclet@token .. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Challenge II: Task Complexity",
          "content": "<label. sec:challenge_task_complexity> The second challenge comes from the higher task complexity in open-world environments. Due to the richness of terrains, objects, and action space, tasks in open-world domains usually require substantially long planning horizons as well as good accuracy and precision. For example, the task ObtainEnchantingTable \n    < g r a p h i c s >\n includes more than 20 different sub-goals and therefore demand significantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved precisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with diamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks on crafting table; otherwise, the subsequent sub-goals won’t be executed due to unfulfilled preconditions. To tackle this, we may refer to some approaches in LLM reasoning, e.glet@token .self-debugging  <cit. selfdebug> and turning the planning into an interactive fashion. In <ref> (Middle), we’ve shown that as the complexity of the task increases, our JARVIS-1, which uses interactive planning  <cit. deps> to mitigate the aforementioned issues (details can be found in <ref>), elicits more significant advantages over the baseline (GPT) planner. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Challenge III: Life-long Learning",
          "content": "<label. sec:challenge3> Finally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult for an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this end, agents in an open world should be able to learn novel tasks while completing existing tasks, i.elet@token .life-long learning. Furthermore, as many open-world agents employ large models  <cit. deps,plan4mc,voyager,gitm>, canonical gradient-based learning could be extremely inefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1tackles this by adopting a memory to save all the experiences on past tasks. By retrieving memory entries relevant to the newly-coming task and putting them into the context as a reference, JARVIS-1is able to accumulate more experiences as the game continues and strengthen its own planning skills without gradient update. As illustrated in <ref> (Right), for instance, both ObtainDiamondPickaxe\n    < g r a p h i c s >\n and ObtainDiamondAxe\n    < g r a p h i c s >\n require gathering almost identical materials. Therefore, they can help each other by using the experiences from the other task. Compared to completing these challenging tasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1can bring significant advantages. ",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    },
    {
      "title": "Multi-task Agent with Memory-Augmented MLM",
      "content": "This section details the architecture of the proposed JARVIS-1agent. We begin with an overview of the modular agent design in <ref>. Next, we elaborate on how to implement an interactive planning scheme with a multimodal language model, which helps with more accurate plans, especially on complex and long-horizon tasks in <ref>. Finally, we show how to augment this planning framework with a multimodal memory to allow JARVIS-1to strengthen its planning skill throughout the episode by in-context life-long learning in <ref> and <ref>. <figure. fig:planner -  Interactive planning in JARVIS-1. After receiving the current task instruction and observation, JARVIS-1will produce an initial plan, which will go through self-check to get possible bugs (marked in colorred) fixed. Further, in case any error (also marked in colorred) occurs during the execution of the refined plan, JARVIS-1will try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check and self-explain significantly boosts the correctness and robustness of JARVIS-1planning. >\n\nOverview\n--------\n<label. sec:overview> We aim to develop an agent capable of solving long-horizon instruction-following tasks using image observations and human-aligned actions. To accomplish this, we propose a multi-modal agent including an interactive planner, a goal-conditioned controller, and a multimodal memory of multimodal experiences. Upon receiving a task and the current observation, JARVIS-1first utilizes the MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences from the memory. These experiences will then be used along with the planning instruction to prompt the MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference plans, the planner will ultimately produce a series of K short-horizon goals g_1,… ,g_K to be executed by the controller. Once the plan is successfully executed, it will be stored in the memory along with the task and the agent situation when it was planned. We also empower JARVIS-1with life-long learning by combining self-instruct, where JARVIS-1will propose some tasks for itself to complete as a means of exploration; and self-improve, where multiple JARVIS-1agents will be running in parallel to gather experiences, therefore helping with better planning later. We provide an illustration in <ref>. \n\nInteractive Planning with MLM\n-----------------------------\n<label. sec:interactive_planning> As we have mentioned in <ref> and <ref>, the primary challenges for planning in Minecraft come from the requirement of being able to plan for long-horizon tasks under dynamic observations. Confirmed by many prior arts  <cit. deps,voyager,plan4mc>, this makes it exceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this end, we take a multimodal language model (MLM) as zero-shot planner and combine it with an interactive planning framework to tackle these challenges. <figure. fig:memory_retrieval - Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1will first think backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited depth. The sub-goal that is present in the memory will join the current visual observation to form the final query. Entries that match the text query will be ranked by the perceiving distance of their states to the obs query and only the top entry of each sub-goal will be retrieved. >Situation-aware planning with MLM.    To achieve situation-aware planning, the planner must take the current observation into account, in addition to the task instruction  <cit. huang2022language,react>. Specifically, we begin with translating the multimodal observation into text descriptions. As opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft items (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT  <cit. gpt3> to generate sentences that describe these observations. For example, a generated sentence could be \"I can see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according to current visual observation during planning. Additional situation details including biome and inventory status are also converted into text using templates. Finally, we prompt the MLM again (the language part only) into a plan given the task instruction and all the aforementioned textual situation descriptions. Compared to end-to-end alternatives  <cit. brohan2023rt,leo>, we find our composable usage of MLM provides higher quality situation descriptions and ultimately, plans with much less hallucination. Planning with self-check.    Our first layer of shield to ensure the correctness of plans involves self-check. Similar to self-debugging   <cit. selfdebug>, given an initial plan, we ask JARVIS-1to progressively simulate the plan execution, predict the resulting state after each step (primarily the state of inventory), and evaluate them. By verifying if these states satisfy the goal’s precondition, JARVIS-1can proactively identify potential plan flaws. Compared to the canonical planner where the agent has to encounter the error first before making a remedy, this upfront plan verification could mitigate the need for the agent to recover (re-plan) from more challenging situations due to plan failure. For instance, if an agent starts digging underground without sufficient wood, it would typically have to return to the surface, which substantially lowers the chance of completing the task. Planning with environment feedback.    Next, our interactive planning framework ventures into allowing JARVIS-1to quickly recover from failure by leveraging environment feedback in a closed-loop fashion. The process is illustrated in <ref>. During plan execution, we feed the feedback to the MLM of JARVIS-1in case there is any execution failure (possibly due to a flawed plan) and utilize its self-explain mechanism  <cit. reflexion> to explain the error and locate the bugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1will produce an improved plan based on both the outside environment feedback and the inside retrospective. Compared to other agents that rely on human intervention or privileged environment information  <cit. innermonologue,gitm>, JARVIS-1has the ability to speculate about the reasons why current goals cannot be achieved, without the need for additional information or design. \n\nPlanning with Multimodal Memory in the Loop\n-------------------------------------------\n<label. sec:planning_with_memory> To address the life-long learning challenge mentioned in <ref>, we equip JARVIS-1with multimodal memory to allow learning from its own past experiences. We will detail the formulation of the retrieval-augmented planning, query generation, and memory layout below. Retrival-augmented planning. Retrieval-augmented generation (RAG)  <cit. lewis2020retrieval,mao2020generation> enhances the quality of responses generated by LLMs by incorporating external sources of knowledge to complement the model’s internal representation. We also utilize RAG to enhance JARVIS-1’s long-term planning capability. Compared to official RAG methods leveraging the external knowledge library, we take the collected multimodal memory as the knowledge library and retrieve the interactive experiences as the demonstration prompt to augment the planning results. The formulation is as follows: \n    p(y | x) ≈∑ _z∈top-k(p(·| x))p_η (z | x) p_θ (y | x,z),\n where x, y, and z denote instruction, plans, and retrieved memory entries respectively, and p_η and p_θ are denoted as retrieval and planning models. Such retrieval-augmented planning method helps JARVIS-1ground the internal knowledge into the open-ended environments efficiently and leverage the historical interaction feedback to solve the hallucination within LLMs and produce more accurate plans. Multimodal memory.    We have demonstrated the layout of our multimodal memory on the right side of Figure  <ref. fig:memory_retrieval>. From a high level, it is a key-value memory where the keys are multimodal, comprising both the task and the observation (or situation) made when this memory entry was created. The values are the plans that were successfully executed. Note that, since the plans in an open-world environment like Minecraft are situated (see <ref>), there could be multiple entries that are with the same task but different observations and plans. As a result, JARVIS-1needs to produce multimodal queries based on the current task and situations to retrieve the relevant memory entries. Query generation via reasoning.     When presented with an instruction as a task, we employ query generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks, which will then be used as textual queries to retrieve relevant planning experiences as references for solving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty inventory\" as shown in Figure  <ref. fig:memory_retrieval>. JARVIS-1queries the MLMs to identify the tasks that are required for achieving the main task in a backward search fashion, e.g., “obtain book \n    < g r a p h i c s >\n/diamond \n    < g r a p h i c s >\n/obsidian \n    < g r a p h i c s >\n with empty inventory”. The search depth is bounded for efficiency. Further, instead of relying solely on retrieval based on the text query  <cit. voyager,gitm>, we also propose to append the agent’s current visual observation to the textual query, resulting in a multimodal query to take the situation into account during memory retrieval. Multimodal retrieval. After obtaining the textual and visual query, we compute the alignment between the query and each trajectory in multimodal memory. We first use the text encoder of CLIP model to compute the embedding of the query and task key of each entry in memory. We select the memory entries with similarity higher than the confidence threshold as the candidate entries. Then we will compute the visual state embedding of query and states in candidate entires. Then we sort the candidate entries with the visual embedding similarities, which can be formed as: \n    p_η (z| x) ∝   CLIP_v(s_z)^⊤CLIP_v(s_x),\n where s_z and s_x are the visual key of memory entries and visual query, respectively. Finally, we retrieve the plan of top-k candidate entries as reference prompt z.  Characteristics of 11 task groups encompassing over 200 minecraft tasks. <label. tab:mc_task_set> 0.950.0pt!   Group Task Num.Max.StepsInitial Inventory Biome  Language Instruction Wood  34  12k  null  Plains/Forest  Pick up a wooden_ pickaxe. Wood-Variants  43  12k  null Savanna/Jungle/Taiga Pick up a acacia_ boat. Stone  10  12k  iron_ axe  Plains/Forest  Craft a furnace given an iron axe. Iron  22  12k  iron_ axe  Plains/Forest  Smelt and craft an iron_ door given an iron axe. Gold  9  36k  iron_ axe  Plains/Forest  Smelt and craft an golden_ axe given an iron axe. Diamond  7  36k  iron_ axe  Plains/Forest Dig down to mine diamond and craft diamond_ pickaxe.Redstone  7  36k  iron_ axe  Plains/Forest  Mine redstone and make dropper given an iron axe. Blocks  15  12-36k  iron_ axe  Plains/Forest  Dig down to mine lapis_ lazuli block. Armor  17  12-36k  iron_ axe  Plains/Forest  Craft diamond_ boots given an iron axe and equip it. Decoration  17  12k  iron_ axe  Flower Forest  Obtain the bed and dye it red. Food  9  12k  iron_ axe  Plains Kill sheep to obtain mutton and cook it. \n\nSelf-improving Agents\n---------------------\n<label. sec:learning> Learning in Minecraft with memory.    The remaining issue now is where the aforementioned multimodal memory comes from. Inspired by the life-long learning scheme in many close-world and open-world reinforcement learning problems  <cit. lifelong1,lifelong2,voyager>, we propose the following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set of tasks, which form some curricula for the agents to complete as means of exploration of the world. During this process, JARVIS-1produces plans, interacts with the environment, embraces the errors, and stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1on various tasks. Therefore, JARVIS-1is able to produce better plans with the memory teaming up with the planning experiences. In our experiments, we use this as the default setting for all tasks. Exploration using self-instruct.    The key issue to the success of learning with memory is how to effectively acquire useful experiences given a limited amount of time. We propose to use self-instruct  <cit. selfinstruct> to generate the dynamic curriculum and guide JARVIS-1to learn from the interactions with environments. In each round, we prompt the MLM to consider how capable JARVIS-1is at this point and subsequently select tasks from a task pool to explore. We find that the curriculum almost follows the technical tree-growing direction. To accelerate the learning process, we augment the linear self-instruct to distributed learning in distributed environments with shared memory, i.elet@token .speculative execution  <cit. leviathan2023fast>. Specifically, we generate multiple executable tasks as candidate task batches and provide them to agents with the same memory for verification and execution in various different environments. Meanwhile, experiences are collected into a shared centralized memory. When all exploration tasks have been accomplished, we move to the next round, until the memory reaches a certain capacity. Life-long learning.    We’ve also observed that the aforementioned learning (where the memory is being filled) can be extended throughout the whole gameplay, where the agent gradually acquires more and more skills. As the gameplay continues, more and more experiences are pouring in, therefore JARVIS-1can find better references for challenging tasks like ObtainDiamondPickaxe, resulting in an improved success rate on these tasks. Further, there is no gradient update in this thanks to the memory-augmented MLM, i.elet@token .we can do in-context life-long learning. In Section  <ref. sec:long_horizon_experiments>, we offer exploratory experiments to show the potential of such capability of JARVIS-1.  Results of JARVIS-1and baselines on Minecraft. The detailed task instructions, settings and results can be found in the Appendix. <label. tab:main_results> 0.80.0pt!   Group  Task  GPT  ReAct  Inner Monologue  DEPS  JARVIS-1  \n    < g r a p h i c s >\n  26.67  45.00  36.67  75.00  91.55 -2*Wood  AVG  27.30±14.86  40.31±13.30  60.15±19.41  80.23±17.32  88.84±16.82  \n    < g r a p h i c s >\n  6.67  36.67  30.00  36.67  60.47 -2*WoodVar  AVG  [HTML]FFFFFF24.39±11.08  [HTML]FFFFFF38.13±12.81  [HTML]FFFFFF53.39±12.86  [HTML]FFFFFF68.75±12.32  76.78±12.27  \n    < g r a p h i c s >\n  20.00  20.00  66.67  75.00  94.20 -2*Stone  AVG  [HTML]FFFFFF20.21±12.32  [HTML]FFFFFF39.00±12.15  [HTML]FFFFFF52.86±16.90  [HTML]FFFFFF69.27±7.78  88.69±4.87  \n    < g r a p h i c s >\n  0.00  0.00  3.33  20.00  33.82  \n    < g r a p h i c s >\n  3.33  6.67  0.00  20.00  38.10 -3*Iron  AVG  [HTML]FFFFFF3.27±2.85  [HTML]FFFFFF4.61±3.63  [HTML]FFFFFF5.20±5.17  [HTML]FFFFFF16.92±4.69  34.63±10.61  \n    < g r a p h i c s >\n  0.00  2.00  2.00  6.00  14.49 -2*Gold  AVG  [HTML]FFFFFF0.00±0.00  [HTML]FFFFFF0.45±0.60  [HTML]FFFFFF0.59±0.64  [HTML]FFFFFF2.20±1.55  6.85±4.71  \n    < g r a p h i c s >\n  0.00  0.00  1.00  2.00  9.20  \n    < g r a p h i c s >\n  0.00  0.00  0.00  2.50  6.22 -3*Diamond  AVG  [HTML]FFFFFF0.00±0.00  [HTML]FFFFFF0.35±0.48  [HTML]FFFFFF0.96±0.67  [HTML]FFFFFF2.42±1.01  8.99±2.68  \n    < g r a p h i c s >\n  0.00  2.00  0.00  10.00  22.78 -2*Redstone  AVG  [HTML]FFFFFF1.04±1.30  [HTML]FFFFFF1.14±1.18  [HTML]FFFFFF0.69±1.68  [HTML]FFFFFF6.02±3.61  17.51±9.34  \n    < g r a p h i c s >\n  16.67  33.33  43.33  53.33  86.67 -2*Blocks  AVG  [HTML]FFFFFF45.64±33.88  [HTML]FFFFFF49.35±30.51  [HTML]FFFFFF55.71±29.43  [HTML]FFFFFF58.02±27.68  80.34±21.09  \n    < g r a p h i c s >\n  6.67  0.00  10.00  10.00  30.30 -2*Armor  AVG  [HTML]FFFFFF1.36±2.25  [HTML]FFFFFF0.50±0.88  [HTML]FFFFFF3.10±4.71  [HTML]FFFFFF3.71±3.78  13.44±14.62  \n    < g r a p h i c s >\n  15.00  15.00  15.00  25.00  50.00 -2*Decoration  AVG  [HTML]FFFFFF17.12±11.59  [HTML]FFFFFF17.13±9.19  [HTML]FFFFFF12.03±10.19  [HTML]FFFFFF29.59±15.94  46.67±23.39  \n    < g r a p h i c s >\n  13.33  16.67  25.00  16.67  43.55 -2*Food  AVG  [HTML]FFFFFF9.40±4.29  [HTML]FFFFFF15.56±6.83  [HTML]FFFFFF20.78±11.99  [HTML]FFFFFF22.85±8.15  46.75±11.16  ",
      "subsections": [
        {
          "title": "Overview",
          "content": "<label. sec:overview> We aim to develop an agent capable of solving long-horizon instruction-following tasks using image observations and human-aligned actions. To accomplish this, we propose a multi-modal agent including an interactive planner, a goal-conditioned controller, and a multimodal memory of multimodal experiences. Upon receiving a task and the current observation, JARVIS-1first utilizes the MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences from the memory. These experiences will then be used along with the planning instruction to prompt the MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference plans, the planner will ultimately produce a series of K short-horizon goals g_1,… ,g_K to be executed by the controller. Once the plan is successfully executed, it will be stored in the memory along with the task and the agent situation when it was planned. We also empower JARVIS-1with life-long learning by combining self-instruct, where JARVIS-1will propose some tasks for itself to complete as a means of exploration; and self-improve, where multiple JARVIS-1agents will be running in parallel to gather experiences, therefore helping with better planning later. We provide an illustration in <ref>. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Interactive Planning with MLM",
          "content": "<label. sec:interactive_planning> As we have mentioned in <ref> and <ref>, the primary challenges for planning in Minecraft come from the requirement of being able to plan for long-horizon tasks under dynamic observations. Confirmed by many prior arts  <cit. deps,voyager,plan4mc>, this makes it exceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this end, we take a multimodal language model (MLM) as zero-shot planner and combine it with an interactive planning framework to tackle these challenges. <figure. fig:memory_retrieval - Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1will first think backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited depth. The sub-goal that is present in the memory will join the current visual observation to form the final query. Entries that match the text query will be ranked by the perceiving distance of their states to the obs query and only the top entry of each sub-goal will be retrieved. >Situation-aware planning with MLM.    To achieve situation-aware planning, the planner must take the current observation into account, in addition to the task instruction  <cit. huang2022language,react>. Specifically, we begin with translating the multimodal observation into text descriptions. As opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft items (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT  <cit. gpt3> to generate sentences that describe these observations. For example, a generated sentence could be \"I can see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according to current visual observation during planning. Additional situation details including biome and inventory status are also converted into text using templates. Finally, we prompt the MLM again (the language part only) into a plan given the task instruction and all the aforementioned textual situation descriptions. Compared to end-to-end alternatives  <cit. brohan2023rt,leo>, we find our composable usage of MLM provides higher quality situation descriptions and ultimately, plans with much less hallucination. Planning with self-check.    Our first layer of shield to ensure the correctness of plans involves self-check. Similar to self-debugging   <cit. selfdebug>, given an initial plan, we ask JARVIS-1to progressively simulate the plan execution, predict the resulting state after each step (primarily the state of inventory), and evaluate them. By verifying if these states satisfy the goal’s precondition, JARVIS-1can proactively identify potential plan flaws. Compared to the canonical planner where the agent has to encounter the error first before making a remedy, this upfront plan verification could mitigate the need for the agent to recover (re-plan) from more challenging situations due to plan failure. For instance, if an agent starts digging underground without sufficient wood, it would typically have to return to the surface, which substantially lowers the chance of completing the task. Planning with environment feedback.    Next, our interactive planning framework ventures into allowing JARVIS-1to quickly recover from failure by leveraging environment feedback in a closed-loop fashion. The process is illustrated in <ref>. During plan execution, we feed the feedback to the MLM of JARVIS-1in case there is any execution failure (possibly due to a flawed plan) and utilize its self-explain mechanism  <cit. reflexion> to explain the error and locate the bugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1will produce an improved plan based on both the outside environment feedback and the inside retrospective. Compared to other agents that rely on human intervention or privileged environment information  <cit. innermonologue,gitm>, JARVIS-1has the ability to speculate about the reasons why current goals cannot be achieved, without the need for additional information or design. ",
          "subsections": [],
          "figures": {
            "fig:memory_retrieval": {
              "label": "fig:memory_retrieval",
              "path": [
                "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/retrieval_and_reason_1019.pdf"
              ],
              "size": [
                {
                  "scale": 0.5,
                  "height": null,
                  "width": null
                }
              ],
              "caption": "Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1will first think backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited depth. The sub-goal that is present in the memory will join the current visual observation to form the final query. Entries that match the text query will be ranked by the perceiving distance of their states to the obs query and only the top entry of each sub-goal will be retrieved. ",
              "section": "3.2. Interactive Planning with MLM"
            }
          }
        },
        {
          "title": "Planning with Multimodal Memory in the Loop",
          "content": "<label. sec:planning_with_memory> To address the life-long learning challenge mentioned in <ref>, we equip JARVIS-1with multimodal memory to allow learning from its own past experiences. We will detail the formulation of the retrieval-augmented planning, query generation, and memory layout below. Retrival-augmented planning. Retrieval-augmented generation (RAG)  <cit. lewis2020retrieval,mao2020generation> enhances the quality of responses generated by LLMs by incorporating external sources of knowledge to complement the model’s internal representation. We also utilize RAG to enhance JARVIS-1’s long-term planning capability. Compared to official RAG methods leveraging the external knowledge library, we take the collected multimodal memory as the knowledge library and retrieve the interactive experiences as the demonstration prompt to augment the planning results. The formulation is as follows: \n    p(y | x) ≈∑ _z∈top-k(p(·| x))p_η (z | x) p_θ (y | x,z),\n where x, y, and z denote instruction, plans, and retrieved memory entries respectively, and p_η and p_θ are denoted as retrieval and planning models. Such retrieval-augmented planning method helps JARVIS-1ground the internal knowledge into the open-ended environments efficiently and leverage the historical interaction feedback to solve the hallucination within LLMs and produce more accurate plans. Multimodal memory.    We have demonstrated the layout of our multimodal memory on the right side of Figure  <ref. fig:memory_retrieval>. From a high level, it is a key-value memory where the keys are multimodal, comprising both the task and the observation (or situation) made when this memory entry was created. The values are the plans that were successfully executed. Note that, since the plans in an open-world environment like Minecraft are situated (see <ref>), there could be multiple entries that are with the same task but different observations and plans. As a result, JARVIS-1needs to produce multimodal queries based on the current task and situations to retrieve the relevant memory entries. Query generation via reasoning.     When presented with an instruction as a task, we employ query generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks, which will then be used as textual queries to retrieve relevant planning experiences as references for solving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty inventory\" as shown in Figure  <ref. fig:memory_retrieval>. JARVIS-1queries the MLMs to identify the tasks that are required for achieving the main task in a backward search fashion, e.g., “obtain book \n    < g r a p h i c s >\n/diamond \n    < g r a p h i c s >\n/obsidian \n    < g r a p h i c s >\n with empty inventory”. The search depth is bounded for efficiency. Further, instead of relying solely on retrieval based on the text query  <cit. voyager,gitm>, we also propose to append the agent’s current visual observation to the textual query, resulting in a multimodal query to take the situation into account during memory retrieval. Multimodal retrieval. After obtaining the textual and visual query, we compute the alignment between the query and each trajectory in multimodal memory. We first use the text encoder of CLIP model to compute the embedding of the query and task key of each entry in memory. We select the memory entries with similarity higher than the confidence threshold as the candidate entries. Then we will compute the visual state embedding of query and states in candidate entires. Then we sort the candidate entries with the visual embedding similarities, which can be formed as: \n    p_η (z| x) ∝   CLIP_v(s_z)^⊤CLIP_v(s_x),\n where s_z and s_x are the visual key of memory entries and visual query, respectively. Finally, we retrieve the plan of top-k candidate entries as reference prompt z.  Characteristics of 11 task groups encompassing over 200 minecraft tasks. <label. tab:mc_task_set> 0.950.0pt!   Group Task Num.Max.StepsInitial Inventory Biome  Language Instruction Wood  34  12k  null  Plains/Forest  Pick up a wooden_ pickaxe. Wood-Variants  43  12k  null Savanna/Jungle/Taiga Pick up a acacia_ boat. Stone  10  12k  iron_ axe  Plains/Forest  Craft a furnace given an iron axe. Iron  22  12k  iron_ axe  Plains/Forest  Smelt and craft an iron_ door given an iron axe. Gold  9  36k  iron_ axe  Plains/Forest  Smelt and craft an golden_ axe given an iron axe. Diamond  7  36k  iron_ axe  Plains/Forest Dig down to mine diamond and craft diamond_ pickaxe.Redstone  7  36k  iron_ axe  Plains/Forest  Mine redstone and make dropper given an iron axe. Blocks  15  12-36k  iron_ axe  Plains/Forest  Dig down to mine lapis_ lazuli block. Armor  17  12-36k  iron_ axe  Plains/Forest  Craft diamond_ boots given an iron axe and equip it. Decoration  17  12k  iron_ axe  Flower Forest  Obtain the bed and dye it red. Food  9  12k  iron_ axe  Plains Kill sheep to obtain mutton and cook it. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Self-improving Agents",
          "content": "<label. sec:learning> Learning in Minecraft with memory.    The remaining issue now is where the aforementioned multimodal memory comes from. Inspired by the life-long learning scheme in many close-world and open-world reinforcement learning problems  <cit. lifelong1,lifelong2,voyager>, we propose the following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set of tasks, which form some curricula for the agents to complete as means of exploration of the world. During this process, JARVIS-1produces plans, interacts with the environment, embraces the errors, and stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1on various tasks. Therefore, JARVIS-1is able to produce better plans with the memory teaming up with the planning experiences. In our experiments, we use this as the default setting for all tasks. Exploration using self-instruct.    The key issue to the success of learning with memory is how to effectively acquire useful experiences given a limited amount of time. We propose to use self-instruct  <cit. selfinstruct> to generate the dynamic curriculum and guide JARVIS-1to learn from the interactions with environments. In each round, we prompt the MLM to consider how capable JARVIS-1is at this point and subsequently select tasks from a task pool to explore. We find that the curriculum almost follows the technical tree-growing direction. To accelerate the learning process, we augment the linear self-instruct to distributed learning in distributed environments with shared memory, i.elet@token .speculative execution  <cit. leviathan2023fast>. Specifically, we generate multiple executable tasks as candidate task batches and provide them to agents with the same memory for verification and execution in various different environments. Meanwhile, experiences are collected into a shared centralized memory. When all exploration tasks have been accomplished, we move to the next round, until the memory reaches a certain capacity. Life-long learning.    We’ve also observed that the aforementioned learning (where the memory is being filled) can be extended throughout the whole gameplay, where the agent gradually acquires more and more skills. As the gameplay continues, more and more experiences are pouring in, therefore JARVIS-1can find better references for challenging tasks like ObtainDiamondPickaxe, resulting in an improved success rate on these tasks. Further, there is no gradient update in this thanks to the memory-augmented MLM, i.elet@token .we can do in-context life-long learning. In Section  <ref. sec:long_horizon_experiments>, we offer exploratory experiments to show the potential of such capability of JARVIS-1.  Results of JARVIS-1and baselines on Minecraft. The detailed task instructions, settings and results can be found in the Appendix. <label. tab:main_results> 0.80.0pt!   Group  Task  GPT  ReAct  Inner Monologue  DEPS  JARVIS-1  \n    < g r a p h i c s >\n  26.67  45.00  36.67  75.00  91.55 -2*Wood  AVG  27.30±14.86  40.31±13.30  60.15±19.41  80.23±17.32  88.84±16.82  \n    < g r a p h i c s >\n  6.67  36.67  30.00  36.67  60.47 -2*WoodVar  AVG  [HTML]FFFFFF24.39±11.08  [HTML]FFFFFF38.13±12.81  [HTML]FFFFFF53.39±12.86  [HTML]FFFFFF68.75±12.32  76.78±12.27  \n    < g r a p h i c s >\n  20.00  20.00  66.67  75.00  94.20 -2*Stone  AVG  [HTML]FFFFFF20.21±12.32  [HTML]FFFFFF39.00±12.15  [HTML]FFFFFF52.86±16.90  [HTML]FFFFFF69.27±7.78  88.69±4.87  \n    < g r a p h i c s >\n  0.00  0.00  3.33  20.00  33.82  \n    < g r a p h i c s >\n  3.33  6.67  0.00  20.00  38.10 -3*Iron  AVG  [HTML]FFFFFF3.27±2.85  [HTML]FFFFFF4.61±3.63  [HTML]FFFFFF5.20±5.17  [HTML]FFFFFF16.92±4.69  34.63±10.61  \n    < g r a p h i c s >\n  0.00  2.00  2.00  6.00  14.49 -2*Gold  AVG  [HTML]FFFFFF0.00±0.00  [HTML]FFFFFF0.45±0.60  [HTML]FFFFFF0.59±0.64  [HTML]FFFFFF2.20±1.55  6.85±4.71  \n    < g r a p h i c s >\n  0.00  0.00  1.00  2.00  9.20  \n    < g r a p h i c s >\n  0.00  0.00  0.00  2.50  6.22 -3*Diamond  AVG  [HTML]FFFFFF0.00±0.00  [HTML]FFFFFF0.35±0.48  [HTML]FFFFFF0.96±0.67  [HTML]FFFFFF2.42±1.01  8.99±2.68  \n    < g r a p h i c s >\n  0.00  2.00  0.00  10.00  22.78 -2*Redstone  AVG  [HTML]FFFFFF1.04±1.30  [HTML]FFFFFF1.14±1.18  [HTML]FFFFFF0.69±1.68  [HTML]FFFFFF6.02±3.61  17.51±9.34  \n    < g r a p h i c s >\n  16.67  33.33  43.33  53.33  86.67 -2*Blocks  AVG  [HTML]FFFFFF45.64±33.88  [HTML]FFFFFF49.35±30.51  [HTML]FFFFFF55.71±29.43  [HTML]FFFFFF58.02±27.68  80.34±21.09  \n    < g r a p h i c s >\n  6.67  0.00  10.00  10.00  30.30 -2*Armor  AVG  [HTML]FFFFFF1.36±2.25  [HTML]FFFFFF0.50±0.88  [HTML]FFFFFF3.10±4.71  [HTML]FFFFFF3.71±3.78  13.44±14.62  \n    < g r a p h i c s >\n  15.00  15.00  15.00  25.00  50.00 -2*Decoration  AVG  [HTML]FFFFFF17.12±11.59  [HTML]FFFFFF17.13±9.19  [HTML]FFFFFF12.03±10.19  [HTML]FFFFFF29.59±15.94  46.67±23.39  \n    < g r a p h i c s >\n  13.33  16.67  25.00  16.67  43.55 -2*Food  AVG  [HTML]FFFFFF9.40±4.29  [HTML]FFFFFF15.56±6.83  [HTML]FFFFFF20.78±11.99  [HTML]FFFFFF22.85±8.15  46.75±11.16  ",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {
        "fig:planner": {
          "label": "fig:planner",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/interactive_planner.pdf"
          ],
          "size": [
            {
              "scale": 0.45,
              "height": null,
              "width": null
            }
          ],
          "caption": " Interactive planning in JARVIS-1. After receiving the current task instruction and observation, JARVIS-1will produce an initial plan, which will go through self-check to get possible bugs (marked in colorred) fixed. Further, in case any error (also marked in colorred) occurs during the execution of the refined plan, JARVIS-1will try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check and self-explain significantly boosts the correctness and robustness of JARVIS-1planning. ",
          "section": "3. Multi-task Agent with Memory-Augmented MLM"
        }
      }
    },
    {
      "title": "Experiments",
      "content": "In the experiments, our goal is to 1) evaluate the general performances of JARVIS-1on the challenging Minecraft tasks, especially on its advantages over baselines that do not (fully) address the aforementioned issues in open-world agents; 2) understand the factors that contributes to the general results; 3) explore the potential of JARVIS-1in terms of life-long learning and its benefits to long-horizon tasks. To this end, we will first briefly introduce the evaluation settings, then cover the main comparative results and ablation studies, and conclude with an exploratory trial on long-horizon tasks. \n\nExperimental Setups\n-------------------\n<label. sec:task_setting> We evaluate JARVIS-1in Minecraft, with tasks selected from the recently introduced Minecraft Universe Benchmark  <cit. mcu>. For the reader’s convenience, we provide details on the basic setups below. Environment setting.    To ensure realistic gameplay, the agent needs to utilize observation and action spaces that are similar to those used by humans. Instead of manually designing a custom interface for models to interact with the environment, as done in previous methods such as MineDojo<cit. minedojo>, GITM<cit. gitm>, and Voyager<cit. voyager>, we opt for using the native human interface provided by Minecraft. This applies to both the observation and action space. The model operates at a speed of 20 frames per second and is required to use a mouse and keyboard interface when interacting with human GUIs. For more information on the detailed descriptions of the observation and action spaces, please refer to the Appendix. Task setting.     In Minecraft, players have access to thousands of items, each with specific acquisition requirements or recipes. For example, stone-type items can only be obtained using a pickaxe, and two planks can be crafted into four sticks (these requirements are available on the Minecraft Wikihttps://minecraft.fandom.com/wiki/Minecraft_ Wiki). In survival mode, players must obtain each type of item from the environment or craft/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe Benchmark  <cit. mcu> for evaluation. These tasks are related to items that can be obtained in the Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups according to recommended categorieshttps://minecraft.fandom.com/wiki/Tutorials/Organization# Categories in Minecraft (see Table<ref. tab:mc_task_set>). Due to the varying complexity of these tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is determined by the average time the human players need to accomplish the corresponding task. Other details about each task, such as language instruction, maximum steps, evaluation times, biome, and initial inventory when the agent is born into the world can be found in Appendix Table  <ref. tab:wood_results>-<ref. tab:food_results>. Evaluation metrics.     By default, the agent always starts in survival mode, with an empty inventory. A task is considered a success when the target object is obtained within a specified time. Due to the open-world nature of Minecraft, the world and initial position that the agent is spawned at could vary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported the average success rate to ensure a thorough assessment. Further, since we categorize the tasks into groups, we also report mean and variance values for each group for ease of presentation. \n\nMain Results\n------------\nWe compare JARVIS-1with other multi-task instruction-following agents based on LLM, including Instruct GPT   <cit. huang2022language,instructGPT>, ReAct   <cit. react>, Inner Monologue   <cit. innermonologue>, DEPS   <cit. deps>. Since some methods are not originally experimented in Minecraft, we reproduce them to conform to the Minecraft specification based on prompt and feedback template design. All LLM-based methods access the LLM model through OpenAI API. And all hyper-parameters of LLM including temperature are kept as default. The average success rates for every task group are listed in Table  <ref. tab:main_results>. JARVIS-1achieves the best performance with all meta tasks. It is important to note that in Minecraft, the technology tree can be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly difficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or a diamond, the agents typically need to perform more actions and longer goal sequences in order to complete the task. As a result, the success rate of all agents decreases as the difficulty level increases. It is evident that reasoning methods (ReAct   <cit. react> vs. GPT   <cit. instructGPT,huang2022language>) and interactive re-planning with feedback (Inner Monologue   <cit. innermonologue> vs. GPT) effectively enhance the agent’s task performance in an open world. However, these approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron and Diamond group. DEPS   <cit. deps>, on the other hand, enables agents to accomplish diamond-related tasks through interactive long-horizon planning accompanied by descriptions and explanations. Nevertheless, its reliability remains very low at approximately 2.5% . In comparison to DEPS   <cit. deps> without memory, JARVIS-1demonstrates superior performance even in challenging tasks due to its extensive experience. In diamond-related tasks specifically, the success rate has increased by nearly 3 times (8.99%  vs 2.42% ). And JARVIS-1usually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas DEPS requires more than 6 rounds. This means that JARVIS-1saves a significant amount of LLM tokens and thinking time, enabling more efficient plan execution and providing additional steps and tokens for handling uncertainty in the environment. Based on our observations, we have found that the bottleneck for JARVIS-1in tasks involving diamonds often lies with the Controller’s inability to perfectly execute short-horizon text instructions generated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for the controller to execute or improving the controller’s ability to follow instructions. \n\nAblation Studies\n----------------\n<figure. fig:memory_size -  Success rate by memory size for different items. We evaluated the performance of JARVIS-1at different memory sizes (representing different learning stages) by measuring the success rate (%  Episodes) of completing key items on the Minecraft technology tree. As the learning progressed, we observed an improvement in completion rates for all items, with an increasing number of successful trajectories being included in memory. After 4 epochs of learning, JARVIS-1had accumulated a total of 425 successful trajectories in its memory. >\n\nJARVIS-1based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT  <cit. instructGPT> and GPT-4  <cit. gpt4>. Among these models, GPT-4 has more parameters and has been proven to outperform ChatGPT in extensive research  <cit. voyager>. We also select the open-source pre-trained LLaMA2 70B (LLaMA2 PT) model  <cit. llama2>. Additionally, we gathered a substantial amount of Minecraft-related text from the internet as training data and further fine-tuned LLaMA2 13B (LLaMA FT). The experiments were conducted on a subset of Minecraft tasks using different language models. Each JARVIS-1learns for 4 epochs of interaction with all task sets and evaluates on task subset across at least 20 seeds. The experimental results are presented in Table  <ref. tab:ablation_lm>. \n\n\n\n===================================================\n\n\n\n\n\n\n\n\n\n\nTable: Success rates for different LLMs on Minecraft tasks.\n\n\n\n\n\n=====================================================================================================================================\n\n\n\n\nTable: Success rates for memory ablation on Minecraft tasks.\n\n<figure. fig:long_horizon -  (Left) The success rate of different models in the  challenge over gameplay time. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-1agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it takes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of obtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1. This task has been evaluated over 300 times on different seeds. These curves indicate that as the game progresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1is constantly improving its skills. >Table  <ref. tab:ablation_lm> demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical success rates as GPT-4. This suggests that language models equipped with memory can significantly enhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B exhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks. However, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves substantially. This indicates that the open-source model lacks knowledge specific to Minecraft and requires further finetuning for successful completion of such tasks. \n\nAblation on Memory\n <label. sec:ablation_memory> We also conduct ablation experiments on the multimodality memory and retrieval methods. We set JARVIS-1w/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance with different memory size (representing different learning stages) as shown in Figure  <ref. fig:memory_size>, which demonstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on a subset of Minecraft tasks using three different retrieval methods: retrieval with instruction embedding only (T), reasoning + retrieval with text embedding (T+R), and reasoning + retrieval with multimodality embedding (M+R). Except the memory and retrieval methods, all others are kept same. The results are listed in Table  <ref. tab:ablation_memory>. The experiments show that reasoning before retrieval can effectively improve retrieval accuracy. And retrieval based on multimodal state including vision observation and symbolic information (e.g., inventory, location etc) is better than only considering the text embedding. \n\nLong-Horizon Challenges\n-----------------------\n<label. sec:long_horizon_experiments> Most concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-horizon tasks like CraftingDiamondPickaxe. The VPT foundation model   <cit. vpt> is capable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions. To address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model for specific task completion. However, after fine-tuning, VPT may experience a decline in performance for other tasks while focusing on the specified task. In contrast, Steve-1   <cit. steve1> has implemented goal-conditioned fine-tuning on VPT-earlygame, enabling it to follow human text instructions while maintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like obtaining dirt, collecting flowers and chopping trees. When it comes to long-horizon tasks such as starting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties. DEPS   <cit. deps> also utilizes LLM as a planner, but it lacks the ability to learn from experience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in its re-planning rounds due to the LM’s context constraints. The experiments reveal that DEPS has a success rate of less than 50%  in generating accurate and executable plans for acquiring diamonds. The probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59% . Consequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within the Minecraft world. Even human players who have mastered the distribution pattern of diamonds achieve success rates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds) within 10 minutes at approximately 15%  and 12% , respectively. JARVIS-1performs better in the ObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone RL-finetuned VPT, JARVIS-1has more than doubled the success rate of obtaining a diamond pickaxe (6.22%  vs 2.5%  within 20 minutes). To increase the chances of obtaining diamonds, we extended the game playing time to 60 minutes (72000 game-playing steps, as shown in Figure  <ref. fig:long_horizon>). As a result, JARVIS-1’s success rate in acquiring a diamond pickaxe improved from 6.2%  to 12.5% . The graph on the right side of Figure 7 illustrates how the success rate of intermediate milestone items changes over time, indicating that JARVIS-1tends to improve with longer game-playing time. We also conduct two variants of JARVIS-1with different self-improving curriculum: human-written and random-generated. All three JARVIS-1have collect experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes. The results show that JARVIS-1with GPT-generated curriculum can finish the task within the shortest game-playing steps and achieve the best performance in 60 minutes. In contrast, VPT’s success rate barely changed when we increased the time from 20 minutes to 60 minutes (from 2.5%  to 3% ). This can be attributed to Minecraft’s durability system where prolonged underground exploration often leads to pickaxe damage. When JARVIS-1’s pickaxe breaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL exhibits perplexing behaviors at this stage by using inappropriate tools for mining stone or crafting unnecessary items. This comparison demonstrates that JARVIS-1possesses superior generalization and planning abilities for long-horizon tasks. Note that our method is designed to be multi-task in its nature and not finetuned through imitation learning on specific dataset or reinforcement learning. ",
      "subsections": [
        {
          "title": "Experimental Setups",
          "content": "<label. sec:task_setting> We evaluate JARVIS-1in Minecraft, with tasks selected from the recently introduced Minecraft Universe Benchmark  <cit. mcu>. For the reader’s convenience, we provide details on the basic setups below. Environment setting.    To ensure realistic gameplay, the agent needs to utilize observation and action spaces that are similar to those used by humans. Instead of manually designing a custom interface for models to interact with the environment, as done in previous methods such as MineDojo<cit. minedojo>, GITM<cit. gitm>, and Voyager<cit. voyager>, we opt for using the native human interface provided by Minecraft. This applies to both the observation and action space. The model operates at a speed of 20 frames per second and is required to use a mouse and keyboard interface when interacting with human GUIs. For more information on the detailed descriptions of the observation and action spaces, please refer to the Appendix. Task setting.     In Minecraft, players have access to thousands of items, each with specific acquisition requirements or recipes. For example, stone-type items can only be obtained using a pickaxe, and two planks can be crafted into four sticks (these requirements are available on the Minecraft Wikihttps://minecraft.fandom.com/wiki/Minecraft_ Wiki). In survival mode, players must obtain each type of item from the environment or craft/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe Benchmark  <cit. mcu> for evaluation. These tasks are related to items that can be obtained in the Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups according to recommended categorieshttps://minecraft.fandom.com/wiki/Tutorials/Organization# Categories in Minecraft (see Table<ref. tab:mc_task_set>). Due to the varying complexity of these tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is determined by the average time the human players need to accomplish the corresponding task. Other details about each task, such as language instruction, maximum steps, evaluation times, biome, and initial inventory when the agent is born into the world can be found in Appendix Table  <ref. tab:wood_results>-<ref. tab:food_results>. Evaluation metrics.     By default, the agent always starts in survival mode, with an empty inventory. A task is considered a success when the target object is obtained within a specified time. Due to the open-world nature of Minecraft, the world and initial position that the agent is spawned at could vary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported the average success rate to ensure a thorough assessment. Further, since we categorize the tasks into groups, we also report mean and variance values for each group for ease of presentation. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Main Results",
          "content": "We compare JARVIS-1with other multi-task instruction-following agents based on LLM, including Instruct GPT   <cit. huang2022language,instructGPT>, ReAct   <cit. react>, Inner Monologue   <cit. innermonologue>, DEPS   <cit. deps>. Since some methods are not originally experimented in Minecraft, we reproduce them to conform to the Minecraft specification based on prompt and feedback template design. All LLM-based methods access the LLM model through OpenAI API. And all hyper-parameters of LLM including temperature are kept as default. The average success rates for every task group are listed in Table  <ref. tab:main_results>. JARVIS-1achieves the best performance with all meta tasks. It is important to note that in Minecraft, the technology tree can be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly difficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or a diamond, the agents typically need to perform more actions and longer goal sequences in order to complete the task. As a result, the success rate of all agents decreases as the difficulty level increases. It is evident that reasoning methods (ReAct   <cit. react> vs. GPT   <cit. instructGPT,huang2022language>) and interactive re-planning with feedback (Inner Monologue   <cit. innermonologue> vs. GPT) effectively enhance the agent’s task performance in an open world. However, these approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron and Diamond group. DEPS   <cit. deps>, on the other hand, enables agents to accomplish diamond-related tasks through interactive long-horizon planning accompanied by descriptions and explanations. Nevertheless, its reliability remains very low at approximately 2.5% . In comparison to DEPS   <cit. deps> without memory, JARVIS-1demonstrates superior performance even in challenging tasks due to its extensive experience. In diamond-related tasks specifically, the success rate has increased by nearly 3 times (8.99%  vs 2.42% ). And JARVIS-1usually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas DEPS requires more than 6 rounds. This means that JARVIS-1saves a significant amount of LLM tokens and thinking time, enabling more efficient plan execution and providing additional steps and tokens for handling uncertainty in the environment. Based on our observations, we have found that the bottleneck for JARVIS-1in tasks involving diamonds often lies with the Controller’s inability to perfectly execute short-horizon text instructions generated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for the controller to execute or improving the controller’s ability to follow instructions. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Ablation Studies",
          "content": "<figure. fig:memory_size -  Success rate by memory size for different items. We evaluated the performance of JARVIS-1at different memory sizes (representing different learning stages) by measuring the success rate (%  Episodes) of completing key items on the Minecraft technology tree. As the learning progressed, we observed an improvement in completion rates for all items, with an increasing number of successful trajectories being included in memory. After 4 epochs of learning, JARVIS-1had accumulated a total of 425 successful trajectories in its memory. >\n\nJARVIS-1based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT  <cit. instructGPT> and GPT-4  <cit. gpt4>. Among these models, GPT-4 has more parameters and has been proven to outperform ChatGPT in extensive research  <cit. voyager>. We also select the open-source pre-trained LLaMA2 70B (LLaMA2 PT) model  <cit. llama2>. Additionally, we gathered a substantial amount of Minecraft-related text from the internet as training data and further fine-tuned LLaMA2 13B (LLaMA FT). The experiments were conducted on a subset of Minecraft tasks using different language models. Each JARVIS-1learns for 4 epochs of interaction with all task sets and evaluates on task subset across at least 20 seeds. The experimental results are presented in Table  <ref. tab:ablation_lm>. \n\n\n\n===================================================\n\n\n\n\n\n\n\n\n\n\nTable: Success rates for different LLMs on Minecraft tasks.\n\n\n\n\n\n=====================================================================================================================================\n\n\n\n\nTable: Success rates for memory ablation on Minecraft tasks.\n\n<figure. fig:long_horizon -  (Left) The success rate of different models in the  challenge over gameplay time. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-1agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it takes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of obtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1. This task has been evaluated over 300 times on different seeds. These curves indicate that as the game progresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1is constantly improving its skills. >Table  <ref. tab:ablation_lm> demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical success rates as GPT-4. This suggests that language models equipped with memory can significantly enhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B exhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks. However, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves substantially. This indicates that the open-source model lacks knowledge specific to Minecraft and requires further finetuning for successful completion of such tasks. \n\nAblation on Memory\n <label. sec:ablation_memory> We also conduct ablation experiments on the multimodality memory and retrieval methods. We set JARVIS-1w/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance with different memory size (representing different learning stages) as shown in Figure  <ref. fig:memory_size>, which demonstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on a subset of Minecraft tasks using three different retrieval methods: retrieval with instruction embedding only (T), reasoning + retrieval with text embedding (T+R), and reasoning + retrieval with multimodality embedding (M+R). Except the memory and retrieval methods, all others are kept same. The results are listed in Table  <ref. tab:ablation_memory>. The experiments show that reasoning before retrieval can effectively improve retrieval accuracy. And retrieval based on multimodal state including vision observation and symbolic information (e.g., inventory, location etc) is better than only considering the text embedding. ",
          "subsections": [
            {
              "title": "JARVIS-1based on different LMs",
              "content": "We conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT  <cit. instructGPT> and GPT-4  <cit. gpt4>. Among these models, GPT-4 has more parameters and has been proven to outperform ChatGPT in extensive research  <cit. voyager>. We also select the open-source pre-trained LLaMA2 70B (LLaMA2 PT) model  <cit. llama2>. Additionally, we gathered a substantial amount of Minecraft-related text from the internet as training data and further fine-tuned LLaMA2 13B (LLaMA FT). The experiments were conducted on a subset of Minecraft tasks using different language models. Each JARVIS-1learns for 4 epochs of interaction with all task sets and evaluates on task subset across at least 20 seeds. The experimental results are presented in Table  <ref. tab:ablation_lm>. \n\n\n\n===================================================\n\n\n\n\n\n\n\n\n\n\nTable: Success rates for different LLMs on Minecraft tasks.\n\n\n\n\n\n=====================================================================================================================================\n\n\n\n\nTable: Success rates for memory ablation on Minecraft tasks.\n\n<figure. fig:long_horizon -  (Left) The success rate of different models in the  challenge over gameplay time. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-1agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it takes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of obtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1. This task has been evaluated over 300 times on different seeds. These curves indicate that as the game progresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1is constantly improving its skills. >Table  <ref. tab:ablation_lm> demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical success rates as GPT-4. This suggests that language models equipped with memory can significantly enhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B exhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks. However, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves substantially. This indicates that the open-source model lacks knowledge specific to Minecraft and requires further finetuning for successful completion of such tasks. ",
              "subsections": [],
              "figures": {
                "fig:long_horizon": {
                  "label": "fig:long_horizon",
                  "path": [
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/success_rate_vs_steps.pdf"
                  ],
                  "size": [
                    {
                      "scale": 0.55,
                      "height": null,
                      "width": null
                    }
                  ],
                  "caption": " (Left) The success rate of different models in the  challenge over gameplay time. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-1agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it takes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of obtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1. This task has been evaluated over 300 times on different seeds. These curves indicate that as the game progresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1is constantly improving its skills. ",
                  "section": "4.3.1. JARVIS-1based on different LMs"
                }
              }
            },
            {
              "title": "Ablation on Memory",
              "content": " <label. sec:ablation_memory> We also conduct ablation experiments on the multimodality memory and retrieval methods. We set JARVIS-1w/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance with different memory size (representing different learning stages) as shown in Figure  <ref. fig:memory_size>, which demonstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on a subset of Minecraft tasks using three different retrieval methods: retrieval with instruction embedding only (T), reasoning + retrieval with text embedding (T+R), and reasoning + retrieval with multimodality embedding (M+R). Except the memory and retrieval methods, all others are kept same. The results are listed in Table  <ref. tab:ablation_memory>. The experiments show that reasoning before retrieval can effectively improve retrieval accuracy. And retrieval based on multimodal state including vision observation and symbolic information (e.g., inventory, location etc) is better than only considering the text embedding. ",
              "subsections": [],
              "figures": {}
            }
          ],
          "figures": {
            "fig:memory_size": {
              "label": "fig:memory_size",
              "path": [
                "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/jarvis_1/source/figures/memory_size.pdf"
              ],
              "size": [
                {
                  "scale": 0.5,
                  "height": null,
                  "width": null
                }
              ],
              "caption": " Success rate by memory size for different items. We evaluated the performance of JARVIS-1at different memory sizes (representing different learning stages) by measuring the success rate (%  Episodes) of completing key items on the Minecraft technology tree. As the learning progressed, we observed an improvement in completion rates for all items, with an increasing number of successful trajectories being included in memory. After 4 epochs of learning, JARVIS-1had accumulated a total of 425 successful trajectories in its memory. ",
              "section": "4.3. Ablation Studies"
            }
          }
        },
        {
          "title": "Long-Horizon Challenges",
          "content": "<label. sec:long_horizon_experiments> Most concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-horizon tasks like CraftingDiamondPickaxe. The VPT foundation model   <cit. vpt> is capable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions. To address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model for specific task completion. However, after fine-tuning, VPT may experience a decline in performance for other tasks while focusing on the specified task. In contrast, Steve-1   <cit. steve1> has implemented goal-conditioned fine-tuning on VPT-earlygame, enabling it to follow human text instructions while maintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like obtaining dirt, collecting flowers and chopping trees. When it comes to long-horizon tasks such as starting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties. DEPS   <cit. deps> also utilizes LLM as a planner, but it lacks the ability to learn from experience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in its re-planning rounds due to the LM’s context constraints. The experiments reveal that DEPS has a success rate of less than 50%  in generating accurate and executable plans for acquiring diamonds. The probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59% . Consequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within the Minecraft world. Even human players who have mastered the distribution pattern of diamonds achieve success rates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds) within 10 minutes at approximately 15%  and 12% , respectively. JARVIS-1performs better in the ObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone RL-finetuned VPT, JARVIS-1has more than doubled the success rate of obtaining a diamond pickaxe (6.22%  vs 2.5%  within 20 minutes). To increase the chances of obtaining diamonds, we extended the game playing time to 60 minutes (72000 game-playing steps, as shown in Figure  <ref. fig:long_horizon>). As a result, JARVIS-1’s success rate in acquiring a diamond pickaxe improved from 6.2%  to 12.5% . The graph on the right side of Figure 7 illustrates how the success rate of intermediate milestone items changes over time, indicating that JARVIS-1tends to improve with longer game-playing time. We also conduct two variants of JARVIS-1with different self-improving curriculum: human-written and random-generated. All three JARVIS-1have collect experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes. The results show that JARVIS-1with GPT-generated curriculum can finish the task within the shortest game-playing steps and achieve the best performance in 60 minutes. In contrast, VPT’s success rate barely changed when we increased the time from 20 minutes to 60 minutes (from 2.5%  to 3% ). This can be attributed to Minecraft’s durability system where prolonged underground exploration often leads to pickaxe damage. When JARVIS-1’s pickaxe breaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL exhibits perplexing behaviors at this stage by using inappropriate tools for mining stone or crafting unnecessary items. This comparison demonstrates that JARVIS-1possesses superior generalization and planning abilities for long-horizon tasks. Note that our method is designed to be multi-task in its nature and not finetuned through imitation learning on specific dataset or reinforcement learning. ",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    },
    {
      "title": "Related Works",
      "content": "\n\nPlanning with LLM\n-----------------\nThere have been some methods leveraging the large language model to generate action plans for high-level tasks in embodied environments  <cit. socraticmodels,reporter,mai2023llm,liu2023llm+,proagent,rladapter,jxma_llm1>. <cit. huang2022language> decompose natural language commands into sequences of executable actions by text completion and semantic translation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances from value functions <cit. saycan>. Some methods also leverage the LLM to produce the program code as plan for better executation  <cit. progprompt,codeaspolicies,text2motion>. However, the above methods assume that the initial plan from the LLM is correct. When there are bugs in the initial plan, it’s difficult for the agent to finish the task successfully. Recent research frequently employs LLM as an interactive planner, harnessing its self-updating capabilities to enhance the plan’s executability over time  <cit. deps,reflexion,sun2023adaplanner>. Inner Monologue  <cit. innermonologue> pilots the front of interactive planning with LLMs, which introduces the feedback (including success detection and scene description) to the planner. However, we found it could still suffer from accumulative planning errors, especially in long-horizon open-world tasks. ReAct  <cit. react> will reason about the agent state before acting, which indicates that various reasoning methods  <cit. chainofthought,treeofthought,spring> are benefitial for planning. LLM-based planning methods often use the fixed pretrained LLM as the agent, while we focus more on life-long and continual learning for agents in open-world environments  <cit. continual1,continual2,voyager>. For better leveraging historical interaction between agent and environments, an explicit memory  <cit. generativeagents,gitm> for more historical chatting has been leveraged for bigger storage of agent experiences. However, the above methods usually rely only on a text-based environment and struggle to execute plans in partial-observed visual open-world environments. \n\nMinecraft Agents\n----------------\n Developing generally capable agents in Minecraft to solve open-world tasks has gained increasing interests  <cit. clip4mc,minedojo,vpt,shaofei,groot,rladapter,plan4mc,gitm>. As an early attempt, <cit. oh2017zero> studied task generalization in a simple Minecraft environment variant. It designed a two-stage pipeline, first mastering the prerequisite skills with parameterization trick, and then learning a meta controller to execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works <cit. oh2017zero,mao2022seihai,lin2021juewu> explored the hierarchical architecture. In recent years, influenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged, who are utilizing vast amounts of internet knowledge to train intelligent agents. <cit. minedojo> trained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles and video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy learning. <cit. vpt> utilizes a pre-trained inverse dynamics model to label actions in YouTube videos which are used to learn a foundation policy VPT through imitation learning. By bridging MineCLIP and VPT, <cit. steve1> creates a performant instruction-following policy Steve-1 to solve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1 can not solve complicated process-oriented tasks due to the expressive capability of its goal space. <cit. groot> learns to follow reference videos as the instruction by merely watching gameplay videos, which improves the capacity of goal space and reduces the cost of policy training. All of these methods focus on improving the smoothness and robustness of interaction between policy and environment. Inspired by the powerful language understanding and reasoning capabilities of large language models, researchers have begun to build Minecraft agents based on LLMs. <cit. voyager> used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills, making novel discoveries, and generating goal proposals. <cit. gitm> integrated LLM with text-based knowledge and memory to equip the agent with common sense and past experiences for higher reasoning efficiency. <cit. plan4mc> used LLM to guide the agent to explore the Minecraft world and interact with the environment with reinforcement learning control policies. ",
      "subsections": [
        {
          "title": "Planning with LLM",
          "content": "There have been some methods leveraging the large language model to generate action plans for high-level tasks in embodied environments  <cit. socraticmodels,reporter,mai2023llm,liu2023llm+,proagent,rladapter,jxma_llm1>. <cit. huang2022language> decompose natural language commands into sequences of executable actions by text completion and semantic translation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances from value functions <cit. saycan>. Some methods also leverage the LLM to produce the program code as plan for better executation  <cit. progprompt,codeaspolicies,text2motion>. However, the above methods assume that the initial plan from the LLM is correct. When there are bugs in the initial plan, it’s difficult for the agent to finish the task successfully. Recent research frequently employs LLM as an interactive planner, harnessing its self-updating capabilities to enhance the plan’s executability over time  <cit. deps,reflexion,sun2023adaplanner>. Inner Monologue  <cit. innermonologue> pilots the front of interactive planning with LLMs, which introduces the feedback (including success detection and scene description) to the planner. However, we found it could still suffer from accumulative planning errors, especially in long-horizon open-world tasks. ReAct  <cit. react> will reason about the agent state before acting, which indicates that various reasoning methods  <cit. chainofthought,treeofthought,spring> are benefitial for planning. LLM-based planning methods often use the fixed pretrained LLM as the agent, while we focus more on life-long and continual learning for agents in open-world environments  <cit. continual1,continual2,voyager>. For better leveraging historical interaction between agent and environments, an explicit memory  <cit. generativeagents,gitm> for more historical chatting has been leveraged for bigger storage of agent experiences. However, the above methods usually rely only on a text-based environment and struggle to execute plans in partial-observed visual open-world environments. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Minecraft Agents",
          "content": " Developing generally capable agents in Minecraft to solve open-world tasks has gained increasing interests  <cit. clip4mc,minedojo,vpt,shaofei,groot,rladapter,plan4mc,gitm>. As an early attempt, <cit. oh2017zero> studied task generalization in a simple Minecraft environment variant. It designed a two-stage pipeline, first mastering the prerequisite skills with parameterization trick, and then learning a meta controller to execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works <cit. oh2017zero,mao2022seihai,lin2021juewu> explored the hierarchical architecture. In recent years, influenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged, who are utilizing vast amounts of internet knowledge to train intelligent agents. <cit. minedojo> trained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles and video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy learning. <cit. vpt> utilizes a pre-trained inverse dynamics model to label actions in YouTube videos which are used to learn a foundation policy VPT through imitation learning. By bridging MineCLIP and VPT, <cit. steve1> creates a performant instruction-following policy Steve-1 to solve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1 can not solve complicated process-oriented tasks due to the expressive capability of its goal space. <cit. groot> learns to follow reference videos as the instruction by merely watching gameplay videos, which improves the capacity of goal space and reduces the cost of policy training. All of these methods focus on improving the smoothness and robustness of interaction between policy and environment. Inspired by the powerful language understanding and reasoning capabilities of large language models, researchers have begun to build Minecraft agents based on LLMs. <cit. voyager> used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills, making novel discoveries, and generating goal proposals. <cit. gitm> integrated LLM with text-based knowledge and memory to equip the agent with common sense and past experiences for higher reasoning efficiency. <cit. plan4mc> used LLM to guide the agent to explore the Minecraft world and interact with the environment with reinforcement learning control policies. ",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    },
    {
      "title": "Conclusion",
      "content": " We propose a multi-task agent JARVIS-1designed for the complex environment of Minecraft, marks a significant advancement in achieving human-like planning within an open-world setting. By leveraging pre-trained Multi-modal Language Models, JARVIS-1not only effectively interprets multimodal inputs but also adeptly translates them into actions. Its integration of a multimodal memory, which draws from both ingrained knowledge and real-time game experiences, enhances its decision-making capabilities. The empirical evidence of its prowess is evident in its impressive performance across a wide array of tasks in Minecraft. Notably, its achievement in the long-horizon diamond pickaxe task, where it achieved a completion rate that surpasses VPT by up to five times, underscores its potential and the strides made in this domain. This breakthrough sets the stage for the future of more versatile and adaptable agents in complex virtual environments. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Acknowledgments",
      "content": " This work is funded in part by the National Key R& D Program of China # 2022ZD0160301, a grant from CCF-Tencent Rhino-Bird Open Research Fund, NSF grants # IIS-1943641, # IIS-1956441, # CCF-1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI. The authors sincerely thank Dr. Rita Zhang, Zhixiang Dai at NVIDIA for the valuable technical support of GPU computing. gong2023mindagent,mialon2023augmented unsrtnat ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Implementation Details",
      "content": "<label. sec:implementation_details> \n\nController\n----------\n<label. sec:controller> Tasks in Minecraft are usually related to mine and craft goals. The mine goals require the agent to collect raw materials from the environment using the appropriate tools. The craft goals ask the agent to use the recipe to generate new items with existing materials in inventory. The mine goals are achieved through STEVE-1<cit. steve1> with text condition during implementation. The environment can directly executes the craft and smelt actions (craft/smelt with argument), which are same as MineDojo  <cit. minedojo> . \n\nInteractive Planner\n-------------------\nJARVIS-1relies on the Multi-modal Language Model for planning, self-checking, and self-explaining, and can accept three types of inputs: visual images, language, and symbolic information (including inventory, located position, home, current life statistics, etc.). Specifically, this is a hybrid model with language processing capabilities derived from the GPT model  <cit. gpt4>. The visual ability comes from MineCLIP  <cit. minedojo>. We collected approximately 1000 Minecraft text data from the internet and calculated the similarity between the current vision observation and these text data. Text above the similarity threshold will be selected into the GPT model’s prompt. Symbolic information is converted into natural language text through a designed template. All modalities are ultimately captured as language and processed by the GPT model. Different modules in JARVIS-1(e.g. self-check and self-explain) are completed through MLM based on different prompts. The specific prompt design are shown below.     backgroundcolor=, caption=Planning prompt in JARVIS-1, label=lst:planning_prompt   \n\nSystem: You are a helper agent in Minecraft. You need to generate the sequences of goals for a certain task in Minecraft. Just refer the history dialogue to give the plan consist of template. Do not explain or give any other instruction. \n\n==========\nUser: My current inventory has nothing. I current locate in plains. How to obtain 1 wooden_pickaxe in Minecraft step-by-step?\nAssistant: The code for obtaining 1 wooden_pickaxe is as follows:\ndef craft_wooden_axe(initial_inventory=):\n    mine(obj = \"log\":3, tool = None)  # step 1: chop down trees to mine logs\n    craft(obj = \"planks\":12, materials = \"log\":3, tool = None)   # step 2: craft 12 planks from 3 logs\n    craft(obj = \"stick\":4, materials = \"planks\":2, tool = None) # step 3: craft 4 sticks from 2 planks\n    craft(obj = \"crafting_table\":1, materials = \"planks\":4, tool = None) # step 4: craft 1 crafting_table from 4 planks\n    craft(obj = \"wooden_axe\":1, \"planks\": 3, \"stick\": 2, \"crafting_table\") # step 5: craft 1 wooden_axe from 3 planks and 2 sticks on crafting table\n    return \"wooden_axe\"\nUser: [Description] I succeed in step 1, 2, 3, 4, 5. \nI finish all steps and I obtain 1 wooden_axe successfully.\n\n==========\nUser: My current inventory has <inventory>. <visual observation>. How to obtain 1 stone_sword in Minecraft step-by-step?\nAssistant: \n\n    backgroundcolor=, caption=Goal parsing prompt in JARVIS-1, label=lst:parser_prompt   \n\nSystem: Extract the action name, action type, goal object, tool and action rank from the input text.\n\nUser: mine(\"log\":3, null); # step 1: chop down trees to mine logs\nAssistant:\nname: mine_log\ntext condition: chop down trees to mine logs\naction: mine\nobject_item: log\nobject_number: 3\ntool: null\nrank: 1\n###\n\ninput: craft(\"planks\":12, \"log\":3, null); # step 2: craft 12 planks from 3 log\nAssistant:\nname: craft_planks\ncondition: craft 12 planks from 3 log\naction: craft\nobject_item: planks\nobject_number: 12\nmaterials: \"log\":3\ntool: null\nrank: 2\n###\n\n\n\n\n    backgroundcolor=, caption=Self-explaining prompt in JARVIS-1, label=lst:explainer_prompt   \n\nSystem: Here are some actions that the agent fails to perform in Minecraft. Please give the explanation of action execution failure according to the current inventory information of the agent.\n\n###\nUser:\nFailed Action: mine(\"iron_ore\":1, null); # step 5: dig down to mine iron blocks\nCurrent Inventory: null\nAssistant: Because mining iron_ore needs to use the tool stone_pickaxe, but my inventory does not have stone_pickaxe. So I need to craft stone_pickaxe first.\n\n###\nUser:\nFailed Action: craft(\"stone_pickaxe\":1, \"cobblestone\":3, \"stick\":2, \"crafting_table\"); # step 1: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on crafting_table\nCurrent Inventory: null\nAssistant: Because crafting stone_pickaxe needs to have 3 cobblestone and 2 stick in inventory, but my inventory does not have cobblestone and stick. So I need to mine cobblestone and craft stick first.\n\n### \nUser:\nFailed Action: craft(\"stick\":4, \"planks\":2, null); # step 3: craft 4 stick from 2 planks first\nCurrent Inventory: null\nAssistant: Because crafting stick needs to have planks in inventory, but my inventory does not have planks. So I need to craft planks first.\n\n###\n\n\n    backgroundcolor=, caption=Self-checking prompt in JARVIS-1, label=lst:self_check_prompt   \n\nSystem: You are a helper agent in Minecraft. Check the plan whether can be finished.\n\n==========\nUser: My current inventory has nothing. I current locate in plains. My task is to obtain 1 wooden axe in Minecraft step-by-step. This is my plan:\ndef craft_wooden_axe(initial_inventory=):\n    mine(obj = \"log\":3, tool = None)  # step 1: chop down trees to mine logs\n    craft(obj = \"planks\":12, materials = \"log\":3, tool = None)   # step 2: craft 12 planks from 3 logs\n    craft(obj = \"stick\":4, materials = \"planks\":2, tool = None) # step 3: craft 4 sticks from 2 planks\n    craft(obj = \"wooden_axe\":1, \"planks\": 3, \"stick\": 2, null) # step 4: craft 1 wooden_axe from 3 planks and 2 sticks\n    return \"wooden_axe\"\n\nAssistant:\nMy current inventory is empty.\n1. I want to mine 3 logs. Mining logs do not require tools. My inventory will has 3 logs.\n2. I want to craft 12 planks from 3 logs. 1 log can produce 4 planks. My inventory will add 12 planks, and delete 3 logs. My inventory will has 12 planks.\n3. I want to craft 4 stick from 2 planks. 2 planks can produce 4 planks. My inventory will add 4 stick and delete 2 planks. My inventory will has 10 planks, 4 stick.\n4. I want to craft 1 wooden pickaxe from 3 planks and 2 sticks. Crafting wooden pickaxe requires crafting_table. But I do not have crafting_table in inventory. This action will failed. \nReturn: Step 4 will failed because of lacking of crafting_table.\n\n\n\nMemory\n------\nOur memory records every successful trajectory experience of JARVIS-1, including the task goals that the agent needs to execute, the actual goal sequence (plan) executed by the agent, and the state (visual observation and symbolic information returned from the environment) when the agent completes the task. In specific implementation, memory is a list where each trajectory experience is encoded as a dictionary, including the keys task, state, and plan. ",
      "subsections": [
        {
          "title": "Controller",
          "content": "<label. sec:controller> Tasks in Minecraft are usually related to mine and craft goals. The mine goals require the agent to collect raw materials from the environment using the appropriate tools. The craft goals ask the agent to use the recipe to generate new items with existing materials in inventory. The mine goals are achieved through STEVE-1<cit. steve1> with text condition during implementation. The environment can directly executes the craft and smelt actions (craft/smelt with argument), which are same as MineDojo  <cit. minedojo> . ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Interactive Planner",
          "content": "JARVIS-1relies on the Multi-modal Language Model for planning, self-checking, and self-explaining, and can accept three types of inputs: visual images, language, and symbolic information (including inventory, located position, home, current life statistics, etc.). Specifically, this is a hybrid model with language processing capabilities derived from the GPT model  <cit. gpt4>. The visual ability comes from MineCLIP  <cit. minedojo>. We collected approximately 1000 Minecraft text data from the internet and calculated the similarity between the current vision observation and these text data. Text above the similarity threshold will be selected into the GPT model’s prompt. Symbolic information is converted into natural language text through a designed template. All modalities are ultimately captured as language and processed by the GPT model. Different modules in JARVIS-1(e.g. self-check and self-explain) are completed through MLM based on different prompts. The specific prompt design are shown below.     backgroundcolor=, caption=Planning prompt in JARVIS-1, label=lst:planning_prompt   \n\nSystem: You are a helper agent in Minecraft. You need to generate the sequences of goals for a certain task in Minecraft. Just refer the history dialogue to give the plan consist of template. Do not explain or give any other instruction. \n\n==========\nUser: My current inventory has nothing. I current locate in plains. How to obtain 1 wooden_pickaxe in Minecraft step-by-step?\nAssistant: The code for obtaining 1 wooden_pickaxe is as follows:\ndef craft_wooden_axe(initial_inventory=):\n    mine(obj = \"log\":3, tool = None)  # step 1: chop down trees to mine logs\n    craft(obj = \"planks\":12, materials = \"log\":3, tool = None)   # step 2: craft 12 planks from 3 logs\n    craft(obj = \"stick\":4, materials = \"planks\":2, tool = None) # step 3: craft 4 sticks from 2 planks\n    craft(obj = \"crafting_table\":1, materials = \"planks\":4, tool = None) # step 4: craft 1 crafting_table from 4 planks\n    craft(obj = \"wooden_axe\":1, \"planks\": 3, \"stick\": 2, \"crafting_table\") # step 5: craft 1 wooden_axe from 3 planks and 2 sticks on crafting table\n    return \"wooden_axe\"\nUser: [Description] I succeed in step 1, 2, 3, 4, 5. \nI finish all steps and I obtain 1 wooden_axe successfully.\n\n==========\nUser: My current inventory has <inventory>. <visual observation>. How to obtain 1 stone_sword in Minecraft step-by-step?\nAssistant: \n\n    backgroundcolor=, caption=Goal parsing prompt in JARVIS-1, label=lst:parser_prompt   \n\nSystem: Extract the action name, action type, goal object, tool and action rank from the input text.\n\nUser: mine(\"log\":3, null); # step 1: chop down trees to mine logs\nAssistant:\nname: mine_log\ntext condition: chop down trees to mine logs\naction: mine\nobject_item: log\nobject_number: 3\ntool: null\nrank: 1\n###\n\ninput: craft(\"planks\":12, \"log\":3, null); # step 2: craft 12 planks from 3 log\nAssistant:\nname: craft_planks\ncondition: craft 12 planks from 3 log\naction: craft\nobject_item: planks\nobject_number: 12\nmaterials: \"log\":3\ntool: null\nrank: 2\n###\n\n\n\n\n    backgroundcolor=, caption=Self-explaining prompt in JARVIS-1, label=lst:explainer_prompt   \n\nSystem: Here are some actions that the agent fails to perform in Minecraft. Please give the explanation of action execution failure according to the current inventory information of the agent.\n\n###\nUser:\nFailed Action: mine(\"iron_ore\":1, null); # step 5: dig down to mine iron blocks\nCurrent Inventory: null\nAssistant: Because mining iron_ore needs to use the tool stone_pickaxe, but my inventory does not have stone_pickaxe. So I need to craft stone_pickaxe first.\n\n###\nUser:\nFailed Action: craft(\"stone_pickaxe\":1, \"cobblestone\":3, \"stick\":2, \"crafting_table\"); # step 1: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on crafting_table\nCurrent Inventory: null\nAssistant: Because crafting stone_pickaxe needs to have 3 cobblestone and 2 stick in inventory, but my inventory does not have cobblestone and stick. So I need to mine cobblestone and craft stick first.\n\n### \nUser:\nFailed Action: craft(\"stick\":4, \"planks\":2, null); # step 3: craft 4 stick from 2 planks first\nCurrent Inventory: null\nAssistant: Because crafting stick needs to have planks in inventory, but my inventory does not have planks. So I need to craft planks first.\n\n###\n\n\n    backgroundcolor=, caption=Self-checking prompt in JARVIS-1, label=lst:self_check_prompt   \n\nSystem: You are a helper agent in Minecraft. Check the plan whether can be finished.\n\n==========\nUser: My current inventory has nothing. I current locate in plains. My task is to obtain 1 wooden axe in Minecraft step-by-step. This is my plan:\ndef craft_wooden_axe(initial_inventory=):\n    mine(obj = \"log\":3, tool = None)  # step 1: chop down trees to mine logs\n    craft(obj = \"planks\":12, materials = \"log\":3, tool = None)   # step 2: craft 12 planks from 3 logs\n    craft(obj = \"stick\":4, materials = \"planks\":2, tool = None) # step 3: craft 4 sticks from 2 planks\n    craft(obj = \"wooden_axe\":1, \"planks\": 3, \"stick\": 2, null) # step 4: craft 1 wooden_axe from 3 planks and 2 sticks\n    return \"wooden_axe\"\n\nAssistant:\nMy current inventory is empty.\n1. I want to mine 3 logs. Mining logs do not require tools. My inventory will has 3 logs.\n2. I want to craft 12 planks from 3 logs. 1 log can produce 4 planks. My inventory will add 12 planks, and delete 3 logs. My inventory will has 12 planks.\n3. I want to craft 4 stick from 2 planks. 2 planks can produce 4 planks. My inventory will add 4 stick and delete 2 planks. My inventory will has 10 planks, 4 stick.\n4. I want to craft 1 wooden pickaxe from 3 planks and 2 sticks. Crafting wooden pickaxe requires crafting_table. But I do not have crafting_table in inventory. This action will failed. \nReturn: Step 4 will failed because of lacking of crafting_table.\n\n",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Memory",
          "content": "Our memory records every successful trajectory experience of JARVIS-1, including the task goals that the agent needs to execute, the actual goal sequence (plan) executed by the agent, and the state (visual observation and symbolic information returned from the environment) when the agent completes the task. In specific implementation, memory is a list where each trajectory experience is encoded as a dictionary, including the keys task, state, and plan. ",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    },
    {
      "title": "Environment Setting",
      "content": "Our Minecraft environment is a hybrid between MineRL   <cit. minerl> and the MCP-Reborn (github.com/Hexeption/MCP-Reborn) Minecraft modding package. Unlike the regular Minecraft game, in which the server (or the \"world\") always runs at 20Hz and the client runs as fast as rendering \n\nObservation Space\n-----------------\nThe environment observations include two parts. One are simply the raw pixels from the Minecraft game that player would see. The overlays including the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or “use” actions are not removed, which are same with the human-playing GUI. Another part is some auxiliary information about the current environment of the agent, including the agent’s current location and current weather. These pieces of information can be obtained by human players by pressing F3. The specific observation information we include are shown in Table  <ref. tab:observation_space>. \n\n\n============================================================================================\n\n\n\n\nTable: The observation space we use in Minecraft.\n\nNote that no high-level observations like voxels and lidar information in Minedojo   <cit. minedojo> can be accessed by agents. During the actual inference process, controller only perceive the raw pixels and interact with the environment, which is same with VPT   <cit. vpt> models. The agent will access information from the environment to generate the text condition of the controller. The environment observations are simply the raw pixels from the Minecraft game that a human would see. Unlike MineRL, we do not remove overlays like the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or “use” actions. The field of view is 70 degrees, which corresponds to the Minecraft default. GUI scale (a parameter controlling the size of the in-game GUI) is set to 2, and brightness is set to 2 (which is not a Minecraft default, but is very frequently used in online videos). The rendering resolution is 640x360, which is downsampled to 128x128 before being input to the models. We empirically found 128x128 to be the smallest resolution for which in-game GUI elements are still discernible, and then chose that to minimize compute costs. Whenever an in-game GUI is open, we additionally render an image of a mouse cursor at the appropriate mouse position to match what a human player’s operating system does (Fig. 12). \n\nAction Space\n------------\n We design a hybrid action space. Some are directly available to human players, including keypresses, mouse movements, and clicks, which are similar to VPT  <cit. vpt>. The keypresses and clicks are binary functional actions, including forward, jump, use and attack etc. In addition to the binary (on/off) keypress actions, our action space also includes mouse movements, as with human gameplay. As with human gameplay, when in-game GUIs are not open, mouse X and Y actions change the agent’s yaw and pitch, respectively. When a GUI is open, camera actions move the mouse cursor. Mouse movements are relative (i.e. they move the mouse or camera relative to the current position, and thus their effect depends on the current position). In Minecraft, interacting with the inventory requires precise mouse movements for tasks like crafting and smelting, while mining and navigating the world can be accomplished with broader mouse actions. To be enable to achieve both the same action space, we abstract the craft and smelt action with GUI into functional binary actions, which are same as MineDojo  <cit. minedojo>. The detailed action space are described in Table  <ref. tab:action_space>. \n\n\n========================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: The action space we use in Minecraft.\n\n\n\nRules\n-----\nWe choose to conduct the test in survival mode of Minecraft 1.16.5. For each environment reset, we have added the following rules: \n - : Set the difficulty of the environment to peaceful mode. \n\n- : Set the environment to daytime forever. \n\n- : Set agent to not drop items upon death. We have added a time limit for each task, within which if the player dies, they will respawn at the spawn point and retain their previous inventory contents. \n\n- : In order to facilitate the display of agent behavior, we have added night vision effects to the agent. \n\n",
      "subsections": [
        {
          "title": "Observation Space",
          "content": "The environment observations include two parts. One are simply the raw pixels from the Minecraft game that player would see. The overlays including the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or “use” actions are not removed, which are same with the human-playing GUI. Another part is some auxiliary information about the current environment of the agent, including the agent’s current location and current weather. These pieces of information can be obtained by human players by pressing F3. The specific observation information we include are shown in Table  <ref. tab:observation_space>. \n\n\n============================================================================================\n\n\n\n\nTable: The observation space we use in Minecraft.\n\nNote that no high-level observations like voxels and lidar information in Minedojo   <cit. minedojo> can be accessed by agents. During the actual inference process, controller only perceive the raw pixels and interact with the environment, which is same with VPT   <cit. vpt> models. The agent will access information from the environment to generate the text condition of the controller. The environment observations are simply the raw pixels from the Minecraft game that a human would see. Unlike MineRL, we do not remove overlays like the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or “use” actions. The field of view is 70 degrees, which corresponds to the Minecraft default. GUI scale (a parameter controlling the size of the in-game GUI) is set to 2, and brightness is set to 2 (which is not a Minecraft default, but is very frequently used in online videos). The rendering resolution is 640x360, which is downsampled to 128x128 before being input to the models. We empirically found 128x128 to be the smallest resolution for which in-game GUI elements are still discernible, and then chose that to minimize compute costs. Whenever an in-game GUI is open, we additionally render an image of a mouse cursor at the appropriate mouse position to match what a human player’s operating system does (Fig. 12). ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Action Space",
          "content": " We design a hybrid action space. Some are directly available to human players, including keypresses, mouse movements, and clicks, which are similar to VPT  <cit. vpt>. The keypresses and clicks are binary functional actions, including forward, jump, use and attack etc. In addition to the binary (on/off) keypress actions, our action space also includes mouse movements, as with human gameplay. As with human gameplay, when in-game GUIs are not open, mouse X and Y actions change the agent’s yaw and pitch, respectively. When a GUI is open, camera actions move the mouse cursor. Mouse movements are relative (i.e. they move the mouse or camera relative to the current position, and thus their effect depends on the current position). In Minecraft, interacting with the inventory requires precise mouse movements for tasks like crafting and smelting, while mining and navigating the world can be accomplished with broader mouse actions. To be enable to achieve both the same action space, we abstract the craft and smelt action with GUI into functional binary actions, which are same as MineDojo  <cit. minedojo>. The detailed action space are described in Table  <ref. tab:action_space>. \n\n\n========================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: The action space we use in Minecraft.\n\n",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Rules",
          "content": "We choose to conduct the test in survival mode of Minecraft 1.16.5. For each environment reset, we have added the following rules: \n - : Set the difficulty of the environment to peaceful mode. \n\n- : Set the environment to daytime forever. \n\n- : Set agent to not drop items upon death. We have added a time limit for each task, within which if the player dies, they will respawn at the spawn point and retain their previous inventory contents. \n\n- : In order to facilitate the display of agent behavior, we have added night vision effects to the agent. \n\n",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    },
    {
      "title": "Results and Details of 200+ tasks in Minecraft Universe Benchmark",
      "content": "<label. sec:detailed_results> We list the evaluation task set belows with details including task name, maximum steps, initial inventory, biome, and language instructions. We also show the evaluation times across different seeds and successful episodes rate. Note that all tasks are evaluated in Minecraft 1.16.5 Survival Mode. \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the \n\n\n=========================\nTable: The results of our agent on various tasks in the ",
      "subsections": [],
      "figures": {}
    }
  ],
  "figures": {}
}