{
  "title": "height 4pt FP8-LM: Training FP8 Large Language Models  height 1pt",
  "content": "\n\n \n\n \n\n                    \n\n  \n\n   \n\n \n\n                 \n\n pgfplots.groupplots compat=1.3 \n\n \n\n \n\n  \n\n  *\n\n\n\n               fancy empty    [LO] \nheight 4pt18.0675pt-0.0pt FP8-LM: Training FP8 Large Language Models  14.454pt-0.0ptheight 1pt6.5043pt  Houwen Peng ^*  Kan Wu ^*  Yixuan Wei ^* \n Guoshuai Zhao  Yuxiang Yang  Ze Liu  Yifan Xiong  Ziyue Yang \n Bolin Ni  Jingcheng Hu  Ruihang Li  Miaosen Zhang  Chen Li  Jia Ning  Ruizhe Wang  Zheng Zhang \n Shuguang Liu  Joe Chau  Han Hu ^†  Peng Cheng ^†  \n \n Microsoft Azure and Microsoft Research   [NO \\title GIVEN]\n    [NO \\author GIVEN]\n    November 17, 2023\n======================\n\nContributions for all the authors can be found in Section <ref. contribution>.footnote-1 * equal work  ^† contact: { hanhu | pengc} @microsoft.comfootnote-1 \n  In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42%  reduction in real memory usage but also ran 64%  faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17% . This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at aka.ms/MS.AMP. \n\nIntroduction\n============\nLarge language models (LLMs) <cit. gpt3, megatron-nlg, palm, opt> have demonstrated unprecedented capabilities in language comprehension and generation, leading to breakthroughs in reasoning, math, science, and many other tasks <cit. gpt4, palm2>. However, training LLMs is extremely costly. For example, PaLM takes 6,144 TPUv4 chips to train a 540B model, while GPT-3 175B consumes several thousand petaflop/s-days of compute for pre-training <cit. palm, gpt3>. This motivates the needs of reducing the training costs of LLMs, especially for the scaling of next-generation super-intelligent models. Low-precision training is one of the most promising directions to reduce the costs, as it can provide high speed, small memory footprint, and low communication overhead. Most existing training systems, e.g., Megatron-LM <cit. megatron-lm>, MetaSeq <cit. opt>, and Colossal-AI <cit. colossal-ai>, train LLMs with either FP32 full-precision or FP16/BF16 mixed-precision by default. This is not essential, however, to achieve full accuracy for large models. With the release of Nvidia H100 GPU, FP8 is becoming the next-generation datatype for low-precision representation <cit. h100-whitepaper, fp8-dl>. Theoretically, FP8 can achieve 2x speed-up, 50%  - 75%  memory cost savings, and 50%  - 75%  communication savings compared with current 16-bit and 32-bit floating point mixed-precision training, which is very promising for scaling-up next-generation foundation models. Unfortunately, the current support for FP8 training is rare and limited. The only usable framework is the Nvidia Transformer Engine (TE) <cit. te>, but it applies FP8 solely for GEMM computation and still retains master weights and gradients using high precision, e.g., FP16 or FP32. As a result, the end-to-end speed-up, memory and communication cost savings are very limited, which does not fully unveil the power of FP8. To address this issue, we propose an extremely optimized FP8 mixed-precision framework for LLM training. The core idea is to infiltrate FP8 compute, storage, and communication into the whole progress of large model training, making the forward and backward pass all used the low-precision FP8, thus largely reducing system workloads compared to previous frameworks <cit. amp, te, fp8-dl>. Specifically, we design three optimization levels that utilize FP8 to streamline mixed-precision and distributed training. The three levels gradually incorporate 8-bit collective communication, optimizer, and distributed parallel training in an incremental manner. The higher optimization level indicates using more FP8 during LLM training. Moreover, for large-scale training, such as GPT-175B trained on thousand of GPUs, our framework provides FP8 low-bit parallelism, including tensor, pipeline, and sequence parallelism, paving the way to next-generation low-precision parallel training. <figure. fig.gpt - An analysis of comparing the maximum model sizes attainable through the utilization of either the prevalent BF16 or our FP8 mixed-precision training approach on a cluster of Nvidia H100 GPUs with 80G memory.>Training LLMs with FP8 is non-trivial. The challenges stem from issues such as data underflow or overflow, coupled with quantization errors arising from the narrower dynamic range and reduced precision inherent in FP8 data formats. These challenges cause numerical instabilities and irreversible divergences throughout the training process. To tackle them, we propose two techniques: precision decoupling and automatic scaling for preventing the loss of critical information. The former one involves decoupling the influence of data precision on parameters such as weights, gradients, optimizer states, and assigning reduced precision to components that are not precision sensitive. The latter one is to preserve gradient values within the representation range of FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflow and overflow occurrences during all-reduce communication. To validate the proposed FP8 low-precision framework, we apply it to GPT-style model training, encompassing both pre-training and supervised fine-tuning (SFT). The experimental results demonstrate the effectiveness of our FP8 methodology, yielding substantial benefits including a 27%  to 42%  reduction in real memory usage (e.g., 27%  reduction for GPT-7B while 42%  for GPT-175B ) and a notable 63%  to 65%  decrease in weight gradient communication overhead compared to the prevalent BF16 mixed-precision training approach. Without changes to any hyper-parameters, such as learning rate and weight decay, the models trained using FP8 exhibit performance equivalency to those employing BF16 high precision, both in pre-training and downstream tasks. It is noteworthy that during the training of GPT-175B model, our FP8 mix-precision framework reduces training time by 17%  compared to TE <cit. te>, while consuming 21%  less memory on H100 GPU platform. More importantly, the reduction in costs achieved through the utilization of low-precision FP8 can be further increased, as the scale of models continues to expand, which is presented in Fig.  <ref. fig.gpt>. For fine-tuning, we employ FP8 mixed-precision for instruction tuning and reinforcement learning with human feedback (RLHF) to better align pre-trained LLMs with end tasks and user preferences. Specifically, we fine-tune pre-trained models on publicly user-shared instruction-following data <cit. sharegpt>. The models tuned with our FP8 mixed-precision demonstrate comparable performance to those utilizing the half-precision BF16 <cit. mt-bench> on the AlpacaEval <cit. alpaca_eval> and MT-Bench <cit. mt-bench> benchmarks, while achieving 27%  improvements in training speed. Moreover, FP8 mixed-precision exhibits considerable potentials in RLHF, a process that necessitates loading multiple models during training. Through the utilization of FP8 in training, the prevalent RLHF framework AlpacaFarm <cit. alpacafarm> can yield a 46%  reduction in model weights and a 62%  reduction in optimizer states’ memory consumption. This further demonstrates the versatility and adaptability of our FP8 low-precision training framework. We are making the following contributions to drive the design of next-generation FP8 low-precision training for LLMs. \n - A new FP8 mixed-precision training framework. It unlocks 8-bit weights, gradients, optimizer, and distributed training gradually in an add-on fashion, which is convenient in use. This 8-bit framework can be used as a simple drop-in replacement for existing 16/32-bit mixed-precision counterparts, without requiring any changes to the hyper-parameters and training receipts. Additionally, we provide a Pytorch implementation that enables 8-bit low-precision training in a few lines of code. \n\n- A new family of GPT-style models trained with FP8. We apply the proposed FP8 scheme to GPT pre-training and fine-tuning (i.e., SFT and RLHF), and demonstrate its potentials on a variety of model scales ranging from 7B to 175B parameters. We equip prevalent parallel computation paradigms with FP8 supports, including tensor, pipeline, and sequence parallelisms, enabling the utilization of FP8 to train large foundation models. We open-source the first FP8 GPT training codebase based upon Megatron-LM <cit. megatron-lm> implementation. \n\nWe expect the release of our FP8 framework will establish a new paradigm for next-generation low-precision training system dedicated to large foundation models.\n\nFP8 LLMs\n========\nMixed-precision <cit. amp> has been widely used in LLM training to improve compute and memory efficiency. The most popular mixed-precision schemes are FP16-FP32 and BF16-FP32. Because of the restricted numerical range of FP16, FP16-FP32 scheme has been known instabilities for training large models <cit. gopher,glm-130b>. Consequently, the community now commonly adopts BF16-FP32 for training LLMs, such as Megatron-Turing NLG-530B <cit. megatron-nlg>, Bloom-175B <cit. bloom> and Gopher <cit. gopher>. The underlying reason is that BF16 has a wide dynamic range to maintain numerical stability while matching the performance of the full-precision FP32. Moreover, BF16 employs half the number of bits as compared to FP32, thus reducing considerable memory footprints while improving compute efficiency. FP8 is a natural evolution from 16-bit data formats to further reducing computing costs. However, training LLMs with reduced-precision FP8 poses new challenges. The dynamic range and representation precision of FP8The details of FP8 data formats are presented in Appendix  <ref. appendix.A>. are much lower than BF16 and FP16, which inevitably induces more training collapses, such as loss spikes or even NaNs. To address the issues, tensor scaling techniques are proposed <cit. sunxiao-fp8, fp8-dl>. The core idea is multiplying higher precision values with a scaling factor prior to their casting to FP8 in order to move them into a range that better overlaps with the representable range of a corresponding FP8 formatThe details of FP8 tensor scaling are introduced in Appendix <ref. appendix.B>. <cit. fp8-dl>. Such a per-tensor scaling technique reduces data quantization errors while improving numerical stability and accuracy, thus enabling the utilization of the lower-precision FP8 for training large models. Unfortunately, the current support for FP8 low-precision training is restricted. Nvidia TE <cit. te> only supports FP8 compute for linear layers in Transformer <cit. transformer>, while leaving all other operations, such as weight update and gradient synchronization, still using higher precision. In this work, we present an extremely optimized FP8 mixed-precision strategy for LLM training. The new FP8 optimization includes three key perspectives: FP8 communication, FP8 optimizer, and FP8 distributed training. By integrating these aspects, the training of LLMs such as the 175B GPT-3 model can fully harness the advantages of FP8 low-precision and improve training efficiency. \n\nFP8 Gradient and All-Reduce Communication\n-----------------------------------------\n <label. sec.fp8grad> Existing mixed-precision training methodologies <cit. amp, te> typically employ 16-bit or 32-bit datatype for the computation and storage of gradients, resulting in a high bandwidth requirement for collective communication throughout the training process. We found that directly applying FP8 to gradients leads to a decrease in accuracy. The fundamental issue lies in the underflow and overflow problems arising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregating gradients across GPUs during all-reduce: pre-scaling and post-scaling. Pre-scaling divides the gradient g_i calculated on the i-th GPU by the total number of GPUs (i.e., N) before being summed, which is formulated as: \n    g = g_1/N + g_2/N + ⋯ +g_N/N.\n When N is large, this division can cause data underflow, especially for FP8 low-precision representation of gradients. To mitigate this issue, post-scaling performs the gradient summation first, followed by the division scaling during the gradient collection process: \n    g = (g_1 + g_2 + ⋯ + g_N)/N.\n This post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectively alleviating the underflow issue. However, this approach encounters overflow issues when aggregating gradients. In contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issues in the pre-scaling and post-scaling approaches. To be specific, we introduce an auto-scaling factor μ, that changes on the fly during the training, to reduce the occurrences of overflow and underflow in gradients: \n    g'_i = μ· g_i. <label. eq_mu>\n A statistical analysis is conducted on the gradient values of g'_i, with the objective of quantifying the proportion of values that attains the maximum feasible value within the FP8 representation range. If the ratio of the maximum value exceeds a specified threshold, i.e., 0.001% , μ is set to 1/2 in the subsequent training step, thereby mitigating the risk of overflow. Conversely, when the ratio consistently remains the threshold, we opt to exponentially increase μ to 2 over the span of 1,000 training steps, thereby effectively mitigating the risk of underflow occurrences. Another key obstacle of FP8 collective communication lies in devising an effective strategy to manage the tensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation <cit. nccl> lacks the capability of performing all-reduce operation considering the additional tensor-wise scaling factors. Meanwhile, efficient implementation is also very challenging, especially considering that the NCCL gradient summation operates at sub-tensor level. This complexity increases significantly when incorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanism that scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g'_i, s'_i) denote a scaling tensor which stores the weight gradient in the i-th GPU, where g'_i is a FP8 tensor and s'_i is the corresponding scaling factor. The actual weight gradient is g’_i / s’_i. Prior to the all-reduce operation over gradient tensors, we first gather the scaling factors s'_i of each gradient tensor on all GPUs and calculate the global minimum scaling factor s'_g as: \n    s'_g = min(s'_1,   s'_2, … ,   s'_N),\n where the global minimum scaling factor s'_g is shared across GPUs. We use this shared scaling factor s'_g to unify the rescaling of the gradient tensors across GPUs. In this way, all gradient tensors associated with the same weight use the same shared scaling factor to quantize the tensors into FP8 format on all GPUs: \n    g”_i = FP8(s'_g·(g'_i / s'_i)). <label. eq.global>\n This approach reduces communication overhead by transmitting only a single scalar s'_g, making the additional synchronization step highly efficient. As the input tensors share the same scaling factor, it eliminates the need of considering all-reduce the scaling factors in parallel and allows for standard NCCL all-reduce operation to be performed. The final collected gradient is obtained as follows: \n    g = g”_1 + g”_2 + ⋯ + g”_N,      s = N · s'_g,\n where g is the final aggregated gradient and s is the corresponding scaling factor. Rescaling the scaling factor for the summed gradient g is equivalent to dividing g by N in theory. By implementing the aforementioned dual strategies of distributed and automated scaling, we can successfully realize FP8 low-bit gradient communication while preserving model accuracy. Furthermore, this approach entails storing gradients in FP8 and conducting communication in FP8 as well, thereby yielding reductions in GPU memory usage and communication bandwidth consumption. \n\nFP8 Optimizer\n-------------\nIn the training of LLMs, Adam and its variants <cit. adam, adamw> are the most frequently-used optimization methods, that maintain copies of model weights, gradients, first-order and second-order gradient moments for model updates. Mixed-precision training <cit. amp> with Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float format for numerical stability <cit. megatron-lm,zero-deepspeed, opt, bloom>. Consequently, the Adam optimizer consumes 16 bytes of memory per parameter during training: \n    4_master weights + 4_gradients +      4    +     4  _Adam states    =     16 bytes. <label. eq:16bitamp>\n When model size is large, the memory consumption of the variables in Adam will become a bottleneck. Previous work <cit. gopher, glm-130b, swinv2> has revealed that reducing precision of the variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale modelsBF16 lacks the precision needed for accuracy, while FP16 has a restricted dynamic range. Given these challenges, prevalent mixed-precision training methodologies rely on utilizing FP32 full-precision for master weights, gradients, and gradient moments.. This prompts an evaluation of which variables in the optimizer should be allocated high precision and which can be accommodated with low-precision. To clarify, we decouple the influence of data precision on the variables in the optimizer and investigate which one can be assigned lower precision, i.e., precision decoupling. We find a guiding principle: the gradient statistics can use lower precision, while the master weights necessitate high precision. More concretely, the first-order gradient moment can tolerate a high quantization error and can be assigned with low-precision FP8, while the second-order moment requires a higher precision, as analyzed in Sec. <ref. sec.ablation>. This stems from the fact that, during model updates in Adam, the direction of the gradient holds greater significance than its magnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as the high-precision tensor, though it introduces precision degradation to some extend. Calculating the square of gradients for the second-order gradient moment might lead to data underflow due to the typically small gradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy. On the other hand, we find that it is crucial to keep the master weights using high precision. The underlying reason is that weight updates can sometimes become extremely small or large during training, higher precision for the master weights helps prevent loss of information when updating weights, ensuring more stable and accurate training. In the implementation, the master weights have two viable options: utilizing either FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage of conserving memory without compromising accuracy. Consequently, our default choice is to employ FP16 with tensor scaling for storing master weights in the optimizer. Our FP8 mixed-precision optimizer consumes 6 bytes of memory per parameter during training: \n    2_master weights + 1_gradients +      1    +     2  _Adam states    =     6 bytes.\n This new low-bit optimizer reduces memory footprints by 2.6x in comparison to the previous solution, as exemplified in Eq.  (<ref. eq:16bitamp>). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments in Sec.  <ref. sec.mainresults> show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175B parameters. \n\nFP8 Distributed Parallel Training\n---------------------------------\nTraining LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. The frequently-used strategies include data parallelism, tensor parallelism, pipeline parallelism, and sequence parallelism. Each parallelism has its own merits and has been used in a complementary fashion in existing systems <cit. megatron-nlg, megatron-lm, opt, bloom, colossal-ai>. For FP8 supports of these strategies, neither data parallelism nor pipeline parallelism necessitates any specific modifications, because they do not involve additional FP8 compute and communication when splitting data batches or model layers into segments across devices. Tensor parallelism partitions individual layers of a model across multiple devices, such that the shards of weight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensor parallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layer computation, enabling the forward compute and backward gradient collective communication all using FP8. On the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequences are fed to different devices to save activation memory. As shown in Fig. <ref. fig:distribute>, sequence and tensor parallelism are performed in parallel to different parts of a Transformer model to make the best use of the available memory and improve training efficiency. There is a converter g between sequence and tensor parallel regions to all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backward pass). We add an FP8 datatype conversion prior to g, such that the all-gather (or reduce-scatter) operation uses FP8 low-bit activation to save communication cost across GPUs. <figure. fig:distribute - Transformer layer with FP8 tensor and sequence parallelism. The FP8 low-bit operation is highlighted with orange. g is all-gather in forward pass and reduce-scatter in backward pass, while g̅ is reduce-scatter in forward pass and all-gather in backward pass. The gather-reduce operation g between sequence parallel and tensor parallel is executed utilizing FP8 low-precision activation, thus saving half communication costs.><figure. fig:scalingZeRO - ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO method, which splits a single tensor into multiple partitions and distributes them to different devices. Right: the proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor scaling into account.>In addition, Zero Redundancy Optimizer (ZeRO) <cit. zero-deepspeed> is another frequently-used distributed learning technique in large model training. The core idea of ZeRO is to shade model states over devices, such that each device only hold a fraction of data (e.g., master weights, gradients, and optimizer states) required for a training step. To reduce memory consumption, ZeRO method generally splits a single tensor into multiple partitions and distributes them to different devices. Directly applying FP8 to ZeRO is infeasible, because it is difficult to handle the scaling factors associated with the FP8 partitions. The per-tensor scaling factors should be distributed along with FP8 partitions. To address this issue, we implement a new FP8 distribution scheme that distributes each tensor as a whole across devices, rather than partitioning it into multiple sub-tensors as in ZeRO <cit. zero-deepspeed>. The distribution of FP8 tensors is processed in a greedy manner, as outlined in Alg. <ref. alg:greedy>. Specifically, our method first sorts the tensors of model states according to their sizes, and then distributes the tensors to different GPUs based upon the remaining memory size of each GPU. The distribution follows the principle that the GPUs with larger remaining memory get a higher priority in receiving new distributed tensors. In this way, the tensor scaling factors can be distributed along with the tensors smoothly, while reducing communication and compute complexity. Figure <ref. fig:scalingZeRO> presents a visual illustration of the difference in ZeRO tensor partitioning between scenarios with and without scaling factors.  [!b] Greedy Distribution Algorithm for ZeRO <label. alg:greedy>  [1] FP8 tensors with their corresponding scaling factors: T={ (s_1, t_1),(s_2, t_2),… ,(s_n, t_n)}, where s denotes scaling factors while t represents 8-bit tensors. The size of each tensor: C={ c_1,c_2,… ,c_n}. Partitions representing scaling tensors assigned to each GPU. Sort T in descending order of their sizes to get T'={ (s'_1, t'_1),(s'_2, t'_2),… ,(s'_n, t'_n)} and C'={ c'_1,c'_2,… ,c'_n}, where c'_1⩾ c'_2⩾…⩾ c'_n. Initialize memory usage u_j=0 and partition p_j=∅ for each GPU G_j. i=1 to n j min _j u_j Find the GPU j ∈ [1,m] with the least memory usage. p_j p_j∪{ (s'_i,t'_i)} Assign (s'_i,t'_i) to G_j. u_j  u_j+ c'_i Update the memory usage of G_j. Partitions P={ p_1,p_2,… ,p_m}   \n\nExperiment\n==========\nIn this section, we assess the effectiveness of the proposed FP8 mixed-precision training approach on GPT-style LLMs, including a wide range of model scales, from 125 million to 175 billion parameters. For performance ablation, we compare GPT models trained with FP8 against those trained with half-precision BF16 and full-precision FP32. For generality evaluation, we conduct experiments encompassing both FP8 low-bit pre-training and fine-tuning, considering instruction tuning and human preference alignment. \n\nExperimental Setup\n------------------\n\n\nTraining Dataset\nOur pre-training data is constructed using open-sourced language collections from several sources, including CommonCrawlhttps://commoncrawl.org, The Pile <cit. pile>, C4 <cit. c4>, OpenWebText <cit. gpt2, Gokaslan2019OpenWebTextCorpus>, CC-NEWS <cit. liu2019roberta>, CC-Stories <cit. trinh2018simple>, Redpajama <cit. redpajama>, and Wikipediahttps://wikipedia.org. We apply fuzzy deduplication <cit. lee_deduplicating_2022> across CommonCrawl snapshots to enhance data quality. Tab. <ref. tab.data> in Appendix <ref. appendix.data> provides details of our pre-training data, including information such as the number of tokens from each source and associated sampling weights. For a more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer to Appendix <ref. appendix.data>. Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1<cit. vicuna>, which uses a publicly user-shared instruction following data <cit. sharegpt>. For reinforcement learning with human feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmless dataset  <cit. HH> and Open-Assistant dataset  <cit. oasst>. The training framework and associated configurations align with the publicly available AlpacaFarm  <cit. alpacafarm>. \n\nModel Configuration\n\n\nparams   dimension   n heads   n layers   TP   PP   SP   learning    batch     n tokens\n                                                           rate       size             \n=======================================================================================\n 125M       768        12         12      1    1         6.0e^-4       1M        100B  \n  7B       4096        32         32      1    1         3.0e^-4       4M        100B  \n 13B       5120        40         40      2    1         3.0e^-4       4M        100B  \n 175B      12288       96         96      8    4         3.0e^-5       1M         5B   \nTable:  Model sizes, architectures, and training hyper-parameters. TP, PP, and SP indicate tensor, pipeline, and sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of the 175B model to a dataset comprising only 5B tokens, which has proven to be sufficient for evaluating system performance.\n\nThe model architecture we used is a decoder-only Transformer <cit. gpt3>, which has been widely-used in recent generative LLMs like PaLM <cit. palm>, OPT <cit. opt>, and LLaMA <cit. llama>. In addition to the base architecture, we integrate several modifications proposed recently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspiration from recent successful experiments <cit. gpt-neox, llama>, we incorporate rotary positional embeddings (RoPE) <cit. rope> into our approach. This addition enables us to capture both absolute and relative positions information, enhancing performance especially when extrapolating to larger context windows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access <cit. ivanov2021data>. Flash Attention <cit. flashatt> proposed an IO-aware exact attention algorithm which uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration. We train the models using the proposed FP8 optimizer, which is built upon Adam <cit. adam> with decoupled weight decay <cit. adamw>, following the common practise with the decay rates β _1 = 0.9, β _2 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final learning rate is 10%  of the maximal learning rate. We train the models for 100B tokens in total with a batch size of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000 iterations. Tab. <ref. tab.arch> presents the details of model configurations and the corresponding training settings. The training is conducted on Azure NDv5 H100 GPU platform <cit. hpc>. <figure. fig:gptloss - A comparison between FP8 and BF16: Analyzing the training loss of GPT models with the parameters ranging from 7 billion to 175 billion.>\n\n \n 0.95  \n\n           HS \n\n     Lambada \n\n     BoolQ \n\n     PIQA \n\n     COPA \n\n     Winogrande \n\n     Arc-C \n\n     Arc-E \n\n     ObQA \n\n     Avg \n\n\n 8lGPT-7B model zero-shot performance \n\n    \n BF16 \n\n     61.3 \n\n     61.4 \n\n     61.2 \n\n     75.0 \n\n     79.0 \n\n     58.5 \n\n     32.9 \n\n     59.7 \n\n     36.4 \n\n     58.4 \n\n\n FP8 \n\n     60.0 \n\n     61.8 \n\n     62.0 \n\n     74.2 \n\n     78.0 \n\n     59.8 \n\n     32.9 \n\n     58.7 \n\n     34.6 \n\n     58.0 \n\n\n 8lGPT-13B model zero-shot performance \n\n    \n BF16 \n\n     64.8 \n\n     64.9 \n\n     63.4 \n\n     75.9 \n\n     82.0 \n\n     61.0 \n\n     35.2 \n\n     61.5 \n\n     40.6 \n\n     61.0 \n\n\n FP8 \n\n     64.1 \n\n     63.4 \n\n     63.9 \n\n     76.2 \n\n     81.0 \n\n     61.6 \n\n     34.9 \n\n     61.3 \n\n     36.8 \n\n     60.4 \n\n\n   Zero-shot performance on downstream tasks. The models are trained with either the standard BF16 mixed-precision scheme <cit. megatron-lm> or the proposed FP8 low-precision scheme.  <label. tab.downstreamtask> \n \n\n\n\nMain Results\n------------\n <label. sec.mainresults> \n\nModel Performance\n We first compare the performance of models trained using FP8 mixed-precision with those trained using BF16. In Fig.   <ref. fig:gptloss>, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. The training configurations and hyper-parameters remain consistent across models trained with FP8 and BF16. The only difference lies in the mixed-precision schemes utilized. As shown in Fig. <ref. fig:gptloss>, the loss curves almost overlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme <cit. megatron-lm, gopher, chinchilla> across a diverse array of model scales. Also, we evaluate the pre-trained models on a wide range of downstream tasks, including HellaSwag (HS) <cit. hellaswag>, Lambada <cit. lambada> BoolQ <cit. boolq>, PIQA <cit. piqa>, COPA <cit. copa>, Winogrande <cit. winogrande>, Arc <cit. arc>, and OpenbookQA (ObQA) <cit. openbookqa>. As reported in Tab. <ref. tab.downstreamtask>, the FP8 pre-trained models exhibit comparable zero-shot performance in comparison to their BF16 counterparts. This result provides further validation that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable to their high-precision counterparts. Furthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instruction following. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 <cit. vicuna>, which adopts the open-sourced LLaMA-7B <cit. llama> as the base model for fine-tuning. Fig. <ref. fig.sft> presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degree of overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 <cit. davinci-003> is also comparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. <ref. tab.sft>. This indicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phase but also to downstream fine-tuning tasks. In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from human feedback (RLHF), a more complex process to align LLMs with user preferences. Following the same training setting as AlpacaFarm <cit. alpacafarm>, a recent RL framework for LLM alignment, we optimize policy models with PPO algorithm <cit. ppo>. The solely difference lies in the choice of mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. <ref. fig.rlhf> and Tab. <ref. tab.rlhf>, we observe a notable reduction in memory utilization, for instance, a 31.8%  memory reduction concerning model weights and a 62.5%  reduction concerning optimizer states. Consequently, it can be inferred that FP8 is capable of replicating the BF16 mixed-precision for RLHF training. This underscores the broader applicability and versatility of our FP8 low-bit training solution. <figure. tab.sft><figure. tab.rlhf>\n\nSystem Performance\nIn this section, we evaluate system-level performance of FP8 mixed-precision, considering communication efficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method employs 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results in a 75%  reduction in communication costs when compared to the mainstream 32-bit scheme (Despite BF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-reduce communication <cit. megatron-lm>). Due to the impact of system transmission loss, the observed practical reduction during GPT model training falls within the range of 63%  to 65% , as indicated in Table <ref. tab.sys>. Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) <cit. te> still relies on full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8 solution. When training GPT models with identical batch sizes, FP8 mixed-precision can lead to a reduction in memory footprint ranging from 27%  to 42%  when compared to BF16, as reported in Tab. <ref. tab.sys>. These reductions in memory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced. Moreover, compared with TE <cit. te>, our solution is also very competitive, obtaining 34.2% , 35.4% , and 44.8%  additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE employs FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more memory than our solution. In addition, the saved memory in our method can be used to train larger batch size or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80G, our approach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion parameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcases the potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train longer sequences with the same GPU resources.Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent BF16 scheme, achieving a notable speed-up of 64%  when applied to GPT-175B model. The model FLOPS utilization (MFU) of FP8 mixed-precision training is 32.0%  on H100 GPUs, being 17.2%  superior to TE. These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces communication costs during the training of large models, and ultimately enhances system utilization efficiency on the latest H100 GPU platform. \n\n\n\n\n===================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: System-level performance on Nvidia H100 GPUs 80G. Here, TP, PP, and DP represent tensor, pipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs utilization. Weight-related communication contains the all-gather operator on weights and the reduce-scatter operator on weight gradients.\n\n<figure. fig:ablate_comm - Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient all-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The experiment is conducted using a GPT-7B model with a data parallelism factor of 128.>\n\nAblation Study\n--------------\n <label. sec.ablation> We ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the performance in Tab. <ref. tab.opt> – <ref. tab:zero> and Fig. <ref. fig:ablate_comm> – <ref. fig.opt>. The ablation experiments are conducted on GPT models, whose architectures and training settings are elaborated in Tab. <ref. tab.arch>. Importantly, our ablation study yields several guidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future research on low-bit model training. <figure. fig.opt>Communication. We first analyze the limitations of the conventional pre-scaling and post-scaling methods when aggregating low-bit gradients during the all-reduce communication process. As shown in Fig. <ref. fig:ablate_comm>, we conduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across different Transformer blocks. It is observed that the pre-scaling method has relative larger underflow rate when quantifying gradients from 32-bit to 8-bit, while the post-scaling method has higher overflow rate. In contrast, the proposed auto-scaling technique can diminish both the underflow ratio and the overflow ratio, while getting much better SNR, as shown in Fig. <ref. fig:ablate_comm> (a). This demonstrates the effectiveness of auto-scaling method in reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce.   0.6  2*Model  2*TP  2*PP  2*DP  Micro  Mixed  Act-related Comm. BS  Precision  Rate (% )  Volume (GB) 2*GPT-13B  2*2  2*1 2*16  2*2  BF16  11.1  4.7  FP8 (Ours)  2.5  3.12*GPT-175B  2*8  2*4  2*4  2*1  BF16  14.8  5.9  FP8 (Ours)  4.5  3.9  Activation-related communication volume reduction in sequence and tensor parallelism, including the all-gather operator on activation and the reduce-scatter on activation gradients. <label. tab:sp> \n   0.65  2*Model  2*TP  2*PP  2*DP  Micro  Mixed  GPU Memory  BS  Precision  Min  Max 3*GPT-7B  3*1  3*1  3*32  3*2  BF16  69.02  69.59  FP8 (TE)  76.93  77.23  FP8 (Ours)  49.31  50.77 3*GPT-13B  3*2  3*1  3*16  3*2  BF16  67.63  68.35  FP8 (TE)  72.85  73.81  FP8 (Ours)  46.78  47.66 3*GPT-175B  3*8  3*4  3*4  3*1  BF16  63.07  63.42  FP8 (TE)  66.39  66.72  FP8 (Ours)  36.39  36.81   Comparing ZeRO distribution methods in terms of memory load across GPUs. Here “Min” and “Max” denote the minimum and maximum memory utilization observed across 32 GPUs. Our FP8 ZeRO method uses less memory while achieving memory-aware load balancing. <label. tab:zero>  \n  Optimizer. We further ablate the impact of reduced precision for the variables in the AdamW optimizer. We set the BF16 mixed-precision optimizer as the baseline, since it has been widely used in existing LLM training frameworks <cit. amp, megatron-lm, te>. Tab. <ref. tab.opt> presents the settings of reduced precision for the variables, while Fig. <ref. fig.opt> plots the corresponding training losses. We observe that: 1) FP8 master weight induces performance degradation (see the # 2 vs. # 3 lines in Fig. <ref. fig.opt>), while FP16 can maintain accuracy as FP32 (see # 2 vs. # 0 and # 1) but requiring using tensor scaling. It reveals that the master weight is precision-sensitive. This can be attributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy. 2) The second-order gradient moment is more precision-sensitive than the first-order one, because the square calculation is easy to cause underflow and leads to accuracy degradation. Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the # 4 dot in Fig. <ref. fig.opt>). Parallelism. In our FP8 LLM training framework, we introduce FP8 low-bit convertors into sequence parallelism and tensor parallelism to reduce activation communication costs across GPUs. Here we conduct an analysis experiment to count the activation-related communication volume during GPT model training, and report the numbers in Tab. <ref. tab:sp>. It is observed that our FP8 parallel scheme results in a substantial reduction of 33%  in activation-related communication costs compared to the original method utilizing BF16. Furthermore, in ZeRO distributed training, our method distributes each FP8 tensor along with its associated scaling factor as a whole, rather than partitioning the tensor into splits across GPUs. This strategy not only results in more GPU memory savings but also maintains a balanced memory load across GPUs, as demonstrated in Tab. <ref. tab:zero>. \n\nRelated Work\n============\n Mixed-precision Training. Efficient training through reduced mixed-precision has been widely used in modern deep learning to save computing costs. While some works have taken bit-reduction to the extreme, i.e. 1-bit binary networks <cit. hubara2016binarized, Xnor-net>, they have not been successful in maintaining model accuracy <cit. fp8-dl>. The most practical scheme now is the FP16 half-precision method <cit. amp>, which can maintain accuracy while improving training efficiency. The computations during forward pass and back propagation use FP16 while the master weights use FP32. Since FP16 has a narrower dynamic range, FP16 mixed-precision entails loss scaling <cit. amp> to prevent loss of accuracy. Fortunately, the need for loss scaling can be avoided by using BF16 datatype, because BF16 maintains the same dynamic range as the full-precision FP32. This results in that large model training now prefers to use BF16 mixed-precision scheme, which is more stable during training <cit. megatron-nlg, bloom, glm-130b>. FP8 is a natural progression from 16-bit data formats to further reducing computing cost. Early pioneering efforts in FP8 low-bit model training <cit. fp8-wangnaigang, sunxiao-fp8, 8bit-opt> have largely remained at the simulation stage. Consequently, there exists a notable gap between the projected capabilities of these approaches and their actual performance on hardware <cit. fp8-dl>. With the advent of Nvidia Hopper GPU architecture <cit. h100-whitepaper>, FP8 is emerging as a viable and practical data type for the next-generation low-precision training, as discussed in <cit. fp8-dl>. At present, the Nvidia Transformer Engine (TE) <cit. te> serves as the primary framework for FP8 mixed-precision training. However, its support for FP8 usage remains somewhat constrained. TE’s current implementation restricts FP8 usage solely to weight computation, retaining the storage of model weights and gradient calculations with 16-bit data types. Consequently, the end-to-end speed-up, memory and communication cost savings are limited. In contrast, our work infiltrates FP8 gradient, optimizer, and distributed training into the whole progress of model training, fully unveiling the capabilities of FP8. Large Language Models. Recent years have witnessed a substantial evolution in the field of LLMs. Autoregressive language modeling – predicting the future of a text sequence from its past – provides a simple yet powerful objective that admits formulation of numerous tasks. While there exist alternative methodologies, such as masked language modeling <cit. bert> and permutation language modeling <cit. xlnet>, the autoregressive method now is more promising because of its strong performance. Following the scaling laws <cit. gpt3> and the refined laws <cit. chinchilla>, various LLMs are have been proposed, including dense models: GPT-3 <cit. gpt3>, Jurassic-1 <cit. jurassic>, Gopher  <cit. gopher>, Chinchilla <cit. chinchilla>, Bloom  <cit. bloom>, OPT <cit. opt> Megatron-Turing NLG <cit. megatron-nlg>, PaLM <cit. palm>, LaMDA <cit. lamda>, LLaMA <cit. llama>, and sparse models: GLaM <cit. glam>, and Switch transformers <cit. switch-trans>. Each of them has demonstrated remarkably competitive few-shot performance across a wide range of tasks at the time of their respective releases. Nonetheless, these models still encounter challenges, such as overwhelming computational requirements and the need for acquiring more high-quality training data. In this work, we delve into the utilization of low-precision techniques to mitigate the training costs, which is a crucial step for the continued expansion of language models. Low-precision training has been widely used in LLM training to reduce compute cost. OPT <cit. opt> and GLM <cit. glm-130b> utilize FP16 for forwards and backwards and FP32 for optimizer states and master weights, to reduce the GPU memory usage and improve training efficiency. Bloom <cit. bloom> find that FP16 can cause numerical instabilities and irreversible divergences, especially when training models larger than 100B parameters, because FP16’s dynamic range is limited. Consequently, Bloom and other LLMs, such as Gopher <cit. gopher> and Chinchilla <cit. chinchilla>, adopt BF16 mixed-precision, because BF16 has a wide dynamic range that is the same as FP32. LLM training and tuning with 8-bit low-precision were not well-explored in previous works, because the hardware support for FP8 is not available before the release of Nvidia Hopper infrastructure. This work presents the first exploration of FP8 pre-training and fine-tuning for LLMs, while proposing an extremely-optimized FP8 mixed-precision scheme. We hope this work could facilitate future research in FP8 and, potentially, extend to exploring even lower precision training, such as 4-bit and 1-bit. \n\nConclusion\n==========\nIn this work, we explore 8-bit training for LLMs. We introduce a new FP8 mixed-precision training framework, which incorporates 8-bit collective communication, optimizer, and distributed parallel training in an incremental manner. To our best knowledge, this is the first work infiltrating FP8 compute, storage and communication into the whole progress of large language model training. Extensive experiments demonstrate the proposed method effectively diminishes communication overhead and curtails memory utilization in the context of GPT model training at various scales. In future work, we plan to scale up the size and training steps of the FP8 GPT models and further train them with our 8-bit mixed-precision scheme. Moreover, we will also use the proposed FP8 scheme to train multi-modal large models, and explore low-bit deployment of LLMs on various edge devices, such as smart phones. \n \n\nContribution and Acknowledgement\n================================\n <label. contribution> This project was initially proposed by Han Hu and Peng Cheng, who are the directional lead. Shuguang Liu served as the product lead throughout the project. The contributions for all the co-authors are detailed as follows: FP8 Framework: Kan Wu, Houwen Peng, Ze Liu, Peng Cheng, Han Hu System: Yifan Xiong, Ziyue Yang, Yuxiang Yang, Guoshuai Zhao, Peng Cheng Hardware Infrastructure: Guoshuai Zhao, Yuxiang Yang, Yifan Xiong, Peng Cheng, Shuguang Liu, Joe Chau Data: Ruihang Li, Miaosen Zhang, Jia Ning, Chen Li, Ruizhe Wang, Houwen Peng, Han Hu Pre-training: Yixuan Wei, Kan Wu, Ze Liu, Miaosen Zhang, Zheng Zhang, Houwen Peng, Han Hu Alignment (SFT, RS, and RLHF): Bolin Ni, Jingcheng Hu, Yixuan Wei, Houwen Peng, Han Hu Evaluation: Yixuan Wei, Bolin Ni, Jingcheng Hu Product Engineering: Yuxiang Yang, Kan Wu, Yifan Xiong, Ziyue Yang, Guoshuai Zhao, Peng Cheng \n We thank Eric Chung, Bita Darvish Rouhani, Yu Pei, Hyunseung Harry Yoo, Zhenghong Zhou, Gongrui Zhang, and Zhirong Wu for helpful discussions. We thank Baining Guo and Lidong Zhou for their guidance and support for this project. plainnat \n\nAppendix\n========\n\n\nFP8 Data Formats\n----------------\n <label. appendix.A> In September 2022, NVIDIA, ARM, and Intel published FP8 specification for standardization as an interchange format for AI <cit. fp8-dl>. The industry has moved from 32-bit precision to 16-bit, and now even 8-bit precision for AI model training. This development reflects a broader industry trend that has transitioned from high-precision to low-precision training. Notably, the proposed FP8 specification introduces two distinct data types, E5M2 and E4M3, which offer a trade-off between a larger range and higher precision of stored values <cit. te-fp8>. \n - E4M3 consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448 and NaN. \n\n- E5M2 consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa. It can store values up to +/-57344, +/- inf and NaN. \n\nThe FP8 format <cit. fp8-dl> roughly follows the IEEE 754 standard. Compared to higher precision data formats such as FP16 and FP32, FP8 suffers from two kinds of representation degradation: \n - Lower representation range. The representation range in a data format specifies the range between the maximum and minimum values that the format can accurately represent. There are two modes, a normal mode, which defines a regular range with relatively constant precision, and a subnormal mode, which extends the range to represent smaller values with lower precision. The normal range primarily depends on the number of exponent (E) bits, with more E bits resulting in a larger normal range. On the other hand, the subnormal range is primarily influenced by the number of mantissa (M) bits, where an increase in M bits leads to a larger subnormal range. As illustrated in Tab. <ref. tab.fp8range>, the representation range of FP8 is notably narrower compared to that of FP16 and FP32, especially in the case of the S1E4M3 sub-format (S denotes the sign bit). This discrepancy represents the primary challenge when employing FP8 for training large models. \n\n- Lower representation precision. The limited number of mantissa (M bits) leads to quantization representation errors. Due to the considerably fewer M bits in FP8, the representation precision of FP8 is substantially lower than that of FP16, as depicted in Tab. <ref. tab.fp8range>. This challenge stands as another significant hurdle when considering the use of FP8 for training large models. \n\nFP8 consists of two sub-formats: S1E4M3 and S1E5M2. The former offers a narrower representation range but higher precision, while the latter provides a larger range but lower precision. These two sub-formats give users the flexibility to strike a balance between their requirements for range and precision in model training. \n\n Representation range and error for different data formats 0.950.0pt! Data format \n\n     3c|Representation Range \n\n     2cMaximum Relative Error \n\n\n 2-6 \n\n     Max normal \n\n     Min normal \n\n     Min subnormal \n\n     Min - Max (normal) \n\n     Min ∼ Max (subnormal) \n\n\n [c]FP32 \n  (S1E8M23)\n\n \n\n     3.40 × 10^38 \n\n     1.18 × 10^-38 \n\n     1.40 × 10^-45 \n\n     1.19 × 10^-7∼ 5.96 × 10^-8 \n\n     5.00 × 10^-1∼ 1.19 × 10^-7 \n\n\n [c]FP16 \n  (S1E5M10)\n\n \n\n     65,504 \n\n     6.10 × 10^-5 \n\n     5.96 × 10^-8 \n\n     9.76 × 10^-4∼ 4.89 × 10^-4 \n\n     5.00 × 10^-1∼ 9.78 × 10^-4 \n\n\n [c]BF16 \n  (S1E8M7)\n\n \n\n     3.39 × 10^38 \n\n     1.18 × 10^-38 \n\n     9.18 × 10^-41 \n\n     7.75 × 10^-3∼ 3.94 × 10^-3 \n\n     5.00 × 10^-1∼ 7.94 × 10^-3 \n\n\n [c]FP8 \n  (S1E4M3)\n\n \n\n     448 \n\n     1.56 × 10^-2 \n\n     1.95 × 10^-3 \n\n     1.11 × 10^-1∼ 7.69 × 10^-2 \n\n     5.00 × 10^-1∼ 1.67 × 10^-1 \n\n\n [c]FP8 \n  (S1E5M2)\n\n \n\n     57,344 \n\n     6.10 × 10^-5 \n\n     1.53 × 10^-5 \n\n     2.00 × 10^-1∼ 1.67 × 10^-1 \n\n     5.00 × 10^-1∼ 5.00 × 10^-1 \n\n\n   <label. tab.fp8range> \n\n\n\nFP8 Tensor Scaling\n------------------\n <label. appendix.B> We now discuss the underlying mechanisms for how large model training with FP8 overcomes the challenges associated with representation range and precision degradation. The key technique behind is tensor scaling, which scales the tensor values that originally locate out the representation range of a data format to its comfort zone, as visualized in Fig. <ref. fig:tensorscaling>. The pioneer scaling techniques <cit. amp,apex> apply a global scaling factor to the loss, such that gradients of all layers are scaled by a single adaptive factor. The utilization of the global loss scaling technique, in conjunction with various other training strategies, has facilitated the widespread adoption of FP16 mixed-precision training on V100 and A100 GPUs. Remarkably, this approach has resulted in minimal to no degradation in accuracy, particularly for small to medium-sized models <cit. amp>. Nonetheless, when dealing with super-large models or complex tasks, such as in the training of models like DALL-E <cit. dalle>, the global loss scaling technique still encounters significant underflow issues. As a consequence, block-wise <cit. dalle> and layer-wise <cit. 4bit> gradient scaling are proposed. While the global scaling technique enables almost no accuracy drop for FP16 training (with a range of [5.96E-8, 6.55E+4]), the fine-grained per-tensor scaling will enable stable model training using even shallower range by FP8 (with a range of [1.95E-3, 448] for E4M3 and a range of [1.53E-5, 5.73E+4] for E5M2). Fig. <ref. fig:tensorscaling> shows that the representation range of FP8 has been large enough to deal with general model training. In the per-tensor scaling technique, various strategies are available for choosing the suitable scaling factor for a given FP8 tensor. Two common approaches are “just-in-time scaling\" and “delayed scaling\" <cit. te-fp8>. \n - Just-in-time scaling. This strategy involves determining the scaling factor based on the maximum absolute value (amax) of the tensor being generated. However, in practical applications, this approach is often infeasible because it necessitates multiple passes through the data. Specifically, the operator first produces and writes out the output in higher precision, then calculates the maximum absolute value of the output, and finally applies this scaling factor to all values to obtain the final FP8 output. This process introduces a significant amount of overhead, which can substantially reduce the benefits of using FP8. \n\n- Delayed scaling. This strategy involves selecting the scaling factor based on the maximum absolute values observed in a certain number of preceding iterations. This approach allows for the full performance benefits of FP8 computation but necessitates the storage of a history of maximum values as additional parameters of the FP8 operators. \n\n<figure. fig:tensorscaling - Scaling gradients to fall within the representation range of the FP8 datatype.>\n\nPre-training Data\n-----------------\n <label. appendix.data> Tab. <ref. tab.data> presents an overview of our collected data sources along with the corresponding sampling weights employed in pre-training. The arXiv and StackExchange subsets are collected from Redpajama <cit. redpajama>, while BookCorpus2 <cit. bookcorpus>, Books3 <cit. Presser2020Books3>, DM-Math <cit. dm-math>, Gutenberg <cit. pg19>, HackerNewshttps://news.ycombinator.com, NIH ExPorterhttps://exporter.nih.gov, OpenSubtitles <cit. opensubtitles>, and USPTOhttps://bulkdata.uspto.gov subsets are extracted from The Pile <cit. pile>. The Wikipedia data is downloaded from HuggingFace <cit. wikihf>. We use the 20220301 dump, including 24 languages: bg, ca, cs, da, de, en, es, fr, hi, hr, hu, it, jp, ko, nl, pl, pt, ro, ru, sl, sr, sv, uk, zh. We pre-process 11 CommonCrawl snapshots, ranging from 2018 to 2023, with the CCNet pipeline <cit. wenzek2019ccnet>. This process involves data deduplication at the line level, followed by language identification utilizing a fastText linear classifier <cit. fasttext> to eliminate non-English pages. A filtering mechanism based on an n-gram language model is employed to exclude low-quality content. In addition, we train a linear classifier <cit. redpajama> to distinguish documents similar to Wikipedia pages from randomly sampled CommonCrawl documents. Documents not classified as resembling Wikipedia are excluded. Finally, we perform fuzzy deduplication <cit. lee_deduplicating_2022> across all the processed snapshots from CommonCrawl. We collect Python code data from Github using a repository list provided by Bing indexing <cit. bing>. The cleaning of the code data includes three steps. First, we remove control characters, except for \\ t and \\ n. Next, we remove copyright comments in the code. An alphanumeric rate filter is then applied, removing lines with a rate below 0.5 if they are comments, and discarding the entire file if its overall alphanumeric rate is less than 0.98. Files with less than 5 lines or a maximum line length exceeding 1,000 characters are also discarded. Also, files with an average line length of more than 100 characters are discarded. Lastly, a pattern search is conducted to identify key Python keywords (e.g., import, from, def, class, if, for, try, etc.) within the code. Files containing less than 3 instances of these keywords are eliminated. This comprehensive process ensures that the remaining Python code data is of high quality and suitable for use in academic research. We additionally add Python code from Stack <cit. kocetkov2022stack>, and perform fuzzy deduplication within all the collected Python code. \n\n          Dataset                     Sampling prop.                     Epochs               Training Tokens (Billion) \n========================================================================================================================\n         Web Crawls                     Web Crawls                     Web Crawls                    Web Crawls         \n        CommonCrawl                       51.71\n             C4                           25.56\n        OpenWebText                       2.73\nTechnical      Science content   Technical      Science content   Technical      Science content      Technical      Science    \n                                                                                                       content          \n           ArXiv                          1.54\n       StackExchange                      1.42\n          DM-Math                         0.39\n           USPTO                          0.52\n        NIH ExPorter                      0.04\n   Programming Languages          Programming Languages          Programming Languages          Programming Languages   \n           Python                         4.50\n   Other Curated Sources          Other Curated Sources          Other Curated Sources          Other Curated Sources   \n         Wikipedia                        4.50\n           Books                          4.50\n            News                          2.00\n          Dialogue                        2.00\n           Total                          Total                          Total                           100            \nTable: Pre-training data. For each subset we list the sampling weight, number of epochs, and training tokens. Books data includes BookCorpus2 <cit. bookcorpus\n\n> ",
  "subsections": [
    {
      "title": "",
      "content": "               fancy empty    [LO] \nheight 4pt18.0675pt-0.0pt FP8-LM: Training FP8 Large Language Models  14.454pt-0.0ptheight 1pt6.5043pt  Houwen Peng ^*  Kan Wu ^*  Yixuan Wei ^* \n Guoshuai Zhao  Yuxiang Yang  Ze Liu  Yifan Xiong  Ziyue Yang \n Bolin Ni  Jingcheng Hu  Ruihang Li  Miaosen Zhang  Chen Li  Jia Ning  Ruizhe Wang  Zheng Zhang \n Shuguang Liu  Joe Chau  Han Hu ^†  Peng Cheng ^†  \n \n Microsoft Azure and Microsoft Research   ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Introduction",
      "content": "Large language models (LLMs) <cit. gpt3, megatron-nlg, palm, opt> have demonstrated unprecedented capabilities in language comprehension and generation, leading to breakthroughs in reasoning, math, science, and many other tasks <cit. gpt4, palm2>. However, training LLMs is extremely costly. For example, PaLM takes 6,144 TPUv4 chips to train a 540B model, while GPT-3 175B consumes several thousand petaflop/s-days of compute for pre-training <cit. palm, gpt3>. This motivates the needs of reducing the training costs of LLMs, especially for the scaling of next-generation super-intelligent models. Low-precision training is one of the most promising directions to reduce the costs, as it can provide high speed, small memory footprint, and low communication overhead. Most existing training systems, e.g., Megatron-LM <cit. megatron-lm>, MetaSeq <cit. opt>, and Colossal-AI <cit. colossal-ai>, train LLMs with either FP32 full-precision or FP16/BF16 mixed-precision by default. This is not essential, however, to achieve full accuracy for large models. With the release of Nvidia H100 GPU, FP8 is becoming the next-generation datatype for low-precision representation <cit. h100-whitepaper, fp8-dl>. Theoretically, FP8 can achieve 2x speed-up, 50%  - 75%  memory cost savings, and 50%  - 75%  communication savings compared with current 16-bit and 32-bit floating point mixed-precision training, which is very promising for scaling-up next-generation foundation models. Unfortunately, the current support for FP8 training is rare and limited. The only usable framework is the Nvidia Transformer Engine (TE) <cit. te>, but it applies FP8 solely for GEMM computation and still retains master weights and gradients using high precision, e.g., FP16 or FP32. As a result, the end-to-end speed-up, memory and communication cost savings are very limited, which does not fully unveil the power of FP8. To address this issue, we propose an extremely optimized FP8 mixed-precision framework for LLM training. The core idea is to infiltrate FP8 compute, storage, and communication into the whole progress of large model training, making the forward and backward pass all used the low-precision FP8, thus largely reducing system workloads compared to previous frameworks <cit. amp, te, fp8-dl>. Specifically, we design three optimization levels that utilize FP8 to streamline mixed-precision and distributed training. The three levels gradually incorporate 8-bit collective communication, optimizer, and distributed parallel training in an incremental manner. The higher optimization level indicates using more FP8 during LLM training. Moreover, for large-scale training, such as GPT-175B trained on thousand of GPUs, our framework provides FP8 low-bit parallelism, including tensor, pipeline, and sequence parallelism, paving the way to next-generation low-precision parallel training. <figure. fig.gpt - An analysis of comparing the maximum model sizes attainable through the utilization of either the prevalent BF16 or our FP8 mixed-precision training approach on a cluster of Nvidia H100 GPUs with 80G memory.>Training LLMs with FP8 is non-trivial. The challenges stem from issues such as data underflow or overflow, coupled with quantization errors arising from the narrower dynamic range and reduced precision inherent in FP8 data formats. These challenges cause numerical instabilities and irreversible divergences throughout the training process. To tackle them, we propose two techniques: precision decoupling and automatic scaling for preventing the loss of critical information. The former one involves decoupling the influence of data precision on parameters such as weights, gradients, optimizer states, and assigning reduced precision to components that are not precision sensitive. The latter one is to preserve gradient values within the representation range of FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflow and overflow occurrences during all-reduce communication. To validate the proposed FP8 low-precision framework, we apply it to GPT-style model training, encompassing both pre-training and supervised fine-tuning (SFT). The experimental results demonstrate the effectiveness of our FP8 methodology, yielding substantial benefits including a 27%  to 42%  reduction in real memory usage (e.g., 27%  reduction for GPT-7B while 42%  for GPT-175B ) and a notable 63%  to 65%  decrease in weight gradient communication overhead compared to the prevalent BF16 mixed-precision training approach. Without changes to any hyper-parameters, such as learning rate and weight decay, the models trained using FP8 exhibit performance equivalency to those employing BF16 high precision, both in pre-training and downstream tasks. It is noteworthy that during the training of GPT-175B model, our FP8 mix-precision framework reduces training time by 17%  compared to TE <cit. te>, while consuming 21%  less memory on H100 GPU platform. More importantly, the reduction in costs achieved through the utilization of low-precision FP8 can be further increased, as the scale of models continues to expand, which is presented in Fig.  <ref. fig.gpt>. For fine-tuning, we employ FP8 mixed-precision for instruction tuning and reinforcement learning with human feedback (RLHF) to better align pre-trained LLMs with end tasks and user preferences. Specifically, we fine-tune pre-trained models on publicly user-shared instruction-following data <cit. sharegpt>. The models tuned with our FP8 mixed-precision demonstrate comparable performance to those utilizing the half-precision BF16 <cit. mt-bench> on the AlpacaEval <cit. alpaca_eval> and MT-Bench <cit. mt-bench> benchmarks, while achieving 27%  improvements in training speed. Moreover, FP8 mixed-precision exhibits considerable potentials in RLHF, a process that necessitates loading multiple models during training. Through the utilization of FP8 in training, the prevalent RLHF framework AlpacaFarm <cit. alpacafarm> can yield a 46%  reduction in model weights and a 62%  reduction in optimizer states’ memory consumption. This further demonstrates the versatility and adaptability of our FP8 low-precision training framework. We are making the following contributions to drive the design of next-generation FP8 low-precision training for LLMs. \n - A new FP8 mixed-precision training framework. It unlocks 8-bit weights, gradients, optimizer, and distributed training gradually in an add-on fashion, which is convenient in use. This 8-bit framework can be used as a simple drop-in replacement for existing 16/32-bit mixed-precision counterparts, without requiring any changes to the hyper-parameters and training receipts. Additionally, we provide a Pytorch implementation that enables 8-bit low-precision training in a few lines of code. \n\n- A new family of GPT-style models trained with FP8. We apply the proposed FP8 scheme to GPT pre-training and fine-tuning (i.e., SFT and RLHF), and demonstrate its potentials on a variety of model scales ranging from 7B to 175B parameters. We equip prevalent parallel computation paradigms with FP8 supports, including tensor, pipeline, and sequence parallelisms, enabling the utilization of FP8 to train large foundation models. We open-source the first FP8 GPT training codebase based upon Megatron-LM <cit. megatron-lm> implementation. \n\nWe expect the release of our FP8 framework will establish a new paradigm for next-generation low-precision training system dedicated to large foundation models.",
      "subsections": [],
      "figures": {
        "fig.gpt": {
          "label": "fig.gpt",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/gpt_size3.pdf"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": 208,
              "width": null
            }
          ],
          "caption": "An analysis of comparing the maximum model sizes attainable through the utilization of either the prevalent BF16 or our FP8 mixed-precision training approach on a cluster of Nvidia H100 GPUs with 80G memory.",
          "section": "2. Introduction"
        }
      }
    },
    {
      "title": "FP8 LLMs",
      "content": "Mixed-precision <cit. amp> has been widely used in LLM training to improve compute and memory efficiency. The most popular mixed-precision schemes are FP16-FP32 and BF16-FP32. Because of the restricted numerical range of FP16, FP16-FP32 scheme has been known instabilities for training large models <cit. gopher,glm-130b>. Consequently, the community now commonly adopts BF16-FP32 for training LLMs, such as Megatron-Turing NLG-530B <cit. megatron-nlg>, Bloom-175B <cit. bloom> and Gopher <cit. gopher>. The underlying reason is that BF16 has a wide dynamic range to maintain numerical stability while matching the performance of the full-precision FP32. Moreover, BF16 employs half the number of bits as compared to FP32, thus reducing considerable memory footprints while improving compute efficiency. FP8 is a natural evolution from 16-bit data formats to further reducing computing costs. However, training LLMs with reduced-precision FP8 poses new challenges. The dynamic range and representation precision of FP8The details of FP8 data formats are presented in Appendix  <ref. appendix.A>. are much lower than BF16 and FP16, which inevitably induces more training collapses, such as loss spikes or even NaNs. To address the issues, tensor scaling techniques are proposed <cit. sunxiao-fp8, fp8-dl>. The core idea is multiplying higher precision values with a scaling factor prior to their casting to FP8 in order to move them into a range that better overlaps with the representable range of a corresponding FP8 formatThe details of FP8 tensor scaling are introduced in Appendix <ref. appendix.B>. <cit. fp8-dl>. Such a per-tensor scaling technique reduces data quantization errors while improving numerical stability and accuracy, thus enabling the utilization of the lower-precision FP8 for training large models. Unfortunately, the current support for FP8 low-precision training is restricted. Nvidia TE <cit. te> only supports FP8 compute for linear layers in Transformer <cit. transformer>, while leaving all other operations, such as weight update and gradient synchronization, still using higher precision. In this work, we present an extremely optimized FP8 mixed-precision strategy for LLM training. The new FP8 optimization includes three key perspectives: FP8 communication, FP8 optimizer, and FP8 distributed training. By integrating these aspects, the training of LLMs such as the 175B GPT-3 model can fully harness the advantages of FP8 low-precision and improve training efficiency. \n\nFP8 Gradient and All-Reduce Communication\n-----------------------------------------\n <label. sec.fp8grad> Existing mixed-precision training methodologies <cit. amp, te> typically employ 16-bit or 32-bit datatype for the computation and storage of gradients, resulting in a high bandwidth requirement for collective communication throughout the training process. We found that directly applying FP8 to gradients leads to a decrease in accuracy. The fundamental issue lies in the underflow and overflow problems arising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregating gradients across GPUs during all-reduce: pre-scaling and post-scaling. Pre-scaling divides the gradient g_i calculated on the i-th GPU by the total number of GPUs (i.e., N) before being summed, which is formulated as: \n    g = g_1/N + g_2/N + ⋯ +g_N/N.\n When N is large, this division can cause data underflow, especially for FP8 low-precision representation of gradients. To mitigate this issue, post-scaling performs the gradient summation first, followed by the division scaling during the gradient collection process: \n    g = (g_1 + g_2 + ⋯ + g_N)/N.\n This post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectively alleviating the underflow issue. However, this approach encounters overflow issues when aggregating gradients. In contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issues in the pre-scaling and post-scaling approaches. To be specific, we introduce an auto-scaling factor μ, that changes on the fly during the training, to reduce the occurrences of overflow and underflow in gradients: \n    g'_i = μ· g_i. <label. eq_mu>\n A statistical analysis is conducted on the gradient values of g'_i, with the objective of quantifying the proportion of values that attains the maximum feasible value within the FP8 representation range. If the ratio of the maximum value exceeds a specified threshold, i.e., 0.001% , μ is set to 1/2 in the subsequent training step, thereby mitigating the risk of overflow. Conversely, when the ratio consistently remains the threshold, we opt to exponentially increase μ to 2 over the span of 1,000 training steps, thereby effectively mitigating the risk of underflow occurrences. Another key obstacle of FP8 collective communication lies in devising an effective strategy to manage the tensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation <cit. nccl> lacks the capability of performing all-reduce operation considering the additional tensor-wise scaling factors. Meanwhile, efficient implementation is also very challenging, especially considering that the NCCL gradient summation operates at sub-tensor level. This complexity increases significantly when incorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanism that scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g'_i, s'_i) denote a scaling tensor which stores the weight gradient in the i-th GPU, where g'_i is a FP8 tensor and s'_i is the corresponding scaling factor. The actual weight gradient is g’_i / s’_i. Prior to the all-reduce operation over gradient tensors, we first gather the scaling factors s'_i of each gradient tensor on all GPUs and calculate the global minimum scaling factor s'_g as: \n    s'_g = min(s'_1,   s'_2, … ,   s'_N),\n where the global minimum scaling factor s'_g is shared across GPUs. We use this shared scaling factor s'_g to unify the rescaling of the gradient tensors across GPUs. In this way, all gradient tensors associated with the same weight use the same shared scaling factor to quantize the tensors into FP8 format on all GPUs: \n    g”_i = FP8(s'_g·(g'_i / s'_i)). <label. eq.global>\n This approach reduces communication overhead by transmitting only a single scalar s'_g, making the additional synchronization step highly efficient. As the input tensors share the same scaling factor, it eliminates the need of considering all-reduce the scaling factors in parallel and allows for standard NCCL all-reduce operation to be performed. The final collected gradient is obtained as follows: \n    g = g”_1 + g”_2 + ⋯ + g”_N,      s = N · s'_g,\n where g is the final aggregated gradient and s is the corresponding scaling factor. Rescaling the scaling factor for the summed gradient g is equivalent to dividing g by N in theory. By implementing the aforementioned dual strategies of distributed and automated scaling, we can successfully realize FP8 low-bit gradient communication while preserving model accuracy. Furthermore, this approach entails storing gradients in FP8 and conducting communication in FP8 as well, thereby yielding reductions in GPU memory usage and communication bandwidth consumption. \n\nFP8 Optimizer\n-------------\nIn the training of LLMs, Adam and its variants <cit. adam, adamw> are the most frequently-used optimization methods, that maintain copies of model weights, gradients, first-order and second-order gradient moments for model updates. Mixed-precision training <cit. amp> with Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float format for numerical stability <cit. megatron-lm,zero-deepspeed, opt, bloom>. Consequently, the Adam optimizer consumes 16 bytes of memory per parameter during training: \n    4_master weights + 4_gradients +      4    +     4  _Adam states    =     16 bytes. <label. eq:16bitamp>\n When model size is large, the memory consumption of the variables in Adam will become a bottleneck. Previous work <cit. gopher, glm-130b, swinv2> has revealed that reducing precision of the variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale modelsBF16 lacks the precision needed for accuracy, while FP16 has a restricted dynamic range. Given these challenges, prevalent mixed-precision training methodologies rely on utilizing FP32 full-precision for master weights, gradients, and gradient moments.. This prompts an evaluation of which variables in the optimizer should be allocated high precision and which can be accommodated with low-precision. To clarify, we decouple the influence of data precision on the variables in the optimizer and investigate which one can be assigned lower precision, i.e., precision decoupling. We find a guiding principle: the gradient statistics can use lower precision, while the master weights necessitate high precision. More concretely, the first-order gradient moment can tolerate a high quantization error and can be assigned with low-precision FP8, while the second-order moment requires a higher precision, as analyzed in Sec. <ref. sec.ablation>. This stems from the fact that, during model updates in Adam, the direction of the gradient holds greater significance than its magnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as the high-precision tensor, though it introduces precision degradation to some extend. Calculating the square of gradients for the second-order gradient moment might lead to data underflow due to the typically small gradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy. On the other hand, we find that it is crucial to keep the master weights using high precision. The underlying reason is that weight updates can sometimes become extremely small or large during training, higher precision for the master weights helps prevent loss of information when updating weights, ensuring more stable and accurate training. In the implementation, the master weights have two viable options: utilizing either FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage of conserving memory without compromising accuracy. Consequently, our default choice is to employ FP16 with tensor scaling for storing master weights in the optimizer. Our FP8 mixed-precision optimizer consumes 6 bytes of memory per parameter during training: \n    2_master weights + 1_gradients +      1    +     2  _Adam states    =     6 bytes.\n This new low-bit optimizer reduces memory footprints by 2.6x in comparison to the previous solution, as exemplified in Eq.  (<ref. eq:16bitamp>). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments in Sec.  <ref. sec.mainresults> show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175B parameters. \n\nFP8 Distributed Parallel Training\n---------------------------------\nTraining LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. The frequently-used strategies include data parallelism, tensor parallelism, pipeline parallelism, and sequence parallelism. Each parallelism has its own merits and has been used in a complementary fashion in existing systems <cit. megatron-nlg, megatron-lm, opt, bloom, colossal-ai>. For FP8 supports of these strategies, neither data parallelism nor pipeline parallelism necessitates any specific modifications, because they do not involve additional FP8 compute and communication when splitting data batches or model layers into segments across devices. Tensor parallelism partitions individual layers of a model across multiple devices, such that the shards of weight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensor parallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layer computation, enabling the forward compute and backward gradient collective communication all using FP8. On the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequences are fed to different devices to save activation memory. As shown in Fig. <ref. fig:distribute>, sequence and tensor parallelism are performed in parallel to different parts of a Transformer model to make the best use of the available memory and improve training efficiency. There is a converter g between sequence and tensor parallel regions to all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backward pass). We add an FP8 datatype conversion prior to g, such that the all-gather (or reduce-scatter) operation uses FP8 low-bit activation to save communication cost across GPUs. <figure. fig:distribute - Transformer layer with FP8 tensor and sequence parallelism. The FP8 low-bit operation is highlighted with orange. g is all-gather in forward pass and reduce-scatter in backward pass, while g̅ is reduce-scatter in forward pass and all-gather in backward pass. The gather-reduce operation g between sequence parallel and tensor parallel is executed utilizing FP8 low-precision activation, thus saving half communication costs.><figure. fig:scalingZeRO - ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO method, which splits a single tensor into multiple partitions and distributes them to different devices. Right: the proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor scaling into account.>In addition, Zero Redundancy Optimizer (ZeRO) <cit. zero-deepspeed> is another frequently-used distributed learning technique in large model training. The core idea of ZeRO is to shade model states over devices, such that each device only hold a fraction of data (e.g., master weights, gradients, and optimizer states) required for a training step. To reduce memory consumption, ZeRO method generally splits a single tensor into multiple partitions and distributes them to different devices. Directly applying FP8 to ZeRO is infeasible, because it is difficult to handle the scaling factors associated with the FP8 partitions. The per-tensor scaling factors should be distributed along with FP8 partitions. To address this issue, we implement a new FP8 distribution scheme that distributes each tensor as a whole across devices, rather than partitioning it into multiple sub-tensors as in ZeRO <cit. zero-deepspeed>. The distribution of FP8 tensors is processed in a greedy manner, as outlined in Alg. <ref. alg:greedy>. Specifically, our method first sorts the tensors of model states according to their sizes, and then distributes the tensors to different GPUs based upon the remaining memory size of each GPU. The distribution follows the principle that the GPUs with larger remaining memory get a higher priority in receiving new distributed tensors. In this way, the tensor scaling factors can be distributed along with the tensors smoothly, while reducing communication and compute complexity. Figure <ref. fig:scalingZeRO> presents a visual illustration of the difference in ZeRO tensor partitioning between scenarios with and without scaling factors.  [!b] Greedy Distribution Algorithm for ZeRO <label. alg:greedy>  [1] FP8 tensors with their corresponding scaling factors: T={ (s_1, t_1),(s_2, t_2),… ,(s_n, t_n)}, where s denotes scaling factors while t represents 8-bit tensors. The size of each tensor: C={ c_1,c_2,… ,c_n}. Partitions representing scaling tensors assigned to each GPU. Sort T in descending order of their sizes to get T'={ (s'_1, t'_1),(s'_2, t'_2),… ,(s'_n, t'_n)} and C'={ c'_1,c'_2,… ,c'_n}, where c'_1⩾ c'_2⩾…⩾ c'_n. Initialize memory usage u_j=0 and partition p_j=∅ for each GPU G_j. i=1 to n j min _j u_j Find the GPU j ∈ [1,m] with the least memory usage. p_j p_j∪{ (s'_i,t'_i)} Assign (s'_i,t'_i) to G_j. u_j  u_j+ c'_i Update the memory usage of G_j. Partitions P={ p_1,p_2,… ,p_m}   ",
      "subsections": [
        {
          "title": "FP8 Gradient and All-Reduce Communication",
          "content": " <label. sec.fp8grad> Existing mixed-precision training methodologies <cit. amp, te> typically employ 16-bit or 32-bit datatype for the computation and storage of gradients, resulting in a high bandwidth requirement for collective communication throughout the training process. We found that directly applying FP8 to gradients leads to a decrease in accuracy. The fundamental issue lies in the underflow and overflow problems arising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregating gradients across GPUs during all-reduce: pre-scaling and post-scaling. Pre-scaling divides the gradient g_i calculated on the i-th GPU by the total number of GPUs (i.e., N) before being summed, which is formulated as: \n    g = g_1/N + g_2/N + ⋯ +g_N/N.\n When N is large, this division can cause data underflow, especially for FP8 low-precision representation of gradients. To mitigate this issue, post-scaling performs the gradient summation first, followed by the division scaling during the gradient collection process: \n    g = (g_1 + g_2 + ⋯ + g_N)/N.\n This post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectively alleviating the underflow issue. However, this approach encounters overflow issues when aggregating gradients. In contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issues in the pre-scaling and post-scaling approaches. To be specific, we introduce an auto-scaling factor μ, that changes on the fly during the training, to reduce the occurrences of overflow and underflow in gradients: \n    g'_i = μ· g_i. <label. eq_mu>\n A statistical analysis is conducted on the gradient values of g'_i, with the objective of quantifying the proportion of values that attains the maximum feasible value within the FP8 representation range. If the ratio of the maximum value exceeds a specified threshold, i.e., 0.001% , μ is set to 1/2 in the subsequent training step, thereby mitigating the risk of overflow. Conversely, when the ratio consistently remains the threshold, we opt to exponentially increase μ to 2 over the span of 1,000 training steps, thereby effectively mitigating the risk of underflow occurrences. Another key obstacle of FP8 collective communication lies in devising an effective strategy to manage the tensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation <cit. nccl> lacks the capability of performing all-reduce operation considering the additional tensor-wise scaling factors. Meanwhile, efficient implementation is also very challenging, especially considering that the NCCL gradient summation operates at sub-tensor level. This complexity increases significantly when incorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanism that scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g'_i, s'_i) denote a scaling tensor which stores the weight gradient in the i-th GPU, where g'_i is a FP8 tensor and s'_i is the corresponding scaling factor. The actual weight gradient is g’_i / s’_i. Prior to the all-reduce operation over gradient tensors, we first gather the scaling factors s'_i of each gradient tensor on all GPUs and calculate the global minimum scaling factor s'_g as: \n    s'_g = min(s'_1,   s'_2, … ,   s'_N),\n where the global minimum scaling factor s'_g is shared across GPUs. We use this shared scaling factor s'_g to unify the rescaling of the gradient tensors across GPUs. In this way, all gradient tensors associated with the same weight use the same shared scaling factor to quantize the tensors into FP8 format on all GPUs: \n    g”_i = FP8(s'_g·(g'_i / s'_i)). <label. eq.global>\n This approach reduces communication overhead by transmitting only a single scalar s'_g, making the additional synchronization step highly efficient. As the input tensors share the same scaling factor, it eliminates the need of considering all-reduce the scaling factors in parallel and allows for standard NCCL all-reduce operation to be performed. The final collected gradient is obtained as follows: \n    g = g”_1 + g”_2 + ⋯ + g”_N,      s = N · s'_g,\n where g is the final aggregated gradient and s is the corresponding scaling factor. Rescaling the scaling factor for the summed gradient g is equivalent to dividing g by N in theory. By implementing the aforementioned dual strategies of distributed and automated scaling, we can successfully realize FP8 low-bit gradient communication while preserving model accuracy. Furthermore, this approach entails storing gradients in FP8 and conducting communication in FP8 as well, thereby yielding reductions in GPU memory usage and communication bandwidth consumption. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "FP8 Optimizer",
          "content": "In the training of LLMs, Adam and its variants <cit. adam, adamw> are the most frequently-used optimization methods, that maintain copies of model weights, gradients, first-order and second-order gradient moments for model updates. Mixed-precision training <cit. amp> with Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float format for numerical stability <cit. megatron-lm,zero-deepspeed, opt, bloom>. Consequently, the Adam optimizer consumes 16 bytes of memory per parameter during training: \n    4_master weights + 4_gradients +      4    +     4  _Adam states    =     16 bytes. <label. eq:16bitamp>\n When model size is large, the memory consumption of the variables in Adam will become a bottleneck. Previous work <cit. gopher, glm-130b, swinv2> has revealed that reducing precision of the variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale modelsBF16 lacks the precision needed for accuracy, while FP16 has a restricted dynamic range. Given these challenges, prevalent mixed-precision training methodologies rely on utilizing FP32 full-precision for master weights, gradients, and gradient moments.. This prompts an evaluation of which variables in the optimizer should be allocated high precision and which can be accommodated with low-precision. To clarify, we decouple the influence of data precision on the variables in the optimizer and investigate which one can be assigned lower precision, i.e., precision decoupling. We find a guiding principle: the gradient statistics can use lower precision, while the master weights necessitate high precision. More concretely, the first-order gradient moment can tolerate a high quantization error and can be assigned with low-precision FP8, while the second-order moment requires a higher precision, as analyzed in Sec. <ref. sec.ablation>. This stems from the fact that, during model updates in Adam, the direction of the gradient holds greater significance than its magnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as the high-precision tensor, though it introduces precision degradation to some extend. Calculating the square of gradients for the second-order gradient moment might lead to data underflow due to the typically small gradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy. On the other hand, we find that it is crucial to keep the master weights using high precision. The underlying reason is that weight updates can sometimes become extremely small or large during training, higher precision for the master weights helps prevent loss of information when updating weights, ensuring more stable and accurate training. In the implementation, the master weights have two viable options: utilizing either FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage of conserving memory without compromising accuracy. Consequently, our default choice is to employ FP16 with tensor scaling for storing master weights in the optimizer. Our FP8 mixed-precision optimizer consumes 6 bytes of memory per parameter during training: \n    2_master weights + 1_gradients +      1    +     2  _Adam states    =     6 bytes.\n This new low-bit optimizer reduces memory footprints by 2.6x in comparison to the previous solution, as exemplified in Eq.  (<ref. eq:16bitamp>). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments in Sec.  <ref. sec.mainresults> show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175B parameters. ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "FP8 Distributed Parallel Training",
          "content": "Training LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. The frequently-used strategies include data parallelism, tensor parallelism, pipeline parallelism, and sequence parallelism. Each parallelism has its own merits and has been used in a complementary fashion in existing systems <cit. megatron-nlg, megatron-lm, opt, bloom, colossal-ai>. For FP8 supports of these strategies, neither data parallelism nor pipeline parallelism necessitates any specific modifications, because they do not involve additional FP8 compute and communication when splitting data batches or model layers into segments across devices. Tensor parallelism partitions individual layers of a model across multiple devices, such that the shards of weight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensor parallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layer computation, enabling the forward compute and backward gradient collective communication all using FP8. On the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequences are fed to different devices to save activation memory. As shown in Fig. <ref. fig:distribute>, sequence and tensor parallelism are performed in parallel to different parts of a Transformer model to make the best use of the available memory and improve training efficiency. There is a converter g between sequence and tensor parallel regions to all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backward pass). We add an FP8 datatype conversion prior to g, such that the all-gather (or reduce-scatter) operation uses FP8 low-bit activation to save communication cost across GPUs. <figure. fig:distribute - Transformer layer with FP8 tensor and sequence parallelism. The FP8 low-bit operation is highlighted with orange. g is all-gather in forward pass and reduce-scatter in backward pass, while g̅ is reduce-scatter in forward pass and all-gather in backward pass. The gather-reduce operation g between sequence parallel and tensor parallel is executed utilizing FP8 low-precision activation, thus saving half communication costs.><figure. fig:scalingZeRO - ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO method, which splits a single tensor into multiple partitions and distributes them to different devices. Right: the proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor scaling into account.>In addition, Zero Redundancy Optimizer (ZeRO) <cit. zero-deepspeed> is another frequently-used distributed learning technique in large model training. The core idea of ZeRO is to shade model states over devices, such that each device only hold a fraction of data (e.g., master weights, gradients, and optimizer states) required for a training step. To reduce memory consumption, ZeRO method generally splits a single tensor into multiple partitions and distributes them to different devices. Directly applying FP8 to ZeRO is infeasible, because it is difficult to handle the scaling factors associated with the FP8 partitions. The per-tensor scaling factors should be distributed along with FP8 partitions. To address this issue, we implement a new FP8 distribution scheme that distributes each tensor as a whole across devices, rather than partitioning it into multiple sub-tensors as in ZeRO <cit. zero-deepspeed>. The distribution of FP8 tensors is processed in a greedy manner, as outlined in Alg. <ref. alg:greedy>. Specifically, our method first sorts the tensors of model states according to their sizes, and then distributes the tensors to different GPUs based upon the remaining memory size of each GPU. The distribution follows the principle that the GPUs with larger remaining memory get a higher priority in receiving new distributed tensors. In this way, the tensor scaling factors can be distributed along with the tensors smoothly, while reducing communication and compute complexity. Figure <ref. fig:scalingZeRO> presents a visual illustration of the difference in ZeRO tensor partitioning between scenarios with and without scaling factors.  [!b] Greedy Distribution Algorithm for ZeRO <label. alg:greedy>  [1] FP8 tensors with their corresponding scaling factors: T={ (s_1, t_1),(s_2, t_2),… ,(s_n, t_n)}, where s denotes scaling factors while t represents 8-bit tensors. The size of each tensor: C={ c_1,c_2,… ,c_n}. Partitions representing scaling tensors assigned to each GPU. Sort T in descending order of their sizes to get T'={ (s'_1, t'_1),(s'_2, t'_2),… ,(s'_n, t'_n)} and C'={ c'_1,c'_2,… ,c'_n}, where c'_1⩾ c'_2⩾…⩾ c'_n. Initialize memory usage u_j=0 and partition p_j=∅ for each GPU G_j. i=1 to n j min _j u_j Find the GPU j ∈ [1,m] with the least memory usage. p_j p_j∪{ (s'_i,t'_i)} Assign (s'_i,t'_i) to G_j. u_j  u_j+ c'_i Update the memory usage of G_j. Partitions P={ p_1,p_2,… ,p_m}   ",
          "subsections": [],
          "figures": {
            "fig:distribute": {
              "label": "fig:distribute",
              "path": [
                "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/distribute3.pdf"
              ],
              "size": [
                {
                  "scale": 1.0,
                  "height": null,
                  "width": 0
                }
              ],
              "caption": "Transformer layer with FP8 tensor and sequence parallelism. The FP8 low-bit operation is highlighted with orange. g is all-gather in forward pass and reduce-scatter in backward pass, while g̅ is reduce-scatter in forward pass and all-gather in backward pass. The gather-reduce operation g between sequence parallel and tensor parallel is executed utilizing FP8 low-precision activation, thus saving half communication costs.",
              "section": "3.3. FP8 Distributed Parallel Training"
            },
            "fig:scalingZeRO": {
              "label": "fig:scalingZeRO",
              "path": [
                "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/scalingZeRO.pdf"
              ],
              "size": [
                {
                  "scale": 1.0,
                  "height": null,
                  "width": 0
                }
              ],
              "caption": "ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO method, which splits a single tensor into multiple partitions and distributes them to different devices. Right: the proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor scaling into account.",
              "section": "3.3. FP8 Distributed Parallel Training"
            }
          }
        }
      ],
      "figures": {}
    },
    {
      "title": "Experiment",
      "content": "In this section, we assess the effectiveness of the proposed FP8 mixed-precision training approach on GPT-style LLMs, including a wide range of model scales, from 125 million to 175 billion parameters. For performance ablation, we compare GPT models trained with FP8 against those trained with half-precision BF16 and full-precision FP32. For generality evaluation, we conduct experiments encompassing both FP8 low-bit pre-training and fine-tuning, considering instruction tuning and human preference alignment. \n\nExperimental Setup\n------------------\n\n\nTraining Dataset\nOur pre-training data is constructed using open-sourced language collections from several sources, including CommonCrawlhttps://commoncrawl.org, The Pile <cit. pile>, C4 <cit. c4>, OpenWebText <cit. gpt2, Gokaslan2019OpenWebTextCorpus>, CC-NEWS <cit. liu2019roberta>, CC-Stories <cit. trinh2018simple>, Redpajama <cit. redpajama>, and Wikipediahttps://wikipedia.org. We apply fuzzy deduplication <cit. lee_deduplicating_2022> across CommonCrawl snapshots to enhance data quality. Tab. <ref. tab.data> in Appendix <ref. appendix.data> provides details of our pre-training data, including information such as the number of tokens from each source and associated sampling weights. For a more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer to Appendix <ref. appendix.data>. Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1<cit. vicuna>, which uses a publicly user-shared instruction following data <cit. sharegpt>. For reinforcement learning with human feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmless dataset  <cit. HH> and Open-Assistant dataset  <cit. oasst>. The training framework and associated configurations align with the publicly available AlpacaFarm  <cit. alpacafarm>. \n\nModel Configuration\n\n\nparams   dimension   n heads   n layers   TP   PP   SP   learning    batch     n tokens\n                                                           rate       size             \n=======================================================================================\n 125M       768        12         12      1    1         6.0e^-4       1M        100B  \n  7B       4096        32         32      1    1         3.0e^-4       4M        100B  \n 13B       5120        40         40      2    1         3.0e^-4       4M        100B  \n 175B      12288       96         96      8    4         3.0e^-5       1M         5B   \nTable:  Model sizes, architectures, and training hyper-parameters. TP, PP, and SP indicate tensor, pipeline, and sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of the 175B model to a dataset comprising only 5B tokens, which has proven to be sufficient for evaluating system performance.\n\nThe model architecture we used is a decoder-only Transformer <cit. gpt3>, which has been widely-used in recent generative LLMs like PaLM <cit. palm>, OPT <cit. opt>, and LLaMA <cit. llama>. In addition to the base architecture, we integrate several modifications proposed recently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspiration from recent successful experiments <cit. gpt-neox, llama>, we incorporate rotary positional embeddings (RoPE) <cit. rope> into our approach. This addition enables us to capture both absolute and relative positions information, enhancing performance especially when extrapolating to larger context windows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access <cit. ivanov2021data>. Flash Attention <cit. flashatt> proposed an IO-aware exact attention algorithm which uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration. We train the models using the proposed FP8 optimizer, which is built upon Adam <cit. adam> with decoupled weight decay <cit. adamw>, following the common practise with the decay rates β _1 = 0.9, β _2 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final learning rate is 10%  of the maximal learning rate. We train the models for 100B tokens in total with a batch size of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000 iterations. Tab. <ref. tab.arch> presents the details of model configurations and the corresponding training settings. The training is conducted on Azure NDv5 H100 GPU platform <cit. hpc>. <figure. fig:gptloss - A comparison between FP8 and BF16: Analyzing the training loss of GPT models with the parameters ranging from 7 billion to 175 billion.>\n\n \n 0.95  \n\n           HS \n\n     Lambada \n\n     BoolQ \n\n     PIQA \n\n     COPA \n\n     Winogrande \n\n     Arc-C \n\n     Arc-E \n\n     ObQA \n\n     Avg \n\n\n 8lGPT-7B model zero-shot performance \n\n    \n BF16 \n\n     61.3 \n\n     61.4 \n\n     61.2 \n\n     75.0 \n\n     79.0 \n\n     58.5 \n\n     32.9 \n\n     59.7 \n\n     36.4 \n\n     58.4 \n\n\n FP8 \n\n     60.0 \n\n     61.8 \n\n     62.0 \n\n     74.2 \n\n     78.0 \n\n     59.8 \n\n     32.9 \n\n     58.7 \n\n     34.6 \n\n     58.0 \n\n\n 8lGPT-13B model zero-shot performance \n\n    \n BF16 \n\n     64.8 \n\n     64.9 \n\n     63.4 \n\n     75.9 \n\n     82.0 \n\n     61.0 \n\n     35.2 \n\n     61.5 \n\n     40.6 \n\n     61.0 \n\n\n FP8 \n\n     64.1 \n\n     63.4 \n\n     63.9 \n\n     76.2 \n\n     81.0 \n\n     61.6 \n\n     34.9 \n\n     61.3 \n\n     36.8 \n\n     60.4 \n\n\n   Zero-shot performance on downstream tasks. The models are trained with either the standard BF16 mixed-precision scheme <cit. megatron-lm> or the proposed FP8 low-precision scheme.  <label. tab.downstreamtask> \n \n\n\n\nMain Results\n------------\n <label. sec.mainresults> \n\nModel Performance\n We first compare the performance of models trained using FP8 mixed-precision with those trained using BF16. In Fig.   <ref. fig:gptloss>, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. The training configurations and hyper-parameters remain consistent across models trained with FP8 and BF16. The only difference lies in the mixed-precision schemes utilized. As shown in Fig. <ref. fig:gptloss>, the loss curves almost overlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme <cit. megatron-lm, gopher, chinchilla> across a diverse array of model scales. Also, we evaluate the pre-trained models on a wide range of downstream tasks, including HellaSwag (HS) <cit. hellaswag>, Lambada <cit. lambada> BoolQ <cit. boolq>, PIQA <cit. piqa>, COPA <cit. copa>, Winogrande <cit. winogrande>, Arc <cit. arc>, and OpenbookQA (ObQA) <cit. openbookqa>. As reported in Tab. <ref. tab.downstreamtask>, the FP8 pre-trained models exhibit comparable zero-shot performance in comparison to their BF16 counterparts. This result provides further validation that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable to their high-precision counterparts. Furthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instruction following. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 <cit. vicuna>, which adopts the open-sourced LLaMA-7B <cit. llama> as the base model for fine-tuning. Fig. <ref. fig.sft> presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degree of overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 <cit. davinci-003> is also comparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. <ref. tab.sft>. This indicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phase but also to downstream fine-tuning tasks. In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from human feedback (RLHF), a more complex process to align LLMs with user preferences. Following the same training setting as AlpacaFarm <cit. alpacafarm>, a recent RL framework for LLM alignment, we optimize policy models with PPO algorithm <cit. ppo>. The solely difference lies in the choice of mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. <ref. fig.rlhf> and Tab. <ref. tab.rlhf>, we observe a notable reduction in memory utilization, for instance, a 31.8%  memory reduction concerning model weights and a 62.5%  reduction concerning optimizer states. Consequently, it can be inferred that FP8 is capable of replicating the BF16 mixed-precision for RLHF training. This underscores the broader applicability and versatility of our FP8 low-bit training solution. <figure. tab.sft><figure. tab.rlhf>\n\nSystem Performance\nIn this section, we evaluate system-level performance of FP8 mixed-precision, considering communication efficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method employs 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results in a 75%  reduction in communication costs when compared to the mainstream 32-bit scheme (Despite BF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-reduce communication <cit. megatron-lm>). Due to the impact of system transmission loss, the observed practical reduction during GPT model training falls within the range of 63%  to 65% , as indicated in Table <ref. tab.sys>. Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) <cit. te> still relies on full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8 solution. When training GPT models with identical batch sizes, FP8 mixed-precision can lead to a reduction in memory footprint ranging from 27%  to 42%  when compared to BF16, as reported in Tab. <ref. tab.sys>. These reductions in memory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced. Moreover, compared with TE <cit. te>, our solution is also very competitive, obtaining 34.2% , 35.4% , and 44.8%  additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE employs FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more memory than our solution. In addition, the saved memory in our method can be used to train larger batch size or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80G, our approach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion parameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcases the potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train longer sequences with the same GPU resources.Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent BF16 scheme, achieving a notable speed-up of 64%  when applied to GPT-175B model. The model FLOPS utilization (MFU) of FP8 mixed-precision training is 32.0%  on H100 GPUs, being 17.2%  superior to TE. These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces communication costs during the training of large models, and ultimately enhances system utilization efficiency on the latest H100 GPU platform. \n\n\n\n\n===================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: System-level performance on Nvidia H100 GPUs 80G. Here, TP, PP, and DP represent tensor, pipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs utilization. Weight-related communication contains the all-gather operator on weights and the reduce-scatter operator on weight gradients.\n\n<figure. fig:ablate_comm - Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient all-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The experiment is conducted using a GPT-7B model with a data parallelism factor of 128.>\n\nAblation Study\n--------------\n <label. sec.ablation> We ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the performance in Tab. <ref. tab.opt> – <ref. tab:zero> and Fig. <ref. fig:ablate_comm> – <ref. fig.opt>. The ablation experiments are conducted on GPT models, whose architectures and training settings are elaborated in Tab. <ref. tab.arch>. Importantly, our ablation study yields several guidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future research on low-bit model training. <figure. fig.opt>Communication. We first analyze the limitations of the conventional pre-scaling and post-scaling methods when aggregating low-bit gradients during the all-reduce communication process. As shown in Fig. <ref. fig:ablate_comm>, we conduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across different Transformer blocks. It is observed that the pre-scaling method has relative larger underflow rate when quantifying gradients from 32-bit to 8-bit, while the post-scaling method has higher overflow rate. In contrast, the proposed auto-scaling technique can diminish both the underflow ratio and the overflow ratio, while getting much better SNR, as shown in Fig. <ref. fig:ablate_comm> (a). This demonstrates the effectiveness of auto-scaling method in reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce.   0.6  2*Model  2*TP  2*PP  2*DP  Micro  Mixed  Act-related Comm. BS  Precision  Rate (% )  Volume (GB) 2*GPT-13B  2*2  2*1 2*16  2*2  BF16  11.1  4.7  FP8 (Ours)  2.5  3.12*GPT-175B  2*8  2*4  2*4  2*1  BF16  14.8  5.9  FP8 (Ours)  4.5  3.9  Activation-related communication volume reduction in sequence and tensor parallelism, including the all-gather operator on activation and the reduce-scatter on activation gradients. <label. tab:sp> \n   0.65  2*Model  2*TP  2*PP  2*DP  Micro  Mixed  GPU Memory  BS  Precision  Min  Max 3*GPT-7B  3*1  3*1  3*32  3*2  BF16  69.02  69.59  FP8 (TE)  76.93  77.23  FP8 (Ours)  49.31  50.77 3*GPT-13B  3*2  3*1  3*16  3*2  BF16  67.63  68.35  FP8 (TE)  72.85  73.81  FP8 (Ours)  46.78  47.66 3*GPT-175B  3*8  3*4  3*4  3*1  BF16  63.07  63.42  FP8 (TE)  66.39  66.72  FP8 (Ours)  36.39  36.81   Comparing ZeRO distribution methods in terms of memory load across GPUs. Here “Min” and “Max” denote the minimum and maximum memory utilization observed across 32 GPUs. Our FP8 ZeRO method uses less memory while achieving memory-aware load balancing. <label. tab:zero>  \n  Optimizer. We further ablate the impact of reduced precision for the variables in the AdamW optimizer. We set the BF16 mixed-precision optimizer as the baseline, since it has been widely used in existing LLM training frameworks <cit. amp, megatron-lm, te>. Tab. <ref. tab.opt> presents the settings of reduced precision for the variables, while Fig. <ref. fig.opt> plots the corresponding training losses. We observe that: 1) FP8 master weight induces performance degradation (see the # 2 vs. # 3 lines in Fig. <ref. fig.opt>), while FP16 can maintain accuracy as FP32 (see # 2 vs. # 0 and # 1) but requiring using tensor scaling. It reveals that the master weight is precision-sensitive. This can be attributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy. 2) The second-order gradient moment is more precision-sensitive than the first-order one, because the square calculation is easy to cause underflow and leads to accuracy degradation. Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the # 4 dot in Fig. <ref. fig.opt>). Parallelism. In our FP8 LLM training framework, we introduce FP8 low-bit convertors into sequence parallelism and tensor parallelism to reduce activation communication costs across GPUs. Here we conduct an analysis experiment to count the activation-related communication volume during GPT model training, and report the numbers in Tab. <ref. tab:sp>. It is observed that our FP8 parallel scheme results in a substantial reduction of 33%  in activation-related communication costs compared to the original method utilizing BF16. Furthermore, in ZeRO distributed training, our method distributes each FP8 tensor along with its associated scaling factor as a whole, rather than partitioning the tensor into splits across GPUs. This strategy not only results in more GPU memory savings but also maintains a balanced memory load across GPUs, as demonstrated in Tab. <ref. tab:zero>. ",
      "subsections": [
        {
          "title": "Experimental Setup",
          "content": "\n\nTraining Dataset\nOur pre-training data is constructed using open-sourced language collections from several sources, including CommonCrawlhttps://commoncrawl.org, The Pile <cit. pile>, C4 <cit. c4>, OpenWebText <cit. gpt2, Gokaslan2019OpenWebTextCorpus>, CC-NEWS <cit. liu2019roberta>, CC-Stories <cit. trinh2018simple>, Redpajama <cit. redpajama>, and Wikipediahttps://wikipedia.org. We apply fuzzy deduplication <cit. lee_deduplicating_2022> across CommonCrawl snapshots to enhance data quality. Tab. <ref. tab.data> in Appendix <ref. appendix.data> provides details of our pre-training data, including information such as the number of tokens from each source and associated sampling weights. For a more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer to Appendix <ref. appendix.data>. Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1<cit. vicuna>, which uses a publicly user-shared instruction following data <cit. sharegpt>. For reinforcement learning with human feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmless dataset  <cit. HH> and Open-Assistant dataset  <cit. oasst>. The training framework and associated configurations align with the publicly available AlpacaFarm  <cit. alpacafarm>. \n\nModel Configuration\n\n\nparams   dimension   n heads   n layers   TP   PP   SP   learning    batch     n tokens\n                                                           rate       size             \n=======================================================================================\n 125M       768        12         12      1    1         6.0e^-4       1M        100B  \n  7B       4096        32         32      1    1         3.0e^-4       4M        100B  \n 13B       5120        40         40      2    1         3.0e^-4       4M        100B  \n 175B      12288       96         96      8    4         3.0e^-5       1M         5B   \nTable:  Model sizes, architectures, and training hyper-parameters. TP, PP, and SP indicate tensor, pipeline, and sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of the 175B model to a dataset comprising only 5B tokens, which has proven to be sufficient for evaluating system performance.\n\nThe model architecture we used is a decoder-only Transformer <cit. gpt3>, which has been widely-used in recent generative LLMs like PaLM <cit. palm>, OPT <cit. opt>, and LLaMA <cit. llama>. In addition to the base architecture, we integrate several modifications proposed recently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspiration from recent successful experiments <cit. gpt-neox, llama>, we incorporate rotary positional embeddings (RoPE) <cit. rope> into our approach. This addition enables us to capture both absolute and relative positions information, enhancing performance especially when extrapolating to larger context windows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access <cit. ivanov2021data>. Flash Attention <cit. flashatt> proposed an IO-aware exact attention algorithm which uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration. We train the models using the proposed FP8 optimizer, which is built upon Adam <cit. adam> with decoupled weight decay <cit. adamw>, following the common practise with the decay rates β _1 = 0.9, β _2 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final learning rate is 10%  of the maximal learning rate. We train the models for 100B tokens in total with a batch size of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000 iterations. Tab. <ref. tab.arch> presents the details of model configurations and the corresponding training settings. The training is conducted on Azure NDv5 H100 GPU platform <cit. hpc>. <figure. fig:gptloss - A comparison between FP8 and BF16: Analyzing the training loss of GPT models with the parameters ranging from 7 billion to 175 billion.>\n\n \n 0.95  \n\n           HS \n\n     Lambada \n\n     BoolQ \n\n     PIQA \n\n     COPA \n\n     Winogrande \n\n     Arc-C \n\n     Arc-E \n\n     ObQA \n\n     Avg \n\n\n 8lGPT-7B model zero-shot performance \n\n    \n BF16 \n\n     61.3 \n\n     61.4 \n\n     61.2 \n\n     75.0 \n\n     79.0 \n\n     58.5 \n\n     32.9 \n\n     59.7 \n\n     36.4 \n\n     58.4 \n\n\n FP8 \n\n     60.0 \n\n     61.8 \n\n     62.0 \n\n     74.2 \n\n     78.0 \n\n     59.8 \n\n     32.9 \n\n     58.7 \n\n     34.6 \n\n     58.0 \n\n\n 8lGPT-13B model zero-shot performance \n\n    \n BF16 \n\n     64.8 \n\n     64.9 \n\n     63.4 \n\n     75.9 \n\n     82.0 \n\n     61.0 \n\n     35.2 \n\n     61.5 \n\n     40.6 \n\n     61.0 \n\n\n FP8 \n\n     64.1 \n\n     63.4 \n\n     63.9 \n\n     76.2 \n\n     81.0 \n\n     61.6 \n\n     34.9 \n\n     61.3 \n\n     36.8 \n\n     60.4 \n\n\n   Zero-shot performance on downstream tasks. The models are trained with either the standard BF16 mixed-precision scheme <cit. megatron-lm> or the proposed FP8 low-precision scheme.  <label. tab.downstreamtask> \n \n\n",
          "subsections": [
            {
              "title": "Training Dataset",
              "content": "Our pre-training data is constructed using open-sourced language collections from several sources, including CommonCrawlhttps://commoncrawl.org, The Pile <cit. pile>, C4 <cit. c4>, OpenWebText <cit. gpt2, Gokaslan2019OpenWebTextCorpus>, CC-NEWS <cit. liu2019roberta>, CC-Stories <cit. trinh2018simple>, Redpajama <cit. redpajama>, and Wikipediahttps://wikipedia.org. We apply fuzzy deduplication <cit. lee_deduplicating_2022> across CommonCrawl snapshots to enhance data quality. Tab. <ref. tab.data> in Appendix <ref. appendix.data> provides details of our pre-training data, including information such as the number of tokens from each source and associated sampling weights. For a more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer to Appendix <ref. appendix.data>. Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1<cit. vicuna>, which uses a publicly user-shared instruction following data <cit. sharegpt>. For reinforcement learning with human feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmless dataset  <cit. HH> and Open-Assistant dataset  <cit. oasst>. The training framework and associated configurations align with the publicly available AlpacaFarm  <cit. alpacafarm>. ",
              "subsections": [],
              "figures": {}
            },
            {
              "title": "Model Configuration",
              "content": "\n\nparams   dimension   n heads   n layers   TP   PP   SP   learning    batch     n tokens\n                                                           rate       size             \n=======================================================================================\n 125M       768        12         12      1    1         6.0e^-4       1M        100B  \n  7B       4096        32         32      1    1         3.0e^-4       4M        100B  \n 13B       5120        40         40      2    1         3.0e^-4       4M        100B  \n 175B      12288       96         96      8    4         3.0e^-5       1M         5B   \nTable:  Model sizes, architectures, and training hyper-parameters. TP, PP, and SP indicate tensor, pipeline, and sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of the 175B model to a dataset comprising only 5B tokens, which has proven to be sufficient for evaluating system performance.\n\nThe model architecture we used is a decoder-only Transformer <cit. gpt3>, which has been widely-used in recent generative LLMs like PaLM <cit. palm>, OPT <cit. opt>, and LLaMA <cit. llama>. In addition to the base architecture, we integrate several modifications proposed recently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspiration from recent successful experiments <cit. gpt-neox, llama>, we incorporate rotary positional embeddings (RoPE) <cit. rope> into our approach. This addition enables us to capture both absolute and relative positions information, enhancing performance especially when extrapolating to larger context windows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access <cit. ivanov2021data>. Flash Attention <cit. flashatt> proposed an IO-aware exact attention algorithm which uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration. We train the models using the proposed FP8 optimizer, which is built upon Adam <cit. adam> with decoupled weight decay <cit. adamw>, following the common practise with the decay rates β _1 = 0.9, β _2 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final learning rate is 10%  of the maximal learning rate. We train the models for 100B tokens in total with a batch size of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000 iterations. Tab. <ref. tab.arch> presents the details of model configurations and the corresponding training settings. The training is conducted on Azure NDv5 H100 GPU platform <cit. hpc>. <figure. fig:gptloss - A comparison between FP8 and BF16: Analyzing the training loss of GPT models with the parameters ranging from 7 billion to 175 billion.>\n\n \n 0.95  \n\n           HS \n\n     Lambada \n\n     BoolQ \n\n     PIQA \n\n     COPA \n\n     Winogrande \n\n     Arc-C \n\n     Arc-E \n\n     ObQA \n\n     Avg \n\n\n 8lGPT-7B model zero-shot performance \n\n    \n BF16 \n\n     61.3 \n\n     61.4 \n\n     61.2 \n\n     75.0 \n\n     79.0 \n\n     58.5 \n\n     32.9 \n\n     59.7 \n\n     36.4 \n\n     58.4 \n\n\n FP8 \n\n     60.0 \n\n     61.8 \n\n     62.0 \n\n     74.2 \n\n     78.0 \n\n     59.8 \n\n     32.9 \n\n     58.7 \n\n     34.6 \n\n     58.0 \n\n\n 8lGPT-13B model zero-shot performance \n\n    \n BF16 \n\n     64.8 \n\n     64.9 \n\n     63.4 \n\n     75.9 \n\n     82.0 \n\n     61.0 \n\n     35.2 \n\n     61.5 \n\n     40.6 \n\n     61.0 \n\n\n FP8 \n\n     64.1 \n\n     63.4 \n\n     63.9 \n\n     76.2 \n\n     81.0 \n\n     61.6 \n\n     34.9 \n\n     61.3 \n\n     36.8 \n\n     60.4 \n\n\n   Zero-shot performance on downstream tasks. The models are trained with either the standard BF16 mixed-precision scheme <cit. megatron-lm> or the proposed FP8 low-precision scheme.  <label. tab.downstreamtask> \n \n\n",
              "subsections": [],
              "figures": {
                "fig:gptloss": {
                  "label": "fig:gptloss",
                  "path": [
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/GPT-6b7.png",
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/GPT-13b.png",
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/GPT-175b.png"
                  ],
                  "size": [
                    {
                      "scale": 1.0,
                      "height": 132,
                      "width": null
                    },
                    {
                      "scale": 1.0,
                      "height": 132,
                      "width": null
                    },
                    {
                      "scale": 1.0,
                      "height": 132,
                      "width": null
                    }
                  ],
                  "caption": "A comparison between FP8 and BF16: Analyzing the training loss of GPT models with the parameters ranging from 7 billion to 175 billion.",
                  "section": "4.1.2. Model Configuration"
                }
              }
            }
          ],
          "figures": {}
        },
        {
          "title": "Main Results",
          "content": " <label. sec.mainresults> \n\nModel Performance\n We first compare the performance of models trained using FP8 mixed-precision with those trained using BF16. In Fig.   <ref. fig:gptloss>, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. The training configurations and hyper-parameters remain consistent across models trained with FP8 and BF16. The only difference lies in the mixed-precision schemes utilized. As shown in Fig. <ref. fig:gptloss>, the loss curves almost overlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme <cit. megatron-lm, gopher, chinchilla> across a diverse array of model scales. Also, we evaluate the pre-trained models on a wide range of downstream tasks, including HellaSwag (HS) <cit. hellaswag>, Lambada <cit. lambada> BoolQ <cit. boolq>, PIQA <cit. piqa>, COPA <cit. copa>, Winogrande <cit. winogrande>, Arc <cit. arc>, and OpenbookQA (ObQA) <cit. openbookqa>. As reported in Tab. <ref. tab.downstreamtask>, the FP8 pre-trained models exhibit comparable zero-shot performance in comparison to their BF16 counterparts. This result provides further validation that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable to their high-precision counterparts. Furthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instruction following. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 <cit. vicuna>, which adopts the open-sourced LLaMA-7B <cit. llama> as the base model for fine-tuning. Fig. <ref. fig.sft> presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degree of overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 <cit. davinci-003> is also comparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. <ref. tab.sft>. This indicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phase but also to downstream fine-tuning tasks. In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from human feedback (RLHF), a more complex process to align LLMs with user preferences. Following the same training setting as AlpacaFarm <cit. alpacafarm>, a recent RL framework for LLM alignment, we optimize policy models with PPO algorithm <cit. ppo>. The solely difference lies in the choice of mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. <ref. fig.rlhf> and Tab. <ref. tab.rlhf>, we observe a notable reduction in memory utilization, for instance, a 31.8%  memory reduction concerning model weights and a 62.5%  reduction concerning optimizer states. Consequently, it can be inferred that FP8 is capable of replicating the BF16 mixed-precision for RLHF training. This underscores the broader applicability and versatility of our FP8 low-bit training solution. <figure. tab.sft><figure. tab.rlhf>\n\nSystem Performance\nIn this section, we evaluate system-level performance of FP8 mixed-precision, considering communication efficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method employs 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results in a 75%  reduction in communication costs when compared to the mainstream 32-bit scheme (Despite BF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-reduce communication <cit. megatron-lm>). Due to the impact of system transmission loss, the observed practical reduction during GPT model training falls within the range of 63%  to 65% , as indicated in Table <ref. tab.sys>. Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) <cit. te> still relies on full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8 solution. When training GPT models with identical batch sizes, FP8 mixed-precision can lead to a reduction in memory footprint ranging from 27%  to 42%  when compared to BF16, as reported in Tab. <ref. tab.sys>. These reductions in memory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced. Moreover, compared with TE <cit. te>, our solution is also very competitive, obtaining 34.2% , 35.4% , and 44.8%  additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE employs FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more memory than our solution. In addition, the saved memory in our method can be used to train larger batch size or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80G, our approach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion parameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcases the potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train longer sequences with the same GPU resources.Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent BF16 scheme, achieving a notable speed-up of 64%  when applied to GPT-175B model. The model FLOPS utilization (MFU) of FP8 mixed-precision training is 32.0%  on H100 GPUs, being 17.2%  superior to TE. These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces communication costs during the training of large models, and ultimately enhances system utilization efficiency on the latest H100 GPU platform. \n\n\n\n\n===================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: System-level performance on Nvidia H100 GPUs 80G. Here, TP, PP, and DP represent tensor, pipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs utilization. Weight-related communication contains the all-gather operator on weights and the reduce-scatter operator on weight gradients.\n\n<figure. fig:ablate_comm - Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient all-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The experiment is conducted using a GPT-7B model with a data parallelism factor of 128.>",
          "subsections": [
            {
              "title": "Model Performance",
              "content": " We first compare the performance of models trained using FP8 mixed-precision with those trained using BF16. In Fig.   <ref. fig:gptloss>, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. The training configurations and hyper-parameters remain consistent across models trained with FP8 and BF16. The only difference lies in the mixed-precision schemes utilized. As shown in Fig. <ref. fig:gptloss>, the loss curves almost overlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme <cit. megatron-lm, gopher, chinchilla> across a diverse array of model scales. Also, we evaluate the pre-trained models on a wide range of downstream tasks, including HellaSwag (HS) <cit. hellaswag>, Lambada <cit. lambada> BoolQ <cit. boolq>, PIQA <cit. piqa>, COPA <cit. copa>, Winogrande <cit. winogrande>, Arc <cit. arc>, and OpenbookQA (ObQA) <cit. openbookqa>. As reported in Tab. <ref. tab.downstreamtask>, the FP8 pre-trained models exhibit comparable zero-shot performance in comparison to their BF16 counterparts. This result provides further validation that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable to their high-precision counterparts. Furthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instruction following. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 <cit. vicuna>, which adopts the open-sourced LLaMA-7B <cit. llama> as the base model for fine-tuning. Fig. <ref. fig.sft> presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degree of overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 <cit. davinci-003> is also comparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. <ref. tab.sft>. This indicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phase but also to downstream fine-tuning tasks. In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from human feedback (RLHF), a more complex process to align LLMs with user preferences. Following the same training setting as AlpacaFarm <cit. alpacafarm>, a recent RL framework for LLM alignment, we optimize policy models with PPO algorithm <cit. ppo>. The solely difference lies in the choice of mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. <ref. fig.rlhf> and Tab. <ref. tab.rlhf>, we observe a notable reduction in memory utilization, for instance, a 31.8%  memory reduction concerning model weights and a 62.5%  reduction concerning optimizer states. Consequently, it can be inferred that FP8 is capable of replicating the BF16 mixed-precision for RLHF training. This underscores the broader applicability and versatility of our FP8 low-bit training solution. <figure. tab.sft><figure. tab.rlhf>",
              "subsections": [],
              "figures": {
                "tab.sft": {
                  "label": "tab.sft",
                  "path": [
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/sft.png"
                  ],
                  "size": [
                    {
                      "scale": 1.0,
                      "height": 132,
                      "width": null
                    }
                  ],
                  "section": "4.2.1. Model Performance"
                },
                "tab.rlhf": {
                  "label": "tab.rlhf",
                  "path": [
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/rlhf.png"
                  ],
                  "size": [
                    {
                      "scale": 1.0,
                      "height": 132,
                      "width": null
                    }
                  ],
                  "section": "4.2.1. Model Performance"
                }
              }
            },
            {
              "title": "System Performance",
              "content": "In this section, we evaluate system-level performance of FP8 mixed-precision, considering communication efficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method employs 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results in a 75%  reduction in communication costs when compared to the mainstream 32-bit scheme (Despite BF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-reduce communication <cit. megatron-lm>). Due to the impact of system transmission loss, the observed practical reduction during GPT model training falls within the range of 63%  to 65% , as indicated in Table <ref. tab.sys>. Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) <cit. te> still relies on full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8 solution. When training GPT models with identical batch sizes, FP8 mixed-precision can lead to a reduction in memory footprint ranging from 27%  to 42%  when compared to BF16, as reported in Tab. <ref. tab.sys>. These reductions in memory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced. Moreover, compared with TE <cit. te>, our solution is also very competitive, obtaining 34.2% , 35.4% , and 44.8%  additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE employs FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more memory than our solution. In addition, the saved memory in our method can be used to train larger batch size or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80G, our approach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion parameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcases the potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train longer sequences with the same GPU resources.Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent BF16 scheme, achieving a notable speed-up of 64%  when applied to GPT-175B model. The model FLOPS utilization (MFU) of FP8 mixed-precision training is 32.0%  on H100 GPUs, being 17.2%  superior to TE. These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces communication costs during the training of large models, and ultimately enhances system utilization efficiency on the latest H100 GPU platform. \n\n\n\n\n===================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable: System-level performance on Nvidia H100 GPUs 80G. Here, TP, PP, and DP represent tensor, pipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs utilization. Weight-related communication contains the all-gather operator on weights and the reduce-scatter operator on weight gradients.\n\n<figure. fig:ablate_comm - Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient all-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The experiment is conducted using a GPT-7B model with a data parallelism factor of 128.>",
              "subsections": [],
              "figures": {
                "fig:ablate_comm": {
                  "label": "fig:ablate_comm",
                  "path": [
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/SNR.pdf",
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/underflow.pdf",
                    "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/overflow.pdf"
                  ],
                  "size": [
                    {
                      "scale": 1.0,
                      "height": 102,
                      "width": null
                    },
                    {
                      "scale": 1.0,
                      "height": 102,
                      "width": null
                    },
                    {
                      "scale": 1.0,
                      "height": 102,
                      "width": null
                    }
                  ],
                  "caption": "Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient all-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The experiment is conducted using a GPT-7B model with a data parallelism factor of 128.",
                  "section": "4.2.2. System Performance"
                }
              }
            }
          ],
          "figures": {}
        },
        {
          "title": "Ablation Study",
          "content": " <label. sec.ablation> We ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the performance in Tab. <ref. tab.opt> – <ref. tab:zero> and Fig. <ref. fig:ablate_comm> – <ref. fig.opt>. The ablation experiments are conducted on GPT models, whose architectures and training settings are elaborated in Tab. <ref. tab.arch>. Importantly, our ablation study yields several guidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future research on low-bit model training. <figure. fig.opt>Communication. We first analyze the limitations of the conventional pre-scaling and post-scaling methods when aggregating low-bit gradients during the all-reduce communication process. As shown in Fig. <ref. fig:ablate_comm>, we conduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across different Transformer blocks. It is observed that the pre-scaling method has relative larger underflow rate when quantifying gradients from 32-bit to 8-bit, while the post-scaling method has higher overflow rate. In contrast, the proposed auto-scaling technique can diminish both the underflow ratio and the overflow ratio, while getting much better SNR, as shown in Fig. <ref. fig:ablate_comm> (a). This demonstrates the effectiveness of auto-scaling method in reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce.   0.6  2*Model  2*TP  2*PP  2*DP  Micro  Mixed  Act-related Comm. BS  Precision  Rate (% )  Volume (GB) 2*GPT-13B  2*2  2*1 2*16  2*2  BF16  11.1  4.7  FP8 (Ours)  2.5  3.12*GPT-175B  2*8  2*4  2*4  2*1  BF16  14.8  5.9  FP8 (Ours)  4.5  3.9  Activation-related communication volume reduction in sequence and tensor parallelism, including the all-gather operator on activation and the reduce-scatter on activation gradients. <label. tab:sp> \n   0.65  2*Model  2*TP  2*PP  2*DP  Micro  Mixed  GPU Memory  BS  Precision  Min  Max 3*GPT-7B  3*1  3*1  3*32  3*2  BF16  69.02  69.59  FP8 (TE)  76.93  77.23  FP8 (Ours)  49.31  50.77 3*GPT-13B  3*2  3*1  3*16  3*2  BF16  67.63  68.35  FP8 (TE)  72.85  73.81  FP8 (Ours)  46.78  47.66 3*GPT-175B  3*8  3*4  3*4  3*1  BF16  63.07  63.42  FP8 (TE)  66.39  66.72  FP8 (Ours)  36.39  36.81   Comparing ZeRO distribution methods in terms of memory load across GPUs. Here “Min” and “Max” denote the minimum and maximum memory utilization observed across 32 GPUs. Our FP8 ZeRO method uses less memory while achieving memory-aware load balancing. <label. tab:zero>  \n  Optimizer. We further ablate the impact of reduced precision for the variables in the AdamW optimizer. We set the BF16 mixed-precision optimizer as the baseline, since it has been widely used in existing LLM training frameworks <cit. amp, megatron-lm, te>. Tab. <ref. tab.opt> presents the settings of reduced precision for the variables, while Fig. <ref. fig.opt> plots the corresponding training losses. We observe that: 1) FP8 master weight induces performance degradation (see the # 2 vs. # 3 lines in Fig. <ref. fig.opt>), while FP16 can maintain accuracy as FP32 (see # 2 vs. # 0 and # 1) but requiring using tensor scaling. It reveals that the master weight is precision-sensitive. This can be attributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy. 2) The second-order gradient moment is more precision-sensitive than the first-order one, because the square calculation is easy to cause underflow and leads to accuracy degradation. Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the # 4 dot in Fig. <ref. fig.opt>). Parallelism. In our FP8 LLM training framework, we introduce FP8 low-bit convertors into sequence parallelism and tensor parallelism to reduce activation communication costs across GPUs. Here we conduct an analysis experiment to count the activation-related communication volume during GPT model training, and report the numbers in Tab. <ref. tab:sp>. It is observed that our FP8 parallel scheme results in a substantial reduction of 33%  in activation-related communication costs compared to the original method utilizing BF16. Furthermore, in ZeRO distributed training, our method distributes each FP8 tensor along with its associated scaling factor as a whole, rather than partitioning the tensor into splits across GPUs. This strategy not only results in more GPU memory savings but also maintains a balanced memory load across GPUs, as demonstrated in Tab. <ref. tab:zero>. ",
          "subsections": [],
          "figures": {
            "fig.opt": {
              "label": "fig.opt",
              "path": [
                "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/ablate_opt.pdf"
              ],
              "size": [
                {
                  "scale": 1.0,
                  "height": null,
                  "width": 0
                }
              ],
              "section": "4.3. Ablation Study"
            }
          }
        }
      ],
      "figures": {}
    },
    {
      "title": "Related Work",
      "content": " Mixed-precision Training. Efficient training through reduced mixed-precision has been widely used in modern deep learning to save computing costs. While some works have taken bit-reduction to the extreme, i.e. 1-bit binary networks <cit. hubara2016binarized, Xnor-net>, they have not been successful in maintaining model accuracy <cit. fp8-dl>. The most practical scheme now is the FP16 half-precision method <cit. amp>, which can maintain accuracy while improving training efficiency. The computations during forward pass and back propagation use FP16 while the master weights use FP32. Since FP16 has a narrower dynamic range, FP16 mixed-precision entails loss scaling <cit. amp> to prevent loss of accuracy. Fortunately, the need for loss scaling can be avoided by using BF16 datatype, because BF16 maintains the same dynamic range as the full-precision FP32. This results in that large model training now prefers to use BF16 mixed-precision scheme, which is more stable during training <cit. megatron-nlg, bloom, glm-130b>. FP8 is a natural progression from 16-bit data formats to further reducing computing cost. Early pioneering efforts in FP8 low-bit model training <cit. fp8-wangnaigang, sunxiao-fp8, 8bit-opt> have largely remained at the simulation stage. Consequently, there exists a notable gap between the projected capabilities of these approaches and their actual performance on hardware <cit. fp8-dl>. With the advent of Nvidia Hopper GPU architecture <cit. h100-whitepaper>, FP8 is emerging as a viable and practical data type for the next-generation low-precision training, as discussed in <cit. fp8-dl>. At present, the Nvidia Transformer Engine (TE) <cit. te> serves as the primary framework for FP8 mixed-precision training. However, its support for FP8 usage remains somewhat constrained. TE’s current implementation restricts FP8 usage solely to weight computation, retaining the storage of model weights and gradient calculations with 16-bit data types. Consequently, the end-to-end speed-up, memory and communication cost savings are limited. In contrast, our work infiltrates FP8 gradient, optimizer, and distributed training into the whole progress of model training, fully unveiling the capabilities of FP8. Large Language Models. Recent years have witnessed a substantial evolution in the field of LLMs. Autoregressive language modeling – predicting the future of a text sequence from its past – provides a simple yet powerful objective that admits formulation of numerous tasks. While there exist alternative methodologies, such as masked language modeling <cit. bert> and permutation language modeling <cit. xlnet>, the autoregressive method now is more promising because of its strong performance. Following the scaling laws <cit. gpt3> and the refined laws <cit. chinchilla>, various LLMs are have been proposed, including dense models: GPT-3 <cit. gpt3>, Jurassic-1 <cit. jurassic>, Gopher  <cit. gopher>, Chinchilla <cit. chinchilla>, Bloom  <cit. bloom>, OPT <cit. opt> Megatron-Turing NLG <cit. megatron-nlg>, PaLM <cit. palm>, LaMDA <cit. lamda>, LLaMA <cit. llama>, and sparse models: GLaM <cit. glam>, and Switch transformers <cit. switch-trans>. Each of them has demonstrated remarkably competitive few-shot performance across a wide range of tasks at the time of their respective releases. Nonetheless, these models still encounter challenges, such as overwhelming computational requirements and the need for acquiring more high-quality training data. In this work, we delve into the utilization of low-precision techniques to mitigate the training costs, which is a crucial step for the continued expansion of language models. Low-precision training has been widely used in LLM training to reduce compute cost. OPT <cit. opt> and GLM <cit. glm-130b> utilize FP16 for forwards and backwards and FP32 for optimizer states and master weights, to reduce the GPU memory usage and improve training efficiency. Bloom <cit. bloom> find that FP16 can cause numerical instabilities and irreversible divergences, especially when training models larger than 100B parameters, because FP16’s dynamic range is limited. Consequently, Bloom and other LLMs, such as Gopher <cit. gopher> and Chinchilla <cit. chinchilla>, adopt BF16 mixed-precision, because BF16 has a wide dynamic range that is the same as FP32. LLM training and tuning with 8-bit low-precision were not well-explored in previous works, because the hardware support for FP8 is not available before the release of Nvidia Hopper infrastructure. This work presents the first exploration of FP8 pre-training and fine-tuning for LLMs, while proposing an extremely-optimized FP8 mixed-precision scheme. We hope this work could facilitate future research in FP8 and, potentially, extend to exploring even lower precision training, such as 4-bit and 1-bit. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Conclusion",
      "content": "In this work, we explore 8-bit training for LLMs. We introduce a new FP8 mixed-precision training framework, which incorporates 8-bit collective communication, optimizer, and distributed parallel training in an incremental manner. To our best knowledge, this is the first work infiltrating FP8 compute, storage and communication into the whole progress of large language model training. Extensive experiments demonstrate the proposed method effectively diminishes communication overhead and curtails memory utilization in the context of GPT model training at various scales. In future work, we plan to scale up the size and training steps of the FP8 GPT models and further train them with our 8-bit mixed-precision scheme. Moreover, we will also use the proposed FP8 scheme to train multi-modal large models, and explore low-bit deployment of LLMs on various edge devices, such as smart phones. \n ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Contribution and Acknowledgement",
      "content": " <label. contribution> This project was initially proposed by Han Hu and Peng Cheng, who are the directional lead. Shuguang Liu served as the product lead throughout the project. The contributions for all the co-authors are detailed as follows: FP8 Framework: Kan Wu, Houwen Peng, Ze Liu, Peng Cheng, Han Hu System: Yifan Xiong, Ziyue Yang, Yuxiang Yang, Guoshuai Zhao, Peng Cheng Hardware Infrastructure: Guoshuai Zhao, Yuxiang Yang, Yifan Xiong, Peng Cheng, Shuguang Liu, Joe Chau Data: Ruihang Li, Miaosen Zhang, Jia Ning, Chen Li, Ruizhe Wang, Houwen Peng, Han Hu Pre-training: Yixuan Wei, Kan Wu, Ze Liu, Miaosen Zhang, Zheng Zhang, Houwen Peng, Han Hu Alignment (SFT, RS, and RLHF): Bolin Ni, Jingcheng Hu, Yixuan Wei, Houwen Peng, Han Hu Evaluation: Yixuan Wei, Bolin Ni, Jingcheng Hu Product Engineering: Yuxiang Yang, Kan Wu, Yifan Xiong, Ziyue Yang, Guoshuai Zhao, Peng Cheng \n We thank Eric Chung, Bita Darvish Rouhani, Yu Pei, Hyunseung Harry Yoo, Zhenghong Zhou, Gongrui Zhang, and Zhirong Wu for helpful discussions. We thank Baining Guo and Lidong Zhou for their guidance and support for this project. plainnat ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Appendix",
      "content": "\n\nFP8 Data Formats\n----------------\n <label. appendix.A> In September 2022, NVIDIA, ARM, and Intel published FP8 specification for standardization as an interchange format for AI <cit. fp8-dl>. The industry has moved from 32-bit precision to 16-bit, and now even 8-bit precision for AI model training. This development reflects a broader industry trend that has transitioned from high-precision to low-precision training. Notably, the proposed FP8 specification introduces two distinct data types, E5M2 and E4M3, which offer a trade-off between a larger range and higher precision of stored values <cit. te-fp8>. \n - E4M3 consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448 and NaN. \n\n- E5M2 consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa. It can store values up to +/-57344, +/- inf and NaN. \n\nThe FP8 format <cit. fp8-dl> roughly follows the IEEE 754 standard. Compared to higher precision data formats such as FP16 and FP32, FP8 suffers from two kinds of representation degradation: \n - Lower representation range. The representation range in a data format specifies the range between the maximum and minimum values that the format can accurately represent. There are two modes, a normal mode, which defines a regular range with relatively constant precision, and a subnormal mode, which extends the range to represent smaller values with lower precision. The normal range primarily depends on the number of exponent (E) bits, with more E bits resulting in a larger normal range. On the other hand, the subnormal range is primarily influenced by the number of mantissa (M) bits, where an increase in M bits leads to a larger subnormal range. As illustrated in Tab. <ref. tab.fp8range>, the representation range of FP8 is notably narrower compared to that of FP16 and FP32, especially in the case of the S1E4M3 sub-format (S denotes the sign bit). This discrepancy represents the primary challenge when employing FP8 for training large models. \n\n- Lower representation precision. The limited number of mantissa (M bits) leads to quantization representation errors. Due to the considerably fewer M bits in FP8, the representation precision of FP8 is substantially lower than that of FP16, as depicted in Tab. <ref. tab.fp8range>. This challenge stands as another significant hurdle when considering the use of FP8 for training large models. \n\nFP8 consists of two sub-formats: S1E4M3 and S1E5M2. The former offers a narrower representation range but higher precision, while the latter provides a larger range but lower precision. These two sub-formats give users the flexibility to strike a balance between their requirements for range and precision in model training. \n\n Representation range and error for different data formats 0.950.0pt! Data format \n\n     3c|Representation Range \n\n     2cMaximum Relative Error \n\n\n 2-6 \n\n     Max normal \n\n     Min normal \n\n     Min subnormal \n\n     Min - Max (normal) \n\n     Min ∼ Max (subnormal) \n\n\n [c]FP32 \n  (S1E8M23)\n\n \n\n     3.40 × 10^38 \n\n     1.18 × 10^-38 \n\n     1.40 × 10^-45 \n\n     1.19 × 10^-7∼ 5.96 × 10^-8 \n\n     5.00 × 10^-1∼ 1.19 × 10^-7 \n\n\n [c]FP16 \n  (S1E5M10)\n\n \n\n     65,504 \n\n     6.10 × 10^-5 \n\n     5.96 × 10^-8 \n\n     9.76 × 10^-4∼ 4.89 × 10^-4 \n\n     5.00 × 10^-1∼ 9.78 × 10^-4 \n\n\n [c]BF16 \n  (S1E8M7)\n\n \n\n     3.39 × 10^38 \n\n     1.18 × 10^-38 \n\n     9.18 × 10^-41 \n\n     7.75 × 10^-3∼ 3.94 × 10^-3 \n\n     5.00 × 10^-1∼ 7.94 × 10^-3 \n\n\n [c]FP8 \n  (S1E4M3)\n\n \n\n     448 \n\n     1.56 × 10^-2 \n\n     1.95 × 10^-3 \n\n     1.11 × 10^-1∼ 7.69 × 10^-2 \n\n     5.00 × 10^-1∼ 1.67 × 10^-1 \n\n\n [c]FP8 \n  (S1E5M2)\n\n \n\n     57,344 \n\n     6.10 × 10^-5 \n\n     1.53 × 10^-5 \n\n     2.00 × 10^-1∼ 1.67 × 10^-1 \n\n     5.00 × 10^-1∼ 5.00 × 10^-1 \n\n\n   <label. tab.fp8range> \n\n\n\nFP8 Tensor Scaling\n------------------\n <label. appendix.B> We now discuss the underlying mechanisms for how large model training with FP8 overcomes the challenges associated with representation range and precision degradation. The key technique behind is tensor scaling, which scales the tensor values that originally locate out the representation range of a data format to its comfort zone, as visualized in Fig. <ref. fig:tensorscaling>. The pioneer scaling techniques <cit. amp,apex> apply a global scaling factor to the loss, such that gradients of all layers are scaled by a single adaptive factor. The utilization of the global loss scaling technique, in conjunction with various other training strategies, has facilitated the widespread adoption of FP16 mixed-precision training on V100 and A100 GPUs. Remarkably, this approach has resulted in minimal to no degradation in accuracy, particularly for small to medium-sized models <cit. amp>. Nonetheless, when dealing with super-large models or complex tasks, such as in the training of models like DALL-E <cit. dalle>, the global loss scaling technique still encounters significant underflow issues. As a consequence, block-wise <cit. dalle> and layer-wise <cit. 4bit> gradient scaling are proposed. While the global scaling technique enables almost no accuracy drop for FP16 training (with a range of [5.96E-8, 6.55E+4]), the fine-grained per-tensor scaling will enable stable model training using even shallower range by FP8 (with a range of [1.95E-3, 448] for E4M3 and a range of [1.53E-5, 5.73E+4] for E5M2). Fig. <ref. fig:tensorscaling> shows that the representation range of FP8 has been large enough to deal with general model training. In the per-tensor scaling technique, various strategies are available for choosing the suitable scaling factor for a given FP8 tensor. Two common approaches are “just-in-time scaling\" and “delayed scaling\" <cit. te-fp8>. \n - Just-in-time scaling. This strategy involves determining the scaling factor based on the maximum absolute value (amax) of the tensor being generated. However, in practical applications, this approach is often infeasible because it necessitates multiple passes through the data. Specifically, the operator first produces and writes out the output in higher precision, then calculates the maximum absolute value of the output, and finally applies this scaling factor to all values to obtain the final FP8 output. This process introduces a significant amount of overhead, which can substantially reduce the benefits of using FP8. \n\n- Delayed scaling. This strategy involves selecting the scaling factor based on the maximum absolute values observed in a certain number of preceding iterations. This approach allows for the full performance benefits of FP8 computation but necessitates the storage of a history of maximum values as additional parameters of the FP8 operators. \n\n<figure. fig:tensorscaling - Scaling gradients to fall within the representation range of the FP8 datatype.>\n\nPre-training Data\n-----------------\n <label. appendix.data> Tab. <ref. tab.data> presents an overview of our collected data sources along with the corresponding sampling weights employed in pre-training. The arXiv and StackExchange subsets are collected from Redpajama <cit. redpajama>, while BookCorpus2 <cit. bookcorpus>, Books3 <cit. Presser2020Books3>, DM-Math <cit. dm-math>, Gutenberg <cit. pg19>, HackerNewshttps://news.ycombinator.com, NIH ExPorterhttps://exporter.nih.gov, OpenSubtitles <cit. opensubtitles>, and USPTOhttps://bulkdata.uspto.gov subsets are extracted from The Pile <cit. pile>. The Wikipedia data is downloaded from HuggingFace <cit. wikihf>. We use the 20220301 dump, including 24 languages: bg, ca, cs, da, de, en, es, fr, hi, hr, hu, it, jp, ko, nl, pl, pt, ro, ru, sl, sr, sv, uk, zh. We pre-process 11 CommonCrawl snapshots, ranging from 2018 to 2023, with the CCNet pipeline <cit. wenzek2019ccnet>. This process involves data deduplication at the line level, followed by language identification utilizing a fastText linear classifier <cit. fasttext> to eliminate non-English pages. A filtering mechanism based on an n-gram language model is employed to exclude low-quality content. In addition, we train a linear classifier <cit. redpajama> to distinguish documents similar to Wikipedia pages from randomly sampled CommonCrawl documents. Documents not classified as resembling Wikipedia are excluded. Finally, we perform fuzzy deduplication <cit. lee_deduplicating_2022> across all the processed snapshots from CommonCrawl. We collect Python code data from Github using a repository list provided by Bing indexing <cit. bing>. The cleaning of the code data includes three steps. First, we remove control characters, except for \\ t and \\ n. Next, we remove copyright comments in the code. An alphanumeric rate filter is then applied, removing lines with a rate below 0.5 if they are comments, and discarding the entire file if its overall alphanumeric rate is less than 0.98. Files with less than 5 lines or a maximum line length exceeding 1,000 characters are also discarded. Also, files with an average line length of more than 100 characters are discarded. Lastly, a pattern search is conducted to identify key Python keywords (e.g., import, from, def, class, if, for, try, etc.) within the code. Files containing less than 3 instances of these keywords are eliminated. This comprehensive process ensures that the remaining Python code data is of high quality and suitable for use in academic research. We additionally add Python code from Stack <cit. kocetkov2022stack>, and perform fuzzy deduplication within all the collected Python code. \n\n          Dataset                     Sampling prop.                     Epochs               Training Tokens (Billion) \n========================================================================================================================\n         Web Crawls                     Web Crawls                     Web Crawls                    Web Crawls         \n        CommonCrawl                       51.71\n             C4                           25.56\n        OpenWebText                       2.73\nTechnical      Science content   Technical      Science content   Technical      Science content      Technical      Science    \n                                                                                                       content          \n           ArXiv                          1.54\n       StackExchange                      1.42\n          DM-Math                         0.39\n           USPTO                          0.52\n        NIH ExPorter                      0.04\n   Programming Languages          Programming Languages          Programming Languages          Programming Languages   \n           Python                         4.50\n   Other Curated Sources          Other Curated Sources          Other Curated Sources          Other Curated Sources   \n         Wikipedia                        4.50\n           Books                          4.50\n            News                          2.00\n          Dialogue                        2.00\n           Total                          Total                          Total                           100            \nTable: Pre-training data. For each subset we list the sampling weight, number of epochs, and training tokens. Books data includes BookCorpus2 <cit. bookcorpus\n\n>",
      "subsections": [
        {
          "title": "FP8 Data Formats",
          "content": " <label. appendix.A> In September 2022, NVIDIA, ARM, and Intel published FP8 specification for standardization as an interchange format for AI <cit. fp8-dl>. The industry has moved from 32-bit precision to 16-bit, and now even 8-bit precision for AI model training. This development reflects a broader industry trend that has transitioned from high-precision to low-precision training. Notably, the proposed FP8 specification introduces two distinct data types, E5M2 and E4M3, which offer a trade-off between a larger range and higher precision of stored values <cit. te-fp8>. \n - E4M3 consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448 and NaN. \n\n- E5M2 consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa. It can store values up to +/-57344, +/- inf and NaN. \n\nThe FP8 format <cit. fp8-dl> roughly follows the IEEE 754 standard. Compared to higher precision data formats such as FP16 and FP32, FP8 suffers from two kinds of representation degradation: \n - Lower representation range. The representation range in a data format specifies the range between the maximum and minimum values that the format can accurately represent. There are two modes, a normal mode, which defines a regular range with relatively constant precision, and a subnormal mode, which extends the range to represent smaller values with lower precision. The normal range primarily depends on the number of exponent (E) bits, with more E bits resulting in a larger normal range. On the other hand, the subnormal range is primarily influenced by the number of mantissa (M) bits, where an increase in M bits leads to a larger subnormal range. As illustrated in Tab. <ref. tab.fp8range>, the representation range of FP8 is notably narrower compared to that of FP16 and FP32, especially in the case of the S1E4M3 sub-format (S denotes the sign bit). This discrepancy represents the primary challenge when employing FP8 for training large models. \n\n- Lower representation precision. The limited number of mantissa (M bits) leads to quantization representation errors. Due to the considerably fewer M bits in FP8, the representation precision of FP8 is substantially lower than that of FP16, as depicted in Tab. <ref. tab.fp8range>. This challenge stands as another significant hurdle when considering the use of FP8 for training large models. \n\nFP8 consists of two sub-formats: S1E4M3 and S1E5M2. The former offers a narrower representation range but higher precision, while the latter provides a larger range but lower precision. These two sub-formats give users the flexibility to strike a balance between their requirements for range and precision in model training. \n\n Representation range and error for different data formats 0.950.0pt! Data format \n\n     3c|Representation Range \n\n     2cMaximum Relative Error \n\n\n 2-6 \n\n     Max normal \n\n     Min normal \n\n     Min subnormal \n\n     Min - Max (normal) \n\n     Min ∼ Max (subnormal) \n\n\n [c]FP32 \n  (S1E8M23)\n\n \n\n     3.40 × 10^38 \n\n     1.18 × 10^-38 \n\n     1.40 × 10^-45 \n\n     1.19 × 10^-7∼ 5.96 × 10^-8 \n\n     5.00 × 10^-1∼ 1.19 × 10^-7 \n\n\n [c]FP16 \n  (S1E5M10)\n\n \n\n     65,504 \n\n     6.10 × 10^-5 \n\n     5.96 × 10^-8 \n\n     9.76 × 10^-4∼ 4.89 × 10^-4 \n\n     5.00 × 10^-1∼ 9.78 × 10^-4 \n\n\n [c]BF16 \n  (S1E8M7)\n\n \n\n     3.39 × 10^38 \n\n     1.18 × 10^-38 \n\n     9.18 × 10^-41 \n\n     7.75 × 10^-3∼ 3.94 × 10^-3 \n\n     5.00 × 10^-1∼ 7.94 × 10^-3 \n\n\n [c]FP8 \n  (S1E4M3)\n\n \n\n     448 \n\n     1.56 × 10^-2 \n\n     1.95 × 10^-3 \n\n     1.11 × 10^-1∼ 7.69 × 10^-2 \n\n     5.00 × 10^-1∼ 1.67 × 10^-1 \n\n\n [c]FP8 \n  (S1E5M2)\n\n \n\n     57,344 \n\n     6.10 × 10^-5 \n\n     1.53 × 10^-5 \n\n     2.00 × 10^-1∼ 1.67 × 10^-1 \n\n     5.00 × 10^-1∼ 5.00 × 10^-1 \n\n\n   <label. tab.fp8range> \n\n",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "FP8 Tensor Scaling",
          "content": " <label. appendix.B> We now discuss the underlying mechanisms for how large model training with FP8 overcomes the challenges associated with representation range and precision degradation. The key technique behind is tensor scaling, which scales the tensor values that originally locate out the representation range of a data format to its comfort zone, as visualized in Fig. <ref. fig:tensorscaling>. The pioneer scaling techniques <cit. amp,apex> apply a global scaling factor to the loss, such that gradients of all layers are scaled by a single adaptive factor. The utilization of the global loss scaling technique, in conjunction with various other training strategies, has facilitated the widespread adoption of FP16 mixed-precision training on V100 and A100 GPUs. Remarkably, this approach has resulted in minimal to no degradation in accuracy, particularly for small to medium-sized models <cit. amp>. Nonetheless, when dealing with super-large models or complex tasks, such as in the training of models like DALL-E <cit. dalle>, the global loss scaling technique still encounters significant underflow issues. As a consequence, block-wise <cit. dalle> and layer-wise <cit. 4bit> gradient scaling are proposed. While the global scaling technique enables almost no accuracy drop for FP16 training (with a range of [5.96E-8, 6.55E+4]), the fine-grained per-tensor scaling will enable stable model training using even shallower range by FP8 (with a range of [1.95E-3, 448] for E4M3 and a range of [1.53E-5, 5.73E+4] for E5M2). Fig. <ref. fig:tensorscaling> shows that the representation range of FP8 has been large enough to deal with general model training. In the per-tensor scaling technique, various strategies are available for choosing the suitable scaling factor for a given FP8 tensor. Two common approaches are “just-in-time scaling\" and “delayed scaling\" <cit. te-fp8>. \n - Just-in-time scaling. This strategy involves determining the scaling factor based on the maximum absolute value (amax) of the tensor being generated. However, in practical applications, this approach is often infeasible because it necessitates multiple passes through the data. Specifically, the operator first produces and writes out the output in higher precision, then calculates the maximum absolute value of the output, and finally applies this scaling factor to all values to obtain the final FP8 output. This process introduces a significant amount of overhead, which can substantially reduce the benefits of using FP8. \n\n- Delayed scaling. This strategy involves selecting the scaling factor based on the maximum absolute values observed in a certain number of preceding iterations. This approach allows for the full performance benefits of FP8 computation but necessitates the storage of a history of maximum values as additional parameters of the FP8 operators. \n\n<figure. fig:tensorscaling - Scaling gradients to fall within the representation range of the FP8 datatype.>",
          "subsections": [],
          "figures": {
            "fig:tensorscaling": {
              "label": "fig:tensorscaling",
              "path": [
                "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/FP8/source/figures/tensorscaling.png"
              ],
              "size": [
                {
                  "scale": 1.0,
                  "height": null,
                  "width": null
                }
              ],
              "caption": "Scaling gradients to fall within the representation range of the FP8 datatype.",
              "section": "8.2. FP8 Tensor Scaling"
            }
          }
        },
        {
          "title": "Pre-training Data",
          "content": " <label. appendix.data> Tab. <ref. tab.data> presents an overview of our collected data sources along with the corresponding sampling weights employed in pre-training. The arXiv and StackExchange subsets are collected from Redpajama <cit. redpajama>, while BookCorpus2 <cit. bookcorpus>, Books3 <cit. Presser2020Books3>, DM-Math <cit. dm-math>, Gutenberg <cit. pg19>, HackerNewshttps://news.ycombinator.com, NIH ExPorterhttps://exporter.nih.gov, OpenSubtitles <cit. opensubtitles>, and USPTOhttps://bulkdata.uspto.gov subsets are extracted from The Pile <cit. pile>. The Wikipedia data is downloaded from HuggingFace <cit. wikihf>. We use the 20220301 dump, including 24 languages: bg, ca, cs, da, de, en, es, fr, hi, hr, hu, it, jp, ko, nl, pl, pt, ro, ru, sl, sr, sv, uk, zh. We pre-process 11 CommonCrawl snapshots, ranging from 2018 to 2023, with the CCNet pipeline <cit. wenzek2019ccnet>. This process involves data deduplication at the line level, followed by language identification utilizing a fastText linear classifier <cit. fasttext> to eliminate non-English pages. A filtering mechanism based on an n-gram language model is employed to exclude low-quality content. In addition, we train a linear classifier <cit. redpajama> to distinguish documents similar to Wikipedia pages from randomly sampled CommonCrawl documents. Documents not classified as resembling Wikipedia are excluded. Finally, we perform fuzzy deduplication <cit. lee_deduplicating_2022> across all the processed snapshots from CommonCrawl. We collect Python code data from Github using a repository list provided by Bing indexing <cit. bing>. The cleaning of the code data includes three steps. First, we remove control characters, except for \\ t and \\ n. Next, we remove copyright comments in the code. An alphanumeric rate filter is then applied, removing lines with a rate below 0.5 if they are comments, and discarding the entire file if its overall alphanumeric rate is less than 0.98. Files with less than 5 lines or a maximum line length exceeding 1,000 characters are also discarded. Also, files with an average line length of more than 100 characters are discarded. Lastly, a pattern search is conducted to identify key Python keywords (e.g., import, from, def, class, if, for, try, etc.) within the code. Files containing less than 3 instances of these keywords are eliminated. This comprehensive process ensures that the remaining Python code data is of high quality and suitable for use in academic research. We additionally add Python code from Stack <cit. kocetkov2022stack>, and perform fuzzy deduplication within all the collected Python code. \n\n          Dataset                     Sampling prop.                     Epochs               Training Tokens (Billion) \n========================================================================================================================\n         Web Crawls                     Web Crawls                     Web Crawls                    Web Crawls         \n        CommonCrawl                       51.71\n             C4                           25.56\n        OpenWebText                       2.73\nTechnical      Science content   Technical      Science content   Technical      Science content      Technical      Science    \n                                                                                                       content          \n           ArXiv                          1.54\n       StackExchange                      1.42\n          DM-Math                         0.39\n           USPTO                          0.52\n        NIH ExPorter                      0.04\n   Programming Languages          Programming Languages          Programming Languages          Programming Languages   \n           Python                         4.50\n   Other Curated Sources          Other Curated Sources          Other Curated Sources          Other Curated Sources   \n         Wikipedia                        4.50\n           Books                          4.50\n            News                          2.00\n          Dialogue                        2.00\n           Total                          Total                          Total                           100            \nTable: Pre-training data. For each subset we list the sampling weight, number of epochs, and training tokens. Books data includes BookCorpus2 <cit. bookcorpus\n\n>",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    }
  ],
  "figures": {}
}