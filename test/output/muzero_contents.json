{
  "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
  "content": " \n\n                  \n\narrows.meta \n\n  \n\n \n\n       \n\n   \n\n   \n\n \n\n-28.452755905511808pt5.690551181102362pt455.2440944881889pt625.9606299212597pt28.452755905511808pt \n\nMastering Atari, Go, Chess and Shogi by Planning with a Learned Model \n\n Julian Schrittwieser,^1∗ Ioannis Antonoglou,^1,2∗ Thomas Hubert,^1∗ \n Karen Simonyan,^1 Laurent Sifre,^1 Simon Schmitt,^1 Arthur Guez,^1 \n Edward Lockhart,^1 Demis Hassabis,^1 Thore Graepel,^1,2 Timothy Lillicrap,^1 \n David Silver^1,2∗ \n \n ^1DeepMind, 6 Pancras Square, London N1C 4AG. \n ^2University College London, Gower Street, London WC1E 6BT. \n ^∗These authors contributed equally to this work.  \n\n \n\n[NO \\title GIVEN]\n    [NO \\author GIVEN]\n    November 17, 2023\n======================\n\n Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules. \n\nIntroduction\n============\nPlanning algorithms based on lookahead search have achieved remarkable successes in artificial intelligence. Human world champions have been defeated in classic games such as checkers <cit. schaeffer:chinook>, chess <cit. campbell:deep-blue>, Go <cit. Silver16AG> and poker <cit. brown2018superhuman, deepstack>, and planning algorithms have had real-world impact in applications from logistics <cit. vlahavas2013planning> to chemical synthesis <cit. alphachem>. However, these planning algorithms all rely on knowledge of the environment’s dynamics, such as the rules of the game or an accurate simulator, preventing their direct application to real-world domains like robotics, industrial control, or intelligent assistants. Model-based reinforcement learning (RL) <cit. sutton:book> aims to address this issue by first learning a model of the environment’s dynamics, and then planning with respect to the learned model. Typically, these models have either focused on reconstructing the true environmental state <cit. pilco:deisenroth,heess:stochastic_value_gradients,levine:learning_guided_policy>, or the sequence of full observations <cit. hafner:planet, kaiser:simple>. However, prior work <cit. state_space_models, hafner:planet, kaiser:simple> remains far from the state of the art in visually rich domains, such as Atari 2600 games <cit. ALE>. Instead, the most successful methods are based on model-free RL <cit. impala, r2d2, apex> – i.e. they estimate the optimal policy and/or value function directly from interactions with the environment. However, model-free algorithms are in turn far from the state of the art in domains that require precise and sophisticated lookahead, such as chess and Go. In this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art performance in Atari 2600, a visually complex set of domains, while maintaining superhuman performance in precision planning tasks such as chess, shogi and Go. MuZero builds upon AlphaZero’s <cit. Silver18AZ> powerful search and search-based policy iteration algorithms, but incorporates a learned model into the training procedure. MuZero also extends AlphaZero to a broader set of environments including single agent domains and non-zero rewards at intermediate time-steps. The main idea of the algorithm (summarized in Figure 1) is to predict those aspects of the future that are directly relevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an input and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the policy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points scored by playing a move). The model is trained end-to-end, with the sole objective of accurately estimating these three important quantities, so as to match the improved estimates of policy and value generated by search as well as the observed reward. There is no direct constraint or requirement for the hidden state to capture all information necessary to reconstruct the original observation, drastically reducing the amount of information the model has to maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the environment; nor any other constraints on the semantics of state. Instead, the hidden states are free to represent state in whatever way is relevant to predicting current and future values and policies. Intuitively, the agent can invent, internally, the rules or dynamics that lead to most accurate planning. \n\nPrior Work\n==========\nReinforcement learning may be subdivided into two principal categories: model-based, and model-free <cit. sutton:book>. Model-based RL constructs, as an intermediate step, a model of the environment. Classically, this model is represented by a Markov-decision process (MDP) <cit. puterman:MDP> consisting of two components: a state transition model, predicting the next state, and a reward model, predicting the expected reward during that transition. The model is typically conditioned on the selected action, or a temporally abstract behavior such as an option <cit. sutton:between>. Once a model has been constructed, it is straightforward to apply MDP planning algorithms, such as value iteration <cit. puterman:MDP> or Monte-Carlo tree search (MCTS) <cit. coulom:mcts>, to compute the optimal value or optimal policy for the MDP. In large or partially observed environments, the algorithm must first construct the state representation that the model should predict. This tripartite separation between representation learning, model learning, and planning is potentially problematic since the agent is not able to optimize its representation or model for the purpose of effective planning, so that, for example modeling errors may compound during planning. A common approach to model-based RL focuses on directly modeling the observation stream at the pixel-level. It has been hypothesized that deep, stochastic models may mitigate the problems of compounding error <cit. hafner:planet, kaiser:simple>. However, planning at pixel-level granularity is not computationally tractable in large scale problems. Other methods build a latent state-space model that is sufficient to reconstruct the observation stream at pixel level <cit. wahlstrom:pixels_to_torques,watter:embed_to_control>, or to predict its future latent states <cit. ha:world_model,gelada:deepmdp>, which facilitates more efficient planning but still focuses the majority of the model capacity on potentially irrelevant detail. None of these prior methods has constructed a model that facilitates effective planning in visually complex domains such as Atari; results lag behind well-tuned, model-free methods, even in terms of data efficiency <cit. hado:replay>. A quite different approach to model-based RL has recently been developed, focused end-to-end on predicting the value function <cit. silver:predictron>. The main idea of these methods is to construct an abstract MDP model such that planning in the abstract MDP is equivalent to planning in the real environment. This equivalence is achieved by ensuring value equivalence, i.e. that, starting from the same real state, the cumulative reward of a trajectory through the abstract MDP matches the cumulative reward of a trajectory in the real environment. The predictron <cit. silver:predictron> first introduced value equivalent models for predicting value (without actions). Although the underlying model still takes the form of an MDP, there is no requirement for its transition model to match real states in the environment. Instead the MDP model is viewed as a hidden layer of a deep neural network. The unrolled MDP is trained such that the expected cumulative sum of rewards matches the expected value with respect to the real environment, e.g. by temporal-difference learning. Value equivalent models were subsequently extended to optimising value (with actions). TreeQN <cit. farquhar:treeqn> learns an abstract MDP model, such that a tree search over that model (represented by a tree-structured neural network) approximates the optimal value function. Value iteration networks <cit. aviv:vin> learn a local MDP model, such that value iteration over that model (represented by a convolutional neural network) approximates the optimal value function. Value prediction networks <cit. oh:vpn> are perhaps the closest precursor to MuZero: they learn an MDP model grounded in real actions; the unrolled MDP is trained such that the cumulative sum of rewards, conditioned on the actual sequence of actions generated by a simple lookahead search, matches the real environment. Unlike MuZero there is no policy prediction, and the search only utilizes value prediction. \n\nMuZero Algorithm\n================\n<figure. fig:recurrent_search -  <label. fig:recurrent_search> Planning, acting, and training with a learned model. (A) How MuZero uses its model to plan. The model consists of three connected components for representation, dynamics and prediction. Given a previous hidden state s^k-1 and a candidate action a^k, the dynamics function g produces an immediate reward r^k and a new hidden state s^k. The policy p^k and value function v^k are computed from the hidden state s^k by a prediction function f. The initial hidden state s^0 is obtained by passing the past observations (e.g. the Go board or Atari screen) into a representation function h. (B) How MuZero acts in the environment. A Monte-Carlo Tree Search is performed at each timestep t, as described in A. An action a_t+1 is sampled from the search policy π _t, which is proportional to the visit count for each action from the root node. The environment receives the action and generates a new observation o_t+1 and reward u_t+1. At the end of the episode the trajectory data is stored into a replay buffer. (C) How MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation function h receives as input the past observations o_1, ..., o_t from the selected trajectory. The model is subsequently unrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state s^k-1 from the previous step and the real action a_t+k. The parameters of the representation, dynamics and prediction functions are jointly trained, end-to-end by backpropagation-through-time, to predict three quantities: the policy 𝐩^k ≈π _t+k, value function v^k ≈ z_t+k, and reward r_t+k≈ u_t+k, where z_t+k is a sample return: either the final reward (board games) or n-step return (Atari). >We now describe the MuZero algorithm in more detail. Predictions are made at each time-step t, for each of k = 1...K steps, by a model μ _θ, with parameters θ, conditioned on past observations o_1, ..., o_t and future actions a_t+1, ..., a_t+k. The model predicts three future quantities: the policy 𝐩^k_t ≈π (a_t+k+1 | o_1, ..., o_t, a_t+1, ..., a_t+k), the value function v^k_t ≈𝔼[ u_t+k+1 + γ u_t+k+2 + ... | o_1, ..., o_t, a_t+1, ..., a_t+k], and the immediate reward r^k_t ≈ u_t+k, where u_. is the true, observed reward, π is the policy used to select real actions, and γ is the discount function of the environment. Internally, at each time-step t (subscripts _t suppressed for simplicity), the model is represented by the combination of a representation function, a dynamics function, and a prediction function. The dynamics function, r^k, s^k = g_θ (s^k-1, a^k), is a recurrent process that computes, at each hypothetical step k, an immediate reward r^k and an internal state s^k. It mirrors the structure of an MDP model that computes the expected reward and state transition for a given state and action <cit. puterman:MDP>. However, unlike traditional approaches to model-based RL <cit. sutton:book>, this internal state s^k has no semantics of environment state attached to it – it is simply the hidden state of the overall model, and its sole purpose is to accurately predict relevant, future quantities: policies, values, and rewards. In this paper, the dynamics function is represented deterministically; the extension to stochastic transitions is left for future work. The policy and value functions are computed from the internal state s^k by the prediction function, 𝐩^k, v^k = f_θ (s^k), akin to the joint policy and value network of AlphaZero. The “root\" state s^0 is initialized using a representation function that encodes past observations, s^0 = h_θ (o_1, ..., o_t); again this has no special semantics beyond its support for future predictions. Given such a model, it is possible to search over hypothetical future trajectories a^1, ..., a^k given past observations o_1, ..., o_t. For example, a naive search could simply select the k step action sequence that maximizes the value function. More generally, we may apply any MDP planning algorithm to the internal rewards and state space induced by the dynamics function. Specifically, we use an MCTS algorithm similar to AlphaZero’s search, generalized to allow for single agent domains and intermediate rewards (see Methods). At each internal node, it makes use of the policy, value and reward estimates produced by the current model parameters θ. The MCTS algorithm outputs a recommended policy π _t and estimated value ν _t. An action a_t+1∼π _t is then selected. All parameters of the model are trained jointly to accurately match the policy, value, and reward, for every hypothetical step k, to corresponding target values observed after k actual time-steps have elapsed. Similarly to AlphaZero, the improved policy targets are generated by an MCTS search; the first objective is to minimise the error between predicted policy 𝐩_t^k and search policy π _t+k. Also like AlphaZero, the improved value targets are generated by playing the game or MDP. However, unlike AlphaZero, we allow for long episodes with discounting and intermediate rewards by bootstrapping n steps into the future from the search value, z_t = u_t+1 + γ u_t+2 + ... + γ ^n-1 u_t+n + γ ^n ν _t+n. Final outcomes { lose, draw, win} in board games are treated as rewards u_t ∈{  -1, 0, +1 } occuring at the final step of the episode. Specifically, the second objective is to minimize the error between the predicted value v^k_t and the value target, z_t+k For chess, Go and shogi, the same squared error loss as AlphaZero is used for rewards and values. A cross-entropy loss was found to be more stable than a squared error when encountering rewards and values of variable scale in Atari. Cross-entropy was used for the policy loss in both cases.. The reward targets are simply the observed rewards; the third objective is therefore to minimize the error between the predicted reward r^k_t and the observed reward u_t+k. Finally, an L2 regularization term is also added, leading to the overall loss: \n    l_t(θ )     = ∑ _k=0^K l^r (u_t+k, r_t^k) + l^v(z_t+k, v^k_t) + l^p(π _t+k, 𝐩^k_t) + c ||θ ||^2 <label. muzero_eqn>\n where l^r, l^v, and l^p are loss functions for reward, value and policy respectively. Supplementary Figure <ref. fig:muzero_equations> summarizes the equations governing how the MuZero algorithm plans, acts, and learns. \n\nResults\n=======\n Chess \n\n     Shogi \n\n     Go \n\n     Atari \n\n\n 0.62 \n\n     0.745 UTF8min  (e,0.5cm,0.5cm)|c|c|c|c|c|c|c|c|c||c|c|c|c|c|c|c|c|c| [origin=c]180香      [origin=c]180桂 \n\n [origin=c]180銀 \n\n [origin=c]180金 \n\n [origin=c]180玉 \n\n [origin=c]180金 \n\n [origin=c]180銀 \n\n [origin=c]180桂 \n\n [origin=c]180香\n\n\n       [origin=c]180飛 \n\n                         [origin=c]180角 \n\n    \n  [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩\n\n\n                                  \n                                  \n                                  \n  歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩\n\n\n       角 \n\n                         飛 \n\n    \n  香 \n\n     桂 \n\n     銀 \n\n     金 \n\n     玉 \n\n     金 \n\n     銀 \n\n     桂 \n\n     香\n\n\n  \n\n\n\n       0.195 \n[help lines] (0,0) grid (18,18);\n[fill=gray] (3,3) circle[radius=0.1];\n[fill=gray] (3,9) circle[radius=0.1];\n[fill=gray] (3,15) circle[radius=0.1];\n[fill=gray] (9,3) circle[radius=0.1];\n[fill=gray] (9,9) circle[radius=0.1];\n[fill=gray] (9,15) circle[radius=0.1];\n[fill=gray] (15,3) circle[radius=0.1];\n[fill=gray] (15,9) circle[radius=0.1];\n[fill=gray] (15,15) circle[radius=0.1];\n       \n    < g r a p h i c s >\n \n 4c  \n    < g r a p h i c s >\n  \n  <label. fig:results> Evaluation of MuZero throughout training in chess, shogi, Go and Atari. The x-axis shows millions of training steps. For chess, shogi and Go, the y-axis shows Elo rating, established by playing games against AlphaZero using 800 simulations per move for both players. MuZero’s Elo is indicated by the blue line, AlphaZero’s Elo by the horizontal orange line. For Atari, mean (full line) and median (dashed line) human normalized scores across all 57 games are shown on the y-axis. The scores for R2D2 <cit. r2d2>, (the previous state of the art in this domain, based on model-free RL) are indicated by the horizontal orange lines. Performance in Atari was evaluated using 50 simulations every fourth time-step, and then repeating the chosen action four times, as in prior work <cit. dqn>.  We applied the MuZero algorithm to the classic board games Go, chess and shogi  Imperfect information games such as Poker are not directly addressed by our method., as benchmarks for challenging planning problems, and to all 57 games in the Atari Learning Environment <cit. ALE>, as benchmarks for visually complex RL domains. In each case we trained MuZero for K=5 hypothetical steps. Training proceeded for 1 million mini-batches of size 2048 in board games and of size 1024 in Atari. During both training and evaluation, MuZero used 800 simulations for each search in board games, and 50 simulations for each search in Atari. The representation function uses the same convolutional <cit. convnet> and residual <cit. he:resnet> architecture as AlphaZero, but with 16 residual blocks instead of 20. The dynamics function uses the same architecture as the representation function and the prediction function uses the same architecture as AlphaZero. All networks use 256 hidden planes (see Methods for further details). Figure <ref. fig:results> shows the performance throughout training in each game. In Go, MuZero slightly exceeded the performance of AlphaZero, despite using less computation per node in the search tree (16 residual blocks per evaluation in MuZero compared to 20 blocks in AlphaZero). This suggests that MuZero may be caching its computation in the search tree and using each additional application of the dynamics model to gain a deeper understanding of the position. In Atari, MuZero achieved a new state of the art for both mean and median normalized score across the 57 games of the Arcade Learning Environment, outperforming the previous state-of-the-art method R2D2 <cit. r2d2> (a model-free approach) in 42 out of 57 games, and outperforming the previous best model-based approach SimPLe <cit. kaiser:simple> in all games (see Table <ref. tab:atari-results-at30>). We also evaluated a second version of MuZero that was optimised for greater sample efficiency. Specifically, it reanalyzes old trajectories by re-running the MCTS using the latest network parameters to provide fresh targets (see Appendix <ref. reanalyze>). When applied to 57 Atari games, using 200 million frames of experience per game, MuZero Reanalyze achieved 731%  median normalized score, compared to 192% , 231%  and 431%  for previous state-of-the-art model-free approaches IMPALA <cit. impala>, Rainbow <cit. rainbow> and LASER <cit. laser> respectively. \n\n Agent \n\n     Median \n\n     Mean \n\n     Env. Frames \n\n     Training Time \n\n     Training Steps \n\n\n Ape-X <cit. apex> \n\n     434.1%  \n\n     1695.6%  \n\n     22.8B \n\n     5 days \n\n     8.64M\n\n\n R2D2 <cit. r2d2> \n\n     1920.6%  \n\n     4024.9%  \n\n     37.5B \n\n     5 days \n\n     2.16M \n\n\n MuZero \n\n     2041.1%   \n\n     4999.2%   \n\n     20.0B \n\n     12 hours \n\n     1M \n\n\n IMPALA <cit. impala> \n\n     191.8%  \n\n     957.6%  \n\n     200M \n\n     – \n\n     – \n\n\n Rainbow <cit. rainbow> \n\n     231.1%  \n\n     – \n\n     200M \n\n     10 days \n\n     – \n\n\n UNREALa <cit. unreal> \n\n     250% a \n\n     880% a \n\n     250M \n\n     – \n\n     – \n\n\n LASER <cit. laser> \n\n     431%  \n\n     – \n\n     200M \n\n     – \n\n     – \n\n\n MuZero Reanalyze \n\n     731.1%  \n\n     2168.9%  \n\n     200M \n\n     12 hours \n\n     1M \n\n\n \n\n <label. tab:atari-comparison> Comparison of MuZero against previous agents in Atari. We compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, compared to human testers. The best results are highlighted in bold. MuZero sets a new state of the art in both settings. aHyper-parameters were tuned per game. \n\n\n\nTo understand the role of the model in MuZero we also ran several experiments, focusing on the board game of Go and the Atari game of Ms.  Pacman. First, we tested the scalability of planning (Figure <ref. fig:ablations>A), in the canonical planning problem of Go. We compared the performance of search in AlphaZero, using a perfect model, to the performance of search in MuZero, using a learned model. Specifically, the fully trained AlphaZero or MuZero was evaluated by comparing MCTS with different thinking times. MuZero matched the performance of a perfect model, even when doing much larger searches (up to 10s thinking time) than those from which the model was trained (around 0.1s thinking time, see also Figure <ref. fig:extended_ablations>A). We also investigated the scalability of planning across all Atari games (see Figure <ref. fig:ablations>B). We compared MCTS with different numbers of simulations, using the fully trained MuZero. The improvements due to planning are much less marked than in Go, perhaps because of greater model inaccuracy; performance improved slightly with search time, but plateaued at around 100 simulations. Even with a single simulation – i.e. when selecting moves solely according to the policy network – MuZero performed well, suggesting that, by the end of training, the raw policy has learned to internalise the benefits of search (see also Figure <ref. fig:extended_ablations>B). Next, we tested our model-based learning algorithm against a comparable model-free learning algorithm (see Figure <ref. fig:ablations>C). We replaced the training objective of MuZero (Equation 1) with a model-free Q-learning objective (as used by R2D2), and the dual value and policy heads with a single head representing the Q-function Q(· |s_t). Subsequently, we trained and evaluated the new model without using any search. When evaluated on Ms.  Pacman, our model-free algorithm achieved identical results to R2D2, but learned significantly slower than MuZero and converged to a much lower final score. We conjecture that the search-based policy improvement step of MuZero provides a stronger learning signal than the high bias, high variance targets used by Q-learning. To better understand the nature of MuZero’s learning algorithm, we measured how MuZero’s training scales with respect to the amount of search it uses during training. Figure <ref. fig:ablations>D shows the performance in Ms.  Pacman, using an MCTS of different simulation counts per move throughout training. Surprisingly, and in contrast to previous work <cit. negative_mcts_results>, even with only 6 simulations per move – fewer than the number of actions – MuZero learned an effective policy and improved rapidly. With more simulations performance jumped significantly higher. For analysis of the policy improvement during each individual iteration, see also Figure <ref. fig:extended_ablations> C and D. <figure. fig:ablations -  <label. fig:ablations> Evaluations of MuZero on Go (A), all 57 Atari Games (B) and Ms.  Pacman (C-D). (A) Scaling with search time per move in Go, comparing the learned model with the ground truth simulator. Both networks were trained at 800 simulations per search, equivalent to 0.1 seconds per search. Remarkably, the learned model is able to scale well to up to two orders of magnitude longer searches than seen during training. (B) Scaling of final human normalized mean score in Atari with the number of simulations per search. The network was trained at 50 simulations per search. Dark line indicates mean score, shaded regions indicate 25th to 75th and 5th to 95th percentiles. The learned model's performance increases up to 100 simulations per search. Beyond, even when scaling to much longer searches than during training, the learned model's performance remains stable and only decreases slightly. This contrasts with the much better scaling in Go (A), presumably due to greater model inaccuracy in Atari than Go. (C) Comparison of MCTS based training with Q-learning in the MuZero framework on Ms.  Pacman, keeping network size and amount of training constant. The state of the art Q-Learning algorithm R2D2 is shown as a baseline. Our Q-Learning implementation reaches the same final score as R2D2, but improves slower and results in much lower final performance compared to MCTS based training. (D) Different networks trained at different numbers of simulations per move, but all evaluated at 50 simulations per move. Networks trained with more simulations per move improve faster, consistent with ablation (B), where the policy improvement is larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training with less simulations per move than are enough to cover all 8 possible actions in Ms.  Pacman. >\n\nConclusions\n===========\nMany of the breakthroughs in artificial intelligence have been based on either high-performance planning <cit. campbell:deep-blue, Silver16AG, Silver18AZ> or model-free reinforcement learning methods <cit. dqn, openai_dota, alphastar>. In this paper we have introduced a method that combines the benefits of both approaches. Our algorithm, MuZero, has both matched the superhuman performance of high-performance planning algorithms in their favored domains – logically complex board games such as chess and Go – and outperformed state-of-the-art model-free RL algorithms in their favored domains – visually complex Atari games. Crucially, our method does not require any knowledge of the game rules or environment dynamics, potentially paving the way towards the application of powerful learning and planning methods to a host of real-world domains for which there exists no perfect simulator. \n\nAcknowledgments\n===============\nLorrayne Bennett, Oliver Smith and Chris Apps for organizational assistance; Koray Kavukcuoglu for reviewing the paper; Thomas Anthony, Matthew Lai, Nenad Tomasev, Ulrich Paquet, Sumedh Ghaisas for many fruitful discussions; and the rest of the DeepMind team for their support. plain   \n\nSupplementary Materials\n-----------------------\n\n - Pseudocode description of the MuZero algorithm. \n\n- Data for Figures <ref. fig:results>, <ref. fig:ablations>, <ref. fig:muzero_equations>, <ref. fig:extended_ablations>, <ref. fig:learning_curve_atari> and Tables <ref. tab:atari-comparison>, <ref. tab:atari-results-at30>, <ref. tab:atari-results-at30-rnd-starts> in JSON format. \n\nSupplementary materials can be accessed from the ancillary file section of the arXiv submission.   <label. methods> \n\nComparison to AlphaZero\n=======================\nMuZero is designed for a more general setting than AlphaGo Zero <cit. Silver17AG0> and AlphaZero <cit. Silver18AZ>. In AlphaGo Zero and AlphaZero the planning process makes use of two separate components: a simulator implements the rules of the game, which are used to update the state of the game while traversing the search tree; and a neural network jointly predicts the corresponding policy and value of a board position produced by the simulator (see Figure <ref. fig:recurrent_search> A). Specifically, AlphaGo Zero and AlphaZero use knowledge of the rules of the game in three places: (1) state transitions in the search tree, (2) actions available at each node of the search tree, (3) episode termination within the search tree. In MuZero, all of these have been replaced with the use of a single implicit model learned by a neural network (see Figure <ref. fig:recurrent_search> B): \n- State transitions. AlphaZero had access to a perfect simulator of the true dynamics process. In contrast, MuZero employs a learned dynamics model within its search. Under this model, each node in the tree is represented by a corresponding hidden state; by providing a hidden state s_k-1 and an action a_k to the model the search algorithm can transition to a new node s_k = g(s_k-1, a_k). \n\n- Actions available. AlphaZero used the set of legal actions obtained from the simulator to mask the prior produced by the network everywhere in the search tree. MuZero only masks legal actions at the root of the search tree where the environment can be queried, but does not perform any masking within the search tree. This is possible because the network rapidly learns not to predict actions that never occur in the trajectories it is trained on. \n\n- Terminal nodes. AlphaZero stopped the search at tree nodes representing terminal states and used the terminal value provided by the simulator instead of the value produced by the network. MuZero does not give special treatment to terminal nodes and always uses the value predicted by the network. Inside the tree, the search can proceed past a terminal node - in this case the network is expected to always predict the same value. This is achieved by treating terminal states as absorbing states during training. \n\nIn addition, MuZero is designed to operate in the general reinforcement learning setting: single-agent domains with discounted intermediate rewards of arbitrary magnitude. In contrast, AlphaGo Zero and AlphaZero were designed to operate in two-player games with undiscounted terminal rewards of ± 1. \n\nSearch\n======\nWe now describe the search algorithm used by MuZero. Our approach is based upon Monte-Carlo tree search with upper confidence bounds, an approach to planning that converges asymptotically to the optimal policy in single agent domains and to the minimax value function in zero sum games <cit. kocsis-sepesvari>. Every node of the search tree is associated with an internal state s. For each action a from s there is an edge (s,a) that stores a set of statistics { N(s,a), Q(s,a), P(s,a), R(s,a), S(s,a) }, respectively representing visit counts N, mean value Q, policy P, reward R, and state transition S. Similar to AlphaZero, the search is divided into three stages, repeated for a number of simulations. Selection: Each simulation starts from the internal root state s^0, and finishes when the simulation reaches a leaf node s^l. For each hypothetical time-step k = 1 ... l of the simulation, an action a^k is selected according to the stored statistics for internal state s^k-1, by maximizing over an upper confidence bound <cit. rosin:puct><cit. Silver18AZ>, \n    <label. pUCT> a^k = max _a[ Q(s, a) + P(s, a) ·√(∑ _b N(s, b))/1 + N(s, a)(c_1 + log(∑ _b N(s, b) + c_2 + 1/c_2)) ]\nThe constants c_1 and c_2 are used to control the influence of the prior P(s, a) relative to the value Q(s, a) as nodes are visited more often. In our experiments, c_1 = 1.25 and c_2 = 19652. For k<l, the next state and reward are looked up in the state transition and reward table s^k = S(s^k-1, a^k), r^k = R(s^k-1, a^k). Expansion: At the final time-step l of the simulation, the reward and state are computed by the dynamics function, r^l, s^l = g_θ (s^l-1, a^l), and stored in the corresponding tables, R(s^l-1, a^l) = r^l, S(s^l-1, a^l) = s^l. The policy and value are computed by the prediction function, 𝐩^l, v^l = f_θ (s^l). A new node, corresponding to state s^l is added to the search tree. Each edge (s^l, a) from the newly expanded node is initialized to {  N(s^l, a)=0, Q(s^l, a)=0, P(s^l, a)=𝐩^l }. Note that the search algorithm makes at most one call to the dynamics function and prediction function respectively per simulation; the computational cost is of the same order as in AlphaZero. Backup: At the end of the simulation, the statistics along the trajectory are updated. The backup is generalized to the case where the environment can emit intermediate rewards, have a discount γ different from 1, and the value estimates are unbounded In board games the discount is assumed to be 1 and there are no intermediate rewards.. For k= l ... 0, we form an l-k-step estimate of the cumulative discounted reward, bootstrapping from the value function v^l, \n    G^k = ∑ _τ =0^l-1-kγ ^τ r_k+ 1 +τ + γ ^l - k v^l\nFor k= l ... 1, we update the statistics for each edge (s^k-1, a^k) in the simulation path as follows, \n    Q(s^k-1,a^k)      := N(s^k-1,a^k) · Q(s^k-1,a^k) + G^k/N(s^k-1,a^k) + 1\n     N(s^k-1,a^k)      := N(s^k-1,a^k) + 1\nIn two-player zero sum games the value functions are assumed to be bounded within the [0, 1] interval. This choice allows us to combine value estimates with probabilities using the pUCT rule (Eqn <ref. pUCT>). However, since in many environments the value is unbounded, it is necessary to adjust the pUCT rule. A simple solution would be to use the maximum score that can be observed in the environment to either re-scale the value or set the pUCT constants appropriately <cit. MCTS_single_agent>. However, both solutions are game specific and require adding prior knowledge to the MuZero algorithm. To avoid this, MuZero computes normalized Q value estimates Q∈ [0, 1] by using the minimum-maximum values observed in the search tree up to that point. When a node is reached during the selection stage, the algorithm computes the normalized Q values of its edges to be used in the pUCT rule using the equation below: \n    Q(s^k-1, a^k) = Q(s^k-1, a^k) - min _s, a ∈ TreeQ(s, a)/max _s,a ∈ Tree Q(s, a) - min _s,a ∈ Tree Q(s, a)\n\n\nHyperparameters\n===============\nFor simplicity we preferentially use the same architectural choices and hyperparameters as in previous work. Specifically, we started with the network architecture and search choices of AlphaZero <cit. Silver18AZ>. For board games, we use the same UCB constants, dirichlet exploration noise and the same 800 simulations per search as in AlphaZero. Due to the much smaller branching factor and simpler policies in Atari, we only used 50 simulations per search to speed up experiments. As shown in Figure <ref. fig:ablations>B, the algorithm is not very sensitive to this choice. We also use the same discount (0.997) and value transformation (see Network Architecture section) as R2D2 <cit. r2d2>. For parameter values not mentioned in the text, please refer to the pseudocode. \n\nData Generation\n===============\nTo generate training data, the latest checkpoint of the network (updated every 1000 training steps) is used to play games with MCTS. In the board games Go, chess and shogi the search is run for 800 simulations per move to pick an action; in Atari due to the much smaller action space 50 simulations per move are sufficient. For board games, games are sent to the training job as soon as they finish. Due to the much larger length of Atari games (up to 30 minutes or 108,000 frames), intermediate sequences are sent every 200 moves. In board games, the training job keeps an in-memory replay buffer of the most recent 1 million games received; in Atari, where the visual observations are larger, the most recent 125 thousand sequences of length 200 are kept. During the generation of experience in the board game domains, the same exploration scheme as the one described in AlphaZero <cit. Silver18AZ> is used. Using a variation of this scheme, in the Atari domain actions are sampled from the visit count distribution throughout the duration of each game, instead of just the first k moves. Moreover, the visit count distribution is parametrized using a temperature parameter T: \n    p_α = N(α )^1 / T/∑ _b N(b)^1 / T\nT is decayed as a function of the number of training steps of the network. Specifically, for the first 500k training steps a temperature of 1 is used, for the next 250k steps a temperature of 0.5 and for the remaining 250k a temperature of 0.25. This ensures that the action selection becomes greedier as training progresses. \n\nNetwork Input\n=============\n\n\nRepresentation Function\n-----------------------\nThe history over board states used as input to the representation function for Go, chess and shogi is represented similarly to AlphaZero <cit. Silver18AZ>. In Go and shogi we encode the last 8 board states as in AlphaZero; in chess we increased the history to the last 100 board states to allow correct prediction of draws. For Atari, the input of the representation function includes the last 32 RGB frames at resolution 96x96 along with the last 32 actions that led to each of those frames. We encode the historical actions because unlike board games, an action in Atari does not necessarily have a visible effect on the observation. RGB frames are encoded as one plane per color, rescaled to the range [0, 1], for red, green and blue respectively. We perform no other normalization, whitening or other preprocessing of the RGB input. Historical actions are encoded as simple bias planes, scaled as a / 18 (there are 18 total actions in Atari). \n\nDynamics Function\n-----------------\nThe input to the dynamics function is the hidden state produced by the representation function or previous application of the dynamics function, concatenated with a representation of the action for the transition. Actions are encoded spatially in planes of the same resolution as the hidden state. In Atari, this resolution is 6x6 (see description of downsampling in Network Architecture section), in board games this is the same as the board size (19x19 for Go, 8x8 for chess, 9x9 for shogi). In Go, a normal action (playing a stone on the board) is encoded as an all zero plane, with a single one in the position of the played stone. A pass is encoded as an all zero plane. In chess, 8 planes are used to encode the action. The first one-hot plane encodes which position the piece was moved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the target position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or not. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions, not all of which are legal, and we use the same action space for policy prediction and to encode the dynamics function input. The remaining five binary planes are used to indicate the type of promotion, if any (queen, knight, bishop, rook, none). The encoding for shogi is similar, with a total of 11 planes. We use the first 8 planes to indicate where the piece moved from - either a board position (first one-hot plane) or the drop of one of the seven types of prisoner (remaining 7 binary planes). The next two planes are used to encode the target as in chess. The remaining binary plane indicates whether the move was a promotion or not. In Atari, an action is encoded as a one hot vector which is tiled appropriately into planes. \n\nNetwork Architecture\n====================\n<label. sec:network_architecture> The prediction function 𝐩^k, v^k = f_θ (s^k) uses the same architecture as AlphaZero: one or two convolutional layers that preserve the resolution but reduce the number of planes, followed by a fully connected layer to the size of the output. For value and reward prediction in Atari we follow <cit. pohlen2018observe> in scaling targets using an invertible transform h(x) = sign(x)(√(|x| + 1) - 1 + ϵ x), where ϵ = 0.001 in all our experiments. We then apply a transformation ϕ to the scalar reward and value targets in order to obtain equivalent categorical representations. We use a discrete support set of size 601 with one support for every integer between -300 and 300. Under this transformation, each scalar is represented as the linear combination of its two adjacent supports, such that the original value can be recovered by x = x_low * p_low + x_high * p_high. As an example, a target of 3.7 would be represented as a weight of 0.3 on the support for 3 and a weight of 0.7 on the support for 4. The value and reward outputs of the network are also modeled using a softmax output of size 601. During inference the actual value and rewards are obtained by first computing their expected value under their respective softmax distribution and subsequently by inverting the scaling transformation. Scaling and transformation of the value and reward happens transparently on the network side and is not visible to the rest of the algorithm. Both the representation and dynamics function use the same architecture as AlphaZero, but with 16 instead of 20 residual blocks <cit. he:resnet>. We use 3x3 kernels and 256 hidden planes for each convolution. For Atari, where observations have large spatial resolution, the representation function starts with a sequence of convolutions with stride 2 to reduce the spatial resolution. Specifically, starting with an input observation of resolution 96x96 and 128 planes (32 history frames of 3 color channels each, concatenated with the corresponding 32 actions broadcast to planes), we downsample as follows: \n - 1 convolution with stride 2 and 128 output planes, output resolution 48x48. \n\n- 2 residual blocks with 128 planes \n\n- 1 convolution with stride 2 and 256 output planes, output resolution 24x24. \n\n- 3 residual blocks with 256 planes. \n\n- Average pooling with stride 2, output resolution 12x12. \n\n- 3 residual blocks with 256 planes. \n\n- Average pooling with stride 2, output resolution 6x6. \n\nThe kernel size is 3x3 for all operations. For the dynamics function (which always operates at the downsampled resolution of 6x6), the action is first encoded as an image, then stacked with the hidden state of the previous step along the plane dimension. \n\nTraining\n========\nDuring training, the MuZero network is unrolled for K hypothetical steps and aligned to sequences sampled from the trajectories generated by the MCTS actors. Sequences are selected by sampling a state from any game in the replay buffer, then unrolling for K steps from that state. In Atari, samples are drawn according to prioritized replay <cit. Schaul2016>, with priority P(i) = p_i^α/∑ _kp_k^α, where p_i = |ν _i - z_i|, ν is the search value and z the observed n-step return. To correct for sampling bias introduced by the prioritized sampling, we scale the loss using the importance sampling ratio w_i = (1/N·1/P(i))^β. In all our experiments, we set α = β = 1. For board games, states are sampled uniformly. Each observation o_t along the sequence also has a corresponding MCTS policy π _t, estimated value ν _t and environment reward u_t. At each unrolled step k the network has a loss to the value, policy and reward target for that step, summed to produce the total loss for the MuZero network (see Equation <ref. muzero_eqn>). Note that, in board games without intermediate rewards, we omit the reward prediction loss. For board games, we bootstrap directly to the end of the game, equivalent to predicting the final outcome; for Atari we bootstrap for n=10 steps into the future. To maintain roughly similar magnitude of gradient across different unroll steps, we scale the gradient in two separate locations: \n - We scale the loss of each head by 1/K, where K is the number of unroll steps. This ensures that the total gradient has similar magnitude irrespective of how many steps we unroll for. \n\n- We also scale the gradient at the start of the dynamics function by 1/2. This ensures that the total gradient applied to the dynamics function stays constant. \n\nIn the experiments reported in this paper, we always unroll for K=5 steps. For a detailed illustration, see Figure <ref. fig:recurrent_search>. To improve the learning process and bound the activations, we also scale the hidden state to the same range as the action input ([0, 1]): s_scaled = s - min (s)/max (s) - min (s). All experiments were run using third generation Google Cloud TPUs <cit. tpuv3>. For each board game, we used 16 TPUs for training and 1000 TPUs for selfplay. For each game in Atari, we used 8 TPUs for training and 32 TPUs for selfplay. The much smaller proportion of TPUs used for acting in Atari is due to the smaller number of simulations per move (50 instead of 800) and the smaller size of the dynamics function compared to the representation function. \n\nReanalyze\n=========\n <label. reanalyze> To improve the sample efficiency of MuZero we introduced a second variant of the algorithm, MuZero Reanalyze. MuZero Reanalyze revisits its past time-steps and re-executes its search using the latest model parameters, potentially resulting in a better quality policy than the original search. This fresh policy is used as the policy target for 80%  of updates during MuZero training. Furthermore, a target network <cit. dqn> · , v^- = f_θ ^-(s^0), based on recent parameters θ ^-, is used to provide a fresher, stable n-step bootstrapped target for the value function, z_t = u_t+1 + γ u_t+2 + ... + γ ^n-1 u_t+n + γ ^n v^-_t+n. In addition, several other hyperparameters were adjusted – primarily to increase sample reuse and avoid overfitting of the value function. Specifically, 2.0 samples were drawn per state, instead of 0.1; the value target was weighted down to 0.25 compared to weights of 1.0 for policy and reward targets; and the n-step return was reduced to n=5 steps instead of n=10 steps. \n\nEvaluation\n==========\nWe evaluated the relative strength of MuZero (Figure <ref. fig:results>) in board games by measuring the Elo rating of each player. We estimate the probability that player a will defeat player b by a logistic function p(a  defeats  b) = (1 + 10^(c_elo (e(b) - e(a))))^-1, and estimate the ratings e(· ) by Bayesian logistic regression, computed by the BayesElo program <cit. coulom:bayeselo> using the standard constant c_elo = 1/400. Elo ratings were computed from the results of a 800 simulations per move tournament between iterations of MuZero during training, and also a baseline player: either Stockfish, Elmo or AlphaZero respectively. Baseline players used an equivalent search time of 100ms per move. The Elo rating of the baseline players was anchored to publicly available values  <cit. Silver18AZ>. In Atari, we computed mean reward over 1000 episodes per game, limited to the standard 30 minutes or 108,000 frames per episode <cit. gorila>, using 50 simulations per move unless indicated otherwise. In order to mitigate the effects of the deterministic nature of the Atari simulator we employed two different evaluation strategies: 30 noop random starts and human starts. For the former, at the beginning of each episode a random number of between 0 and 30 noop actions are applied to the simulator before handing control to the agent. For the latter, start positions are sampled from human expert play to initialize the Atari simulator before handing the control to the agent <cit. gorila>.  <figure. fig:atari_repeatability -  <label. fig:atari_repeatability> Repeatability of MuZero in Atari for five games. Total reward is shown on the y-axis, millions of training steps on the x-axis. Dark line indicates median score across 10 separate training runs, light lines indicate individual training runs, and the shaded region indicates 25th to 75th percentile. > \n    Model\n       . [                   s^0 = h_θ (o_1, ..., o_t);              r^k, s^k    = g_θ (s^k-1, a^k);              𝐩^k, v^k           = f_θ (s^k) ]}  𝐩^k, v^k, r^k = μ _θ (o_1, ..., o_t, a^1, ..., a^k)\n\n\n\n    Search\n    ν _t, π _t     = MCTS(s^0_t, μ _θ ) \n     a_t    ∼π _t \n       Learning Rule\n    𝐩^k_t, v^k_t, r^k_t     = μ _θ (o_1, ..., o_t, a_t+1, ..., a_t+k) \n     z_t     = {[                                                u_T                                          for games; u_t+1 + γ u_t+2 + ... + γ ^n-1 u_t+n + γ ^n ν _t+n                                   for general MDPs ]. \n     l_t(θ )     = ∑ _k=0^K l^r (u_t+k, r_t^k) + l^v(z_t+k, v^k_t) + l^p(π _t+k, p^k_t) + c ||θ ||^2 \n       Losses\n     l^r(u, r)     = {[                0        for games;      ϕ(u)^T log𝐫 for general MDPs ]. \n     l^v(z, q)     = {[        (z - q)^2        for games;      ϕ(z)^T log𝐪 for general MDPs ]. \n     l^p(π , p)     = π^T log𝐩\n\n\n <label. fig:muzero_equations> Equations summarising the MuZero algorithm. Here, ϕ (x) refers to the representation of a real number x through a linear combination of its adjacent integers, as described in the Network Architecture section. \n\n<figure. fig:extended_ablations -  <label. fig:extended_ablations> Details of MuZero evaluations (A-B) and policy improvement ablations (C-D). (A-B) Distribution of evaluation depth in the search tree for the learned model for the evaluations in Figure <ref. fig:ablations>A-B. The network was trained over 5 hypothetical steps, as indicated by the red line. Dark blue line indicates median depth from the root, dark shaded region shows 25th to 75th percentile, light shaded region shows 5th to 95th percentile. (C) Policy improvement in Ms. Pacman - a single network was trained at 50 simulations per search and is evaluated at different numbers of simulations per search, including playing according to the argmax of the raw policy network. The policy improvement effect of the search over the raw policy network is clearly visible throughout training. This consistent gap between the performance with and without search highlights the policy improvement that MuZero exploits, by continually updating towards the improved policy, to efficiently progress towards the optimal policy. (D) Policy improvement in Go - a single network was trained at 800 simulations per search and is evaluated at different numbers of simulations per search. In Go, the playing strength improvement from longer searches is much larger than in Ms.  Pacman and persists throughout training, consistent with previous results in <cit. Silver17AG0>. This suggests, as might intuitively be expected, that the benefit of models is greatest in precision planning domains. >\n\n \n  Game \n\n     Random \n\n     Human \n\n     SimPLe <cit. kaiser:simple> \n\n     Ape-X <cit. apex> \n\n     R2D2 <cit. r2d2> \n\n     MuZero \n\n     MuZero normalized \n\n\n alien \n\n    227.75 \n\n     7,127.80 \n\n     616.90 \n\n     40,805.00 \n\n     229,496.90 \n\n     741,812.63 \n\n     10,747.5 % \n\n\n amidar \n\n    5.77 \n\n     1,719.53 \n\n     74.30 \n\n     8,659.00 \n\n     29,321.40 \n\n     28,634.39 \n\n     1,670.5 % \n\n\n assault \n\n    222.39 \n\n     742.00 \n\n     527.20 \n\n     24,559.00 \n\n     108,197.00 \n\n     143,972.03 \n\n     27,664.9 % \n\n\n asterix \n\n    210.00 \n\n     8,503.33 \n\n     1,128.30 \n\n     313,305.00 \n\n     999,153.30 \n\n     998,425.00 \n\n     12,036.4 % \n\n\n asteroids \n\n    719.10 \n\n     47,388.67 \n\n     793.60 \n\n     155,495.00 \n\n     357,867.70 \n\n     678,558.64 \n\n     1,452.4 % \n\n\n atlantis \n\n    12,850.00 \n\n     29,028.13 \n\n     20,992.50 \n\n     944,498.00 \n\n     1,620,764.00 \n\n     1,674,767.20 \n\n     10,272.6 % \n\n\n bank heist \n\n    14.20 \n\n     753.13 \n\n     34.20 \n\n     1,716.00 \n\n     24,235.90 \n\n     1,278.98 \n\n     171.2 % \n\n\n battle zone \n\n    2,360.00 \n\n     37,187.50 \n\n     4,031.20 \n\n     98,895.00 \n\n     751,880.00 \n\n     848,623.00 \n\n     2,429.9 % \n\n\n beam rider \n\n    363.88 \n\n     16,926.53 \n\n     621.60 \n\n     63,305.00 \n\n     188,257.40 \n\n     454,993.53 \n\n     2,744.9 % \n\n\n berzerk \n\n    123.65 \n\n     2,630.42 \n\n     - \n\n     57,197.00 \n\n     53,318.70 \n\n     85,932.60 \n\n     3,423.1 % \n\n\n bowling \n\n    23.11 \n\n     160.73 \n\n     30.00 \n\n     18.00 \n\n     219.50 \n\n     260.13 \n\n     172.2 % \n\n\n boxing \n\n    0.05 \n\n     12.06 \n\n     7.80 \n\n     100.00 \n\n     98.50 \n\n     100.00 \n\n     832.2 % \n\n\n breakout \n\n    1.72 \n\n     30.47 \n\n     16.40 \n\n     801.00 \n\n     837.70 \n\n     864.00 \n\n     2,999.2 % \n\n\n centipede \n\n    2,090.87 \n\n     12,017.04 \n\n     - \n\n     12,974.00 \n\n     599,140.30 \n\n     1,159,049.27 \n\n     11,655.6 % \n\n\n chopper command \n\n    811.00 \n\n     7,387.80 \n\n     979.40 \n\n     721,851.00 \n\n     986,652.00 \n\n     991,039.70 \n\n     15,056.4 % \n\n\n crazy climber \n\n    10,780.50 \n\n     35,829.41 \n\n     62,583.60 \n\n     320,426.00 \n\n     366,690.70 \n\n     458,315.40 \n\n     1,786.6 % \n\n\n defender \n\n    2,874.50 \n\n     18,688.89 \n\n     - \n\n     411,944.00 \n\n     665,792.00 \n\n     839,642.95 \n\n     5,291.2 % \n\n\n demon attack \n\n    152.07 \n\n     1,971.00 \n\n     208.10 \n\n     133,086.00 \n\n     140,002.30 \n\n     143,964.26 \n\n     7,906.4 % \n\n\n double dunk \n\n    -18.55 \n\n     -16.40 \n\n     - \n\n     24.00 \n\n     23.70 \n\n     23.94 \n\n     1,976.3 % \n\n\n enduro \n\n    0.00 \n\n     860.53 \n\n     - \n\n     2,177.00 \n\n     2,372.70 \n\n     2,382.44 \n\n     276.9 % \n\n\n fishing derby \n\n    -91.71 \n\n     -38.80 \n\n     -90.70 \n\n     44.00 \n\n     85.80 \n\n     91.16 \n\n     345.6 % \n\n\n freeway \n\n    0.01 \n\n     29.60 \n\n     16.70 \n\n     34.00 \n\n     32.50 \n\n     33.03 \n\n     111.6 % \n\n\n frostbite \n\n    65.20 \n\n     4,334.67 \n\n     236.90 \n\n     9,329.00 \n\n     315,456.40 \n\n     631,378.53 \n\n     14,786.7 % \n\n\n gopher \n\n    257.60 \n\n     2,412.50 \n\n     596.80 \n\n     120,501.00 \n\n     124,776.30 \n\n     130,345.58 \n\n     6,036.8 % \n\n\n gravitar \n\n    173.00 \n\n     3,351.43 \n\n     173.40 \n\n     1,599.00 \n\n     15,680.70 \n\n     6,682.70 \n\n     204.8 % \n\n\n hero \n\n    1,026.97 \n\n     30,826.38 \n\n     2,656.60 \n\n     31,656.00 \n\n     39,537.10 \n\n     49,244.11 \n\n     161.8 % \n\n\n ice hockey \n\n    -11.15 \n\n     0.88 \n\n     -11.60 \n\n     33.00 \n\n     79.30 \n\n     67.04 \n\n     650.0 % \n\n\n jamesbond \n\n    29.00 \n\n     302.80 \n\n     100.50 \n\n     21,323.00 \n\n     25,354.00 \n\n     41,063.25 \n\n     14,986.9 % \n\n\n kangaroo \n\n    52.00 \n\n     3,035.00 \n\n     51.20 \n\n     1,416.00 \n\n     14,130.70 \n\n     16,763.60 \n\n     560.2 % \n\n\n krull \n\n    1,598.05 \n\n     2,665.53 \n\n     2,204.80 \n\n     11,741.00 \n\n     218,448.10 \n\n     269,358.27 \n\n     25,083.4 % \n\n\n kung fu master \n\n    258.50 \n\n     22,736.25 \n\n     14,862.50 \n\n     97,830.00 \n\n     233,413.30 \n\n     204,824.00 \n\n     910.1 % \n\n\n montezuma revenge \n\n    0.00 \n\n     4,753.33 \n\n     - \n\n     2,500.00 \n\n     2,061.30 \n\n     0.00 \n\n     0.0 % \n\n\n ms pacman \n\n    307.30 \n\n     6,951.60 \n\n     1,480.00 \n\n     11,255.00 \n\n     42,281.70 \n\n     243,401.10 \n\n     3,658.7 % \n\n\n name this game \n\n    2,292.35 \n\n     8,049.00 \n\n     2,420.70 \n\n     25,783.00 \n\n     58,182.70 \n\n     157,177.85 \n\n     2,690.5 % \n\n\n phoenix \n\n    761.40 \n\n     7,242.60 \n\n     - \n\n     224,491.00 \n\n     864,020.00 \n\n     955,137.84 \n\n     14,725.3 % \n\n\n pitfall \n\n    -229.44 \n\n     6,463.69 \n\n     - \n\n     -1.00 \n\n     0.00 \n\n     0.00 \n\n     3.4 % \n\n\n pong \n\n    -20.71 \n\n     14.59 \n\n     12.80 \n\n     21.00 \n\n     21.00 \n\n     21.00 \n\n     118.2 % \n\n\n private eye \n\n    24.94 \n\n     69,571.27 \n\n     35.00 \n\n     50.00 \n\n     5,322.70 \n\n     15,299.98 \n\n     22.0 % \n\n\n qbert \n\n    163.88 \n\n     13,455.00 \n\n     1,288.80 \n\n     302,391.00 \n\n     408,850.00 \n\n     72,276.00 \n\n     542.6 % \n\n\n riverraid \n\n    1,338.50 \n\n     17,118.00 \n\n     1,957.80 \n\n     63,864.00 \n\n     45,632.10 \n\n     323,417.18 \n\n     2,041.1 % \n\n\n road runner \n\n    11.50 \n\n     7,845.00 \n\n     5,640.60 \n\n     222,235.00 \n\n     599,246.70 \n\n     613,411.80 \n\n     7,830.5 % \n\n\n robotank \n\n    2.16 \n\n     11.94 \n\n     - \n\n     74.00 \n\n     100.40 \n\n     131.13 \n\n     1,318.7 % \n\n\n seaquest \n\n    68.40 \n\n     42,054.71 \n\n     683.30 \n\n     392,952.00 \n\n     999,996.70 \n\n     999,976.52 \n\n     2,381.5 % \n\n\n skiing \n\n    -17,098.09 \n\n     -4,336.93 \n\n     - \n\n     -10,790.00 \n\n     -30,021.70 \n\n     -29,968.36 \n\n     -100.9 % \n\n\n solaris \n\n    1,236.30 \n\n     12,326.67 \n\n     - \n\n     2,893.00 \n\n     3,787.20 \n\n     56.62 \n\n     -10.6 % \n\n\n space invaders \n\n    148.03 \n\n     1,668.67 \n\n     - \n\n     54,681.00 \n\n     43,223.40 \n\n     74,335.30 \n\n     4,878.7 % \n\n\n star gunner \n\n    664.00 \n\n     10,250.00 \n\n     - \n\n     434,343.00 \n\n     717,344.00 \n\n     549,271.70 \n\n     5,723.0 % \n\n\n surround \n\n    -9.99 \n\n     6.53 \n\n     - \n\n     7.00 \n\n     9.90 \n\n     9.99 \n\n     120.9 % \n\n\n tennis \n\n    -23.84 \n\n     -8.27 \n\n     - \n\n     24.00 \n\n     -0.10 \n\n     0.00 \n\n     153.1 % \n\n\n time pilot \n\n    3,568.00 \n\n     5,229.10 \n\n     - \n\n     87,085.00 \n\n     445,377.30 \n\n     476,763.90 \n\n     28,486.9 % \n\n\n tutankham \n\n    11.43 \n\n     167.59 \n\n     - \n\n     273.00 \n\n     395.30 \n\n     491.48 \n\n     307.4 % \n\n\n up n down \n\n    533.40 \n\n     11,693.23 \n\n     3,350.30 \n\n     401,884.00 \n\n     589,226.90 \n\n     715,545.61 \n\n     6,407.0 % \n\n\n venture \n\n    0.00 \n\n     1,187.50 \n\n     - \n\n     1,813.00 \n\n     1,970.70 \n\n     0.40 \n\n     0.0 % \n\n\n video pinball \n\n    0.00 \n\n     17,667.90 \n\n     - \n\n     565,163.00 \n\n     999,383.20 \n\n     981,791.88 \n\n     5,556.9 % \n\n\n wizard of wor \n\n    563.50 \n\n     4,756.52 \n\n     - \n\n     46,204.00 \n\n     144,362.70 \n\n     197,126.00 \n\n     4,687.9 % \n\n\n yars revenge \n\n    3,092.91 \n\n     54,576.93 \n\n     5,664.30 \n\n     148,595.00 \n\n     995,048.40 \n\n     553,311.46 \n\n     1,068.7 % \n\n\n zaxxon \n\n    32.50 \n\n     9,173.30 \n\n     - \n\n     42,286.00 \n\n     224,910.70 \n\n     725,853.90 \n\n     7,940.5 % \n\n\n #  best \n\n     0 \n\n     5 \n\n     0 \n\n     5 \n\n     13 \n\n     37\n\n\n  \n\n\n <label. tab:atari-results-at30> Evaluation of MuZero in Atari for individual games with 30 random no-op starts. Best result for each game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames). SimPLe was only evaluated on 36 of the 57 games, unavailable results are indicated with `-'. Human normalized score is calculated as s_normalized = s_agent - s_random/s_human - s_random. \n\n\n\n\n\n \n  Game \n\n     Random \n\n     Human \n\n     Ape-X <cit. apex> \n\n     MuZero \n\n     MuZero normalized \n\n\n alien \n\n    128.30 \n\n     6,371.30 \n\n     17,732.00 \n\n     713,387.37 \n\n     11,424.9 % \n\n\n amidar \n\n    11.79 \n\n     1,540.43 \n\n     1,047.00 \n\n     26,638.80 \n\n     1,741.9 % \n\n\n assault \n\n    166.95 \n\n     628.89 \n\n     24,405.00 \n\n     143,900.58 \n\n     31,115.2 % \n\n\n asterix \n\n    164.50 \n\n     7,536.00 \n\n     283,180.00 \n\n     985,801.95 \n\n     13,370.9 % \n\n\n asteroids \n\n    877.10 \n\n     36,517.30 \n\n     117,303.00 \n\n     606,971.12 \n\n     1,700.6 % \n\n\n atlantis \n\n    13,463.00 \n\n     26,575.00 \n\n     918,715.00 \n\n     1,653,202.50 \n\n     12,505.6 % \n\n\n bank heist \n\n    21.70 \n\n     644.50 \n\n     1,201.00 \n\n     962.11 \n\n     151.0 % \n\n\n battle zone \n\n    3,560.00 \n\n     33,030.00 \n\n     92,275.00 \n\n     791,387.00 \n\n     2,673.3 % \n\n\n beam rider \n\n    254.56 \n\n     14,961.02 \n\n     72,234.00 \n\n     419,460.76 \n\n     2,850.5 % \n\n\n berzerk \n\n    196.10 \n\n     2,237.50 \n\n     55,599.00 \n\n     87,308.60 \n\n     4,267.3 % \n\n\n bowling \n\n    35.16 \n\n     146.46 \n\n     30.00 \n\n     194.03 \n\n     142.7 % \n\n\n boxing \n\n    -1.46 \n\n     9.61 \n\n     81.00 \n\n     56.60 \n\n     524.5 % \n\n\n breakout \n\n    1.77 \n\n     27.86 \n\n     757.00 \n\n     849.59 \n\n     3,249.6 % \n\n\n centipede \n\n    1,925.45 \n\n     10,321.89 \n\n     5,712.00 \n\n     1,138,294.60 \n\n     13,533.9 % \n\n\n chopper command \n\n    644.00 \n\n     8,930.00 \n\n     576,602.00 \n\n     932,370.10 \n\n     11,244.6 % \n\n\n crazy climber \n\n    9,337.00 \n\n     32,667.00 \n\n     263,954.00 \n\n     412,213.90 \n\n     1,726.9 % \n\n\n defender \n\n    1,965.50 \n\n     14,296.00 \n\n     399,865.00 \n\n     823,636.00 \n\n     6,663.7 % \n\n\n demon attack \n\n    208.25 \n\n     3,442.85 \n\n     133,002.00 \n\n     143,858.05 \n\n     4,441.0 % \n\n\n double dunk \n\n    -15.97 \n\n     -14.37 \n\n     22.00 \n\n     23.12 \n\n     2,443.1 % \n\n\n enduro \n\n    -81.84 \n\n     740.17 \n\n     2,042.00 \n\n     2,264.20 \n\n     285.4 % \n\n\n fishing derby \n\n    -77.11 \n\n     5.09 \n\n     22.00 \n\n     57.45 \n\n     163.7 % \n\n\n freeway \n\n    0.17 \n\n     25.61 \n\n     29.00 \n\n     28.38 \n\n     110.9 % \n\n\n frostbite \n\n    90.80 \n\n     4,202.80 \n\n     6,512.00 \n\n     613,944.04 \n\n     14,928.3 % \n\n\n gopher \n\n    250.00 \n\n     2,311.00 \n\n     121,168.00 \n\n     129,218.68 \n\n     6,257.6 % \n\n\n gravitar \n\n    245.50 \n\n     3,116.00 \n\n     662.00 \n\n     3,390.65 \n\n     109.6 % \n\n\n hero \n\n    1,580.30 \n\n     25,839.40 \n\n     26,345.00 \n\n     44,129.55 \n\n     175.4 % \n\n\n ice hockey \n\n    -9.67 \n\n     0.53 \n\n     24.00 \n\n     52.40 \n\n     608.5 % \n\n\n jamesbond \n\n    33.50 \n\n     368.50 \n\n     18,992.00 \n\n     39,107.20 \n\n     11,663.8 % \n\n\n kangaroo \n\n    100.00 \n\n     2,739.00 \n\n     578.00 \n\n     13,210.50 \n\n     496.8 % \n\n\n krull \n\n    1,151.90 \n\n     2,109.10 \n\n     8,592.00 \n\n     257,706.70 \n\n     26,802.6 % \n\n\n kung fu master \n\n    304.00 \n\n     20,786.80 \n\n     72,068.00 \n\n     174,623.60 \n\n     851.1 % \n\n\n montezuma revenge \n\n    25.00 \n\n     4,182.00 \n\n     1,079.00 \n\n     57.10 \n\n     0.8 % \n\n\n ms pacman \n\n    197.80 \n\n     15,375.05 \n\n     6,135.00 \n\n     230,650.24 \n\n     1,518.4 % \n\n\n name this game \n\n    1,747.80 \n\n     6,796.00 \n\n     23,830.00 \n\n     152,723.62 \n\n     2,990.7 % \n\n\n phoenix \n\n    1,134.40 \n\n     6,686.20 \n\n     188,789.00 \n\n     943,255.07 \n\n     16,969.6 % \n\n\n pitfall \n\n    -348.80 \n\n     5,998.91 \n\n     -273.00 \n\n     -801.10 \n\n     -7.1 % \n\n\n pong \n\n    -17.95 \n\n     15.46 \n\n     19.00 \n\n     19.20 \n\n     111.2 % \n\n\n private eye \n\n    662.78 \n\n     64,169.07 \n\n     865.00 \n\n     5,067.59 \n\n     6.9 % \n\n\n qbert \n\n    159.38 \n\n     12,085.00 \n\n     380,152.00 \n\n     39,302.10 \n\n     328.2 % \n\n\n riverraid \n\n    588.30 \n\n     14,382.20 \n\n     49,983.00 \n\n     315,353.33 \n\n     2,281.9 % \n\n\n road runner \n\n    200.00 \n\n     6,878.00 \n\n     127,112.00 \n\n     580,445.00 \n\n     8,688.9 % \n\n\n robotank \n\n    2.42 \n\n     8.94 \n\n     69.00 \n\n     128.80 \n\n     1,938.3 % \n\n\n seaquest \n\n    215.50 \n\n     40,425.80 \n\n     377,180.00 \n\n     997,601.01 \n\n     2,480.4 % \n\n\n skiing \n\n    -15,287.35 \n\n     -3,686.58 \n\n     -11,359.00 \n\n     -29,400.75 \n\n     -121.7 % \n\n\n solaris \n\n    2,047.20 \n\n     11,032.60 \n\n     3,116.00 \n\n     2,108.08 \n\n     0.7 % \n\n\n space invaders \n\n    182.55 \n\n     1,464.90 \n\n     50,699.00 \n\n     57,450.41 \n\n     4,465.9 % \n\n\n star gunner \n\n    697.00 \n\n     9,528.00 \n\n     432,958.00 \n\n     539,342.70 \n\n     6,099.5 % \n\n\n surround \n\n    -9.72 \n\n     5.37 \n\n     6.00 \n\n     8.46 \n\n     120.5 % \n\n\n tennis \n\n    -21.43 \n\n     -6.69 \n\n     23.00 \n\n     -2.30 \n\n     129.8 % \n\n\n time pilot \n\n    3,273.00 \n\n     5,650.00 \n\n     71,543.00 \n\n     405,829.30 \n\n     16,935.5 % \n\n\n tutankham \n\n    12.74 \n\n     138.30 \n\n     128.00 \n\n     351.76 \n\n     270.0 % \n\n\n up n down \n\n    707.20 \n\n     9,896.10 \n\n     347,912.00 \n\n     607,807.85 \n\n     6,606.9 % \n\n\n venture \n\n    18.00 \n\n     1,039.00 \n\n     936.00 \n\n     21.10 \n\n     0.3 % \n\n\n video pinball \n\n    0.00 \n\n     15,641.09 \n\n     873,989.00 \n\n     970,881.10 \n\n     6,207.2 % \n\n\n wizard of wor \n\n    804.00 \n\n     4,556.00 \n\n     46,897.00 \n\n     196,279.20 \n\n     5,209.9 % \n\n\n yars revenge \n\n    1,476.88 \n\n     47,135.17 \n\n     131,701.00 \n\n     888,633.84 \n\n     1,943.0 % \n\n\n zaxxon \n\n    475.00 \n\n     8,443.00 \n\n     37,672.00 \n\n     592,238.70 \n\n     7,426.8 % \n\n\n #  best \n\n     0 \n\n     6 \n\n     5 \n\n     46\n\n\n  \n\n\n <label. tab:atari-results-at30-rnd-starts> Evaluation of MuZero in Atari for individual games from human start positions. Best result for each game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames). \n\n\n\n<figure. fig:learning_curve_atari -  <label. fig:learning_curve_atari> Learning curves of MuZero in Atari for individual games. Total reward is shown on the y-axis, millions of training steps on the x-axis. Line indicates mean score across 1000 evaluation games, shaded region indicates standard deviation. > \n\n",
  "subsections": [
    {
      "title": "Introduction",
      "content": "Planning algorithms based on lookahead search have achieved remarkable successes in artificial intelligence. Human world champions have been defeated in classic games such as checkers <cit. schaeffer:chinook>, chess <cit. campbell:deep-blue>, Go <cit. Silver16AG> and poker <cit. brown2018superhuman, deepstack>, and planning algorithms have had real-world impact in applications from logistics <cit. vlahavas2013planning> to chemical synthesis <cit. alphachem>. However, these planning algorithms all rely on knowledge of the environment’s dynamics, such as the rules of the game or an accurate simulator, preventing their direct application to real-world domains like robotics, industrial control, or intelligent assistants. Model-based reinforcement learning (RL) <cit. sutton:book> aims to address this issue by first learning a model of the environment’s dynamics, and then planning with respect to the learned model. Typically, these models have either focused on reconstructing the true environmental state <cit. pilco:deisenroth,heess:stochastic_value_gradients,levine:learning_guided_policy>, or the sequence of full observations <cit. hafner:planet, kaiser:simple>. However, prior work <cit. state_space_models, hafner:planet, kaiser:simple> remains far from the state of the art in visually rich domains, such as Atari 2600 games <cit. ALE>. Instead, the most successful methods are based on model-free RL <cit. impala, r2d2, apex> – i.e. they estimate the optimal policy and/or value function directly from interactions with the environment. However, model-free algorithms are in turn far from the state of the art in domains that require precise and sophisticated lookahead, such as chess and Go. In this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art performance in Atari 2600, a visually complex set of domains, while maintaining superhuman performance in precision planning tasks such as chess, shogi and Go. MuZero builds upon AlphaZero’s <cit. Silver18AZ> powerful search and search-based policy iteration algorithms, but incorporates a learned model into the training procedure. MuZero also extends AlphaZero to a broader set of environments including single agent domains and non-zero rewards at intermediate time-steps. The main idea of the algorithm (summarized in Figure 1) is to predict those aspects of the future that are directly relevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an input and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the policy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points scored by playing a move). The model is trained end-to-end, with the sole objective of accurately estimating these three important quantities, so as to match the improved estimates of policy and value generated by search as well as the observed reward. There is no direct constraint or requirement for the hidden state to capture all information necessary to reconstruct the original observation, drastically reducing the amount of information the model has to maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the environment; nor any other constraints on the semantics of state. Instead, the hidden states are free to represent state in whatever way is relevant to predicting current and future values and policies. Intuitively, the agent can invent, internally, the rules or dynamics that lead to most accurate planning. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Prior Work",
      "content": "Reinforcement learning may be subdivided into two principal categories: model-based, and model-free <cit. sutton:book>. Model-based RL constructs, as an intermediate step, a model of the environment. Classically, this model is represented by a Markov-decision process (MDP) <cit. puterman:MDP> consisting of two components: a state transition model, predicting the next state, and a reward model, predicting the expected reward during that transition. The model is typically conditioned on the selected action, or a temporally abstract behavior such as an option <cit. sutton:between>. Once a model has been constructed, it is straightforward to apply MDP planning algorithms, such as value iteration <cit. puterman:MDP> or Monte-Carlo tree search (MCTS) <cit. coulom:mcts>, to compute the optimal value or optimal policy for the MDP. In large or partially observed environments, the algorithm must first construct the state representation that the model should predict. This tripartite separation between representation learning, model learning, and planning is potentially problematic since the agent is not able to optimize its representation or model for the purpose of effective planning, so that, for example modeling errors may compound during planning. A common approach to model-based RL focuses on directly modeling the observation stream at the pixel-level. It has been hypothesized that deep, stochastic models may mitigate the problems of compounding error <cit. hafner:planet, kaiser:simple>. However, planning at pixel-level granularity is not computationally tractable in large scale problems. Other methods build a latent state-space model that is sufficient to reconstruct the observation stream at pixel level <cit. wahlstrom:pixels_to_torques,watter:embed_to_control>, or to predict its future latent states <cit. ha:world_model,gelada:deepmdp>, which facilitates more efficient planning but still focuses the majority of the model capacity on potentially irrelevant detail. None of these prior methods has constructed a model that facilitates effective planning in visually complex domains such as Atari; results lag behind well-tuned, model-free methods, even in terms of data efficiency <cit. hado:replay>. A quite different approach to model-based RL has recently been developed, focused end-to-end on predicting the value function <cit. silver:predictron>. The main idea of these methods is to construct an abstract MDP model such that planning in the abstract MDP is equivalent to planning in the real environment. This equivalence is achieved by ensuring value equivalence, i.e. that, starting from the same real state, the cumulative reward of a trajectory through the abstract MDP matches the cumulative reward of a trajectory in the real environment. The predictron <cit. silver:predictron> first introduced value equivalent models for predicting value (without actions). Although the underlying model still takes the form of an MDP, there is no requirement for its transition model to match real states in the environment. Instead the MDP model is viewed as a hidden layer of a deep neural network. The unrolled MDP is trained such that the expected cumulative sum of rewards matches the expected value with respect to the real environment, e.g. by temporal-difference learning. Value equivalent models were subsequently extended to optimising value (with actions). TreeQN <cit. farquhar:treeqn> learns an abstract MDP model, such that a tree search over that model (represented by a tree-structured neural network) approximates the optimal value function. Value iteration networks <cit. aviv:vin> learn a local MDP model, such that value iteration over that model (represented by a convolutional neural network) approximates the optimal value function. Value prediction networks <cit. oh:vpn> are perhaps the closest precursor to MuZero: they learn an MDP model grounded in real actions; the unrolled MDP is trained such that the cumulative sum of rewards, conditioned on the actual sequence of actions generated by a simple lookahead search, matches the real environment. Unlike MuZero there is no policy prediction, and the search only utilizes value prediction. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "MuZero Algorithm",
      "content": "<figure. fig:recurrent_search -  <label. fig:recurrent_search> Planning, acting, and training with a learned model. (A) How MuZero uses its model to plan. The model consists of three connected components for representation, dynamics and prediction. Given a previous hidden state s^k-1 and a candidate action a^k, the dynamics function g produces an immediate reward r^k and a new hidden state s^k. The policy p^k and value function v^k are computed from the hidden state s^k by a prediction function f. The initial hidden state s^0 is obtained by passing the past observations (e.g. the Go board or Atari screen) into a representation function h. (B) How MuZero acts in the environment. A Monte-Carlo Tree Search is performed at each timestep t, as described in A. An action a_t+1 is sampled from the search policy π _t, which is proportional to the visit count for each action from the root node. The environment receives the action and generates a new observation o_t+1 and reward u_t+1. At the end of the episode the trajectory data is stored into a replay buffer. (C) How MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation function h receives as input the past observations o_1, ..., o_t from the selected trajectory. The model is subsequently unrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state s^k-1 from the previous step and the real action a_t+k. The parameters of the representation, dynamics and prediction functions are jointly trained, end-to-end by backpropagation-through-time, to predict three quantities: the policy 𝐩^k ≈π _t+k, value function v^k ≈ z_t+k, and reward r_t+k≈ u_t+k, where z_t+k is a sample return: either the final reward (board games) or n-step return (Atari). >We now describe the MuZero algorithm in more detail. Predictions are made at each time-step t, for each of k = 1...K steps, by a model μ _θ, with parameters θ, conditioned on past observations o_1, ..., o_t and future actions a_t+1, ..., a_t+k. The model predicts three future quantities: the policy 𝐩^k_t ≈π (a_t+k+1 | o_1, ..., o_t, a_t+1, ..., a_t+k), the value function v^k_t ≈𝔼[ u_t+k+1 + γ u_t+k+2 + ... | o_1, ..., o_t, a_t+1, ..., a_t+k], and the immediate reward r^k_t ≈ u_t+k, where u_. is the true, observed reward, π is the policy used to select real actions, and γ is the discount function of the environment. Internally, at each time-step t (subscripts _t suppressed for simplicity), the model is represented by the combination of a representation function, a dynamics function, and a prediction function. The dynamics function, r^k, s^k = g_θ (s^k-1, a^k), is a recurrent process that computes, at each hypothetical step k, an immediate reward r^k and an internal state s^k. It mirrors the structure of an MDP model that computes the expected reward and state transition for a given state and action <cit. puterman:MDP>. However, unlike traditional approaches to model-based RL <cit. sutton:book>, this internal state s^k has no semantics of environment state attached to it – it is simply the hidden state of the overall model, and its sole purpose is to accurately predict relevant, future quantities: policies, values, and rewards. In this paper, the dynamics function is represented deterministically; the extension to stochastic transitions is left for future work. The policy and value functions are computed from the internal state s^k by the prediction function, 𝐩^k, v^k = f_θ (s^k), akin to the joint policy and value network of AlphaZero. The “root\" state s^0 is initialized using a representation function that encodes past observations, s^0 = h_θ (o_1, ..., o_t); again this has no special semantics beyond its support for future predictions. Given such a model, it is possible to search over hypothetical future trajectories a^1, ..., a^k given past observations o_1, ..., o_t. For example, a naive search could simply select the k step action sequence that maximizes the value function. More generally, we may apply any MDP planning algorithm to the internal rewards and state space induced by the dynamics function. Specifically, we use an MCTS algorithm similar to AlphaZero’s search, generalized to allow for single agent domains and intermediate rewards (see Methods). At each internal node, it makes use of the policy, value and reward estimates produced by the current model parameters θ. The MCTS algorithm outputs a recommended policy π _t and estimated value ν _t. An action a_t+1∼π _t is then selected. All parameters of the model are trained jointly to accurately match the policy, value, and reward, for every hypothetical step k, to corresponding target values observed after k actual time-steps have elapsed. Similarly to AlphaZero, the improved policy targets are generated by an MCTS search; the first objective is to minimise the error between predicted policy 𝐩_t^k and search policy π _t+k. Also like AlphaZero, the improved value targets are generated by playing the game or MDP. However, unlike AlphaZero, we allow for long episodes with discounting and intermediate rewards by bootstrapping n steps into the future from the search value, z_t = u_t+1 + γ u_t+2 + ... + γ ^n-1 u_t+n + γ ^n ν _t+n. Final outcomes { lose, draw, win} in board games are treated as rewards u_t ∈{  -1, 0, +1 } occuring at the final step of the episode. Specifically, the second objective is to minimize the error between the predicted value v^k_t and the value target, z_t+k For chess, Go and shogi, the same squared error loss as AlphaZero is used for rewards and values. A cross-entropy loss was found to be more stable than a squared error when encountering rewards and values of variable scale in Atari. Cross-entropy was used for the policy loss in both cases.. The reward targets are simply the observed rewards; the third objective is therefore to minimize the error between the predicted reward r^k_t and the observed reward u_t+k. Finally, an L2 regularization term is also added, leading to the overall loss: \n    l_t(θ )     = ∑ _k=0^K l^r (u_t+k, r_t^k) + l^v(z_t+k, v^k_t) + l^p(π _t+k, 𝐩^k_t) + c ||θ ||^2 <label. muzero_eqn>\n where l^r, l^v, and l^p are loss functions for reward, value and policy respectively. Supplementary Figure <ref. fig:muzero_equations> summarizes the equations governing how the MuZero algorithm plans, acts, and learns. ",
      "subsections": [],
      "figures": {
        "fig:recurrent_search": {
          "label": "fig:recurrent_search",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/images/learned_model_search_play_train.png"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": null,
              "width": 607
            }
          ],
          "caption": " <label. fig:recurrent_search> Planning, acting, and training with a learned model. (A) How MuZero uses its model to plan. The model consists of three connected components for representation, dynamics and prediction. Given a previous hidden state s^k-1 and a candidate action a^k, the dynamics function g produces an immediate reward r^k and a new hidden state s^k. The policy p^k and value function v^k are computed from the hidden state s^k by a prediction function f. The initial hidden state s^0 is obtained by passing the past observations (e.g. the Go board or Atari screen) into a representation function h. (B) How MuZero acts in the environment. A Monte-Carlo Tree Search is performed at each timestep t, as described in A. An action a_t+1 is sampled from the search policy π _t, which is proportional to the visit count for each action from the root node. The environment receives the action and generates a new observation o_t+1 and reward u_t+1. At the end of the episode the trajectory data is stored into a replay buffer. (C) How MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation function h receives as input the past observations o_1, ..., o_t from the selected trajectory. The model is subsequently unrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state s^k-1 from the previous step and the real action a_t+k. The parameters of the representation, dynamics and prediction functions are jointly trained, end-to-end by backpropagation-through-time, to predict three quantities: the policy 𝐩^k ≈π _t+k, value function v^k ≈ z_t+k, and reward r_t+k≈ u_t+k, where z_t+k is a sample return: either the final reward (board games) or n-step return (Atari). ",
          "section": "3. MuZero Algorithm"
        }
      }
    },
    {
      "title": "Results",
      "content": " Chess \n\n     Shogi \n\n     Go \n\n     Atari \n\n\n 0.62 \n\n     0.745 UTF8min  (e,0.5cm,0.5cm)|c|c|c|c|c|c|c|c|c||c|c|c|c|c|c|c|c|c| [origin=c]180香      [origin=c]180桂 \n\n [origin=c]180銀 \n\n [origin=c]180金 \n\n [origin=c]180玉 \n\n [origin=c]180金 \n\n [origin=c]180銀 \n\n [origin=c]180桂 \n\n [origin=c]180香\n\n\n       [origin=c]180飛 \n\n                         [origin=c]180角 \n\n    \n  [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩 \n\n     [origin=c]180歩\n\n\n                                  \n                                  \n                                  \n  歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩 \n\n     歩\n\n\n       角 \n\n                         飛 \n\n    \n  香 \n\n     桂 \n\n     銀 \n\n     金 \n\n     玉 \n\n     金 \n\n     銀 \n\n     桂 \n\n     香\n\n\n  \n\n\n\n       0.195 \n[help lines] (0,0) grid (18,18);\n[fill=gray] (3,3) circle[radius=0.1];\n[fill=gray] (3,9) circle[radius=0.1];\n[fill=gray] (3,15) circle[radius=0.1];\n[fill=gray] (9,3) circle[radius=0.1];\n[fill=gray] (9,9) circle[radius=0.1];\n[fill=gray] (9,15) circle[radius=0.1];\n[fill=gray] (15,3) circle[radius=0.1];\n[fill=gray] (15,9) circle[radius=0.1];\n[fill=gray] (15,15) circle[radius=0.1];\n       \n    < g r a p h i c s >\n \n 4c  \n    < g r a p h i c s >\n  \n  <label. fig:results> Evaluation of MuZero throughout training in chess, shogi, Go and Atari. The x-axis shows millions of training steps. For chess, shogi and Go, the y-axis shows Elo rating, established by playing games against AlphaZero using 800 simulations per move for both players. MuZero’s Elo is indicated by the blue line, AlphaZero’s Elo by the horizontal orange line. For Atari, mean (full line) and median (dashed line) human normalized scores across all 57 games are shown on the y-axis. The scores for R2D2 <cit. r2d2>, (the previous state of the art in this domain, based on model-free RL) are indicated by the horizontal orange lines. Performance in Atari was evaluated using 50 simulations every fourth time-step, and then repeating the chosen action four times, as in prior work <cit. dqn>.  We applied the MuZero algorithm to the classic board games Go, chess and shogi  Imperfect information games such as Poker are not directly addressed by our method., as benchmarks for challenging planning problems, and to all 57 games in the Atari Learning Environment <cit. ALE>, as benchmarks for visually complex RL domains. In each case we trained MuZero for K=5 hypothetical steps. Training proceeded for 1 million mini-batches of size 2048 in board games and of size 1024 in Atari. During both training and evaluation, MuZero used 800 simulations for each search in board games, and 50 simulations for each search in Atari. The representation function uses the same convolutional <cit. convnet> and residual <cit. he:resnet> architecture as AlphaZero, but with 16 residual blocks instead of 20. The dynamics function uses the same architecture as the representation function and the prediction function uses the same architecture as AlphaZero. All networks use 256 hidden planes (see Methods for further details). Figure <ref. fig:results> shows the performance throughout training in each game. In Go, MuZero slightly exceeded the performance of AlphaZero, despite using less computation per node in the search tree (16 residual blocks per evaluation in MuZero compared to 20 blocks in AlphaZero). This suggests that MuZero may be caching its computation in the search tree and using each additional application of the dynamics model to gain a deeper understanding of the position. In Atari, MuZero achieved a new state of the art for both mean and median normalized score across the 57 games of the Arcade Learning Environment, outperforming the previous state-of-the-art method R2D2 <cit. r2d2> (a model-free approach) in 42 out of 57 games, and outperforming the previous best model-based approach SimPLe <cit. kaiser:simple> in all games (see Table <ref. tab:atari-results-at30>). We also evaluated a second version of MuZero that was optimised for greater sample efficiency. Specifically, it reanalyzes old trajectories by re-running the MCTS using the latest network parameters to provide fresh targets (see Appendix <ref. reanalyze>). When applied to 57 Atari games, using 200 million frames of experience per game, MuZero Reanalyze achieved 731%  median normalized score, compared to 192% , 231%  and 431%  for previous state-of-the-art model-free approaches IMPALA <cit. impala>, Rainbow <cit. rainbow> and LASER <cit. laser> respectively. \n\n Agent \n\n     Median \n\n     Mean \n\n     Env. Frames \n\n     Training Time \n\n     Training Steps \n\n\n Ape-X <cit. apex> \n\n     434.1%  \n\n     1695.6%  \n\n     22.8B \n\n     5 days \n\n     8.64M\n\n\n R2D2 <cit. r2d2> \n\n     1920.6%  \n\n     4024.9%  \n\n     37.5B \n\n     5 days \n\n     2.16M \n\n\n MuZero \n\n     2041.1%   \n\n     4999.2%   \n\n     20.0B \n\n     12 hours \n\n     1M \n\n\n IMPALA <cit. impala> \n\n     191.8%  \n\n     957.6%  \n\n     200M \n\n     – \n\n     – \n\n\n Rainbow <cit. rainbow> \n\n     231.1%  \n\n     – \n\n     200M \n\n     10 days \n\n     – \n\n\n UNREALa <cit. unreal> \n\n     250% a \n\n     880% a \n\n     250M \n\n     – \n\n     – \n\n\n LASER <cit. laser> \n\n     431%  \n\n     – \n\n     200M \n\n     – \n\n     – \n\n\n MuZero Reanalyze \n\n     731.1%  \n\n     2168.9%  \n\n     200M \n\n     12 hours \n\n     1M \n\n\n \n\n <label. tab:atari-comparison> Comparison of MuZero against previous agents in Atari. We compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, compared to human testers. The best results are highlighted in bold. MuZero sets a new state of the art in both settings. aHyper-parameters were tuned per game. \n\n\n\nTo understand the role of the model in MuZero we also ran several experiments, focusing on the board game of Go and the Atari game of Ms.  Pacman. First, we tested the scalability of planning (Figure <ref. fig:ablations>A), in the canonical planning problem of Go. We compared the performance of search in AlphaZero, using a perfect model, to the performance of search in MuZero, using a learned model. Specifically, the fully trained AlphaZero or MuZero was evaluated by comparing MCTS with different thinking times. MuZero matched the performance of a perfect model, even when doing much larger searches (up to 10s thinking time) than those from which the model was trained (around 0.1s thinking time, see also Figure <ref. fig:extended_ablations>A). We also investigated the scalability of planning across all Atari games (see Figure <ref. fig:ablations>B). We compared MCTS with different numbers of simulations, using the fully trained MuZero. The improvements due to planning are much less marked than in Go, perhaps because of greater model inaccuracy; performance improved slightly with search time, but plateaued at around 100 simulations. Even with a single simulation – i.e. when selecting moves solely according to the policy network – MuZero performed well, suggesting that, by the end of training, the raw policy has learned to internalise the benefits of search (see also Figure <ref. fig:extended_ablations>B). Next, we tested our model-based learning algorithm against a comparable model-free learning algorithm (see Figure <ref. fig:ablations>C). We replaced the training objective of MuZero (Equation 1) with a model-free Q-learning objective (as used by R2D2), and the dual value and policy heads with a single head representing the Q-function Q(· |s_t). Subsequently, we trained and evaluated the new model without using any search. When evaluated on Ms.  Pacman, our model-free algorithm achieved identical results to R2D2, but learned significantly slower than MuZero and converged to a much lower final score. We conjecture that the search-based policy improvement step of MuZero provides a stronger learning signal than the high bias, high variance targets used by Q-learning. To better understand the nature of MuZero’s learning algorithm, we measured how MuZero’s training scales with respect to the amount of search it uses during training. Figure <ref. fig:ablations>D shows the performance in Ms.  Pacman, using an MCTS of different simulation counts per move throughout training. Surprisingly, and in contrast to previous work <cit. negative_mcts_results>, even with only 6 simulations per move – fewer than the number of actions – MuZero learned an effective policy and improved rapidly. With more simulations performance jumped significantly higher. For analysis of the policy improvement during each individual iteration, see also Figure <ref. fig:extended_ablations> C and D. <figure. fig:ablations -  <label. fig:ablations> Evaluations of MuZero on Go (A), all 57 Atari Games (B) and Ms.  Pacman (C-D). (A) Scaling with search time per move in Go, comparing the learned model with the ground truth simulator. Both networks were trained at 800 simulations per search, equivalent to 0.1 seconds per search. Remarkably, the learned model is able to scale well to up to two orders of magnitude longer searches than seen during training. (B) Scaling of final human normalized mean score in Atari with the number of simulations per search. The network was trained at 50 simulations per search. Dark line indicates mean score, shaded regions indicate 25th to 75th and 5th to 95th percentiles. The learned model's performance increases up to 100 simulations per search. Beyond, even when scaling to much longer searches than during training, the learned model's performance remains stable and only decreases slightly. This contrasts with the much better scaling in Go (A), presumably due to greater model inaccuracy in Atari than Go. (C) Comparison of MCTS based training with Q-learning in the MuZero framework on Ms.  Pacman, keeping network size and amount of training constant. The state of the art Q-Learning algorithm R2D2 is shown as a baseline. Our Q-Learning implementation reaches the same final score as R2D2, but improves slower and results in much lower final performance compared to MCTS based training. (D) Different networks trained at different numbers of simulations per move, but all evaluated at 50 simulations per move. Networks trained with more simulations per move improve faster, consistent with ablation (B), where the policy improvement is larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training with less simulations per move than are enough to cover all 8 possible actions in Ms.  Pacman. >",
      "subsections": [],
      "figures": {
        "fig:ablations": {
          "label": "fig:ablations",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/go_scaling_elos.png",
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/atari_scaling_score.png",
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/pacman_ablations_main.png"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": null,
              "width": 303
            },
            {
              "scale": 1.0,
              "height": null,
              "width": 303
            },
            {
              "scale": 1.0,
              "height": null,
              "width": 607
            }
          ],
          "caption": " <label. fig:ablations> Evaluations of MuZero on Go (A), all 57 Atari Games (B) and Ms.  Pacman (C-D). (A) Scaling with search time per move in Go, comparing the learned model with the ground truth simulator. Both networks were trained at 800 simulations per search, equivalent to 0.1 seconds per search. Remarkably, the learned model is able to scale well to up to two orders of magnitude longer searches than seen during training. (B) Scaling of final human normalized mean score in Atari with the number of simulations per search. The network was trained at 50 simulations per search. Dark line indicates mean score, shaded regions indicate 25th to 75th and 5th to 95th percentiles. The learned model's performance increases up to 100 simulations per search. Beyond, even when scaling to much longer searches than during training, the learned model's performance remains stable and only decreases slightly. This contrasts with the much better scaling in Go (A), presumably due to greater model inaccuracy in Atari than Go. (C) Comparison of MCTS based training with Q-learning in the MuZero framework on Ms.  Pacman, keeping network size and amount of training constant. The state of the art Q-Learning algorithm R2D2 is shown as a baseline. Our Q-Learning implementation reaches the same final score as R2D2, but improves slower and results in much lower final performance compared to MCTS based training. (D) Different networks trained at different numbers of simulations per move, but all evaluated at 50 simulations per move. Networks trained with more simulations per move improve faster, consistent with ablation (B), where the policy improvement is larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training with less simulations per move than are enough to cover all 8 possible actions in Ms.  Pacman. ",
          "section": "4. Results"
        }
      }
    },
    {
      "title": "Conclusions",
      "content": "Many of the breakthroughs in artificial intelligence have been based on either high-performance planning <cit. campbell:deep-blue, Silver16AG, Silver18AZ> or model-free reinforcement learning methods <cit. dqn, openai_dota, alphastar>. In this paper we have introduced a method that combines the benefits of both approaches. Our algorithm, MuZero, has both matched the superhuman performance of high-performance planning algorithms in their favored domains – logically complex board games such as chess and Go – and outperformed state-of-the-art model-free RL algorithms in their favored domains – visually complex Atari games. Crucially, our method does not require any knowledge of the game rules or environment dynamics, potentially paving the way towards the application of powerful learning and planning methods to a host of real-world domains for which there exists no perfect simulator. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Acknowledgments",
      "content": "Lorrayne Bennett, Oliver Smith and Chris Apps for organizational assistance; Koray Kavukcuoglu for reviewing the paper; Thomas Anthony, Matthew Lai, Nenad Tomasev, Ulrich Paquet, Sumedh Ghaisas for many fruitful discussions; and the rest of the DeepMind team for their support. plain ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Supplementary Materials",
      "content": "\n - Pseudocode description of the MuZero algorithm. \n\n- Data for Figures <ref. fig:results>, <ref. fig:ablations>, <ref. fig:muzero_equations>, <ref. fig:extended_ablations>, <ref. fig:learning_curve_atari> and Tables <ref. tab:atari-comparison>, <ref. tab:atari-results-at30>, <ref. tab:atari-results-at30-rnd-starts> in JSON format. \n\nSupplementary materials can be accessed from the ancillary file section of the arXiv submission.   <label. methods> ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Comparison to AlphaZero",
      "content": "MuZero is designed for a more general setting than AlphaGo Zero <cit. Silver17AG0> and AlphaZero <cit. Silver18AZ>. In AlphaGo Zero and AlphaZero the planning process makes use of two separate components: a simulator implements the rules of the game, which are used to update the state of the game while traversing the search tree; and a neural network jointly predicts the corresponding policy and value of a board position produced by the simulator (see Figure <ref. fig:recurrent_search> A). Specifically, AlphaGo Zero and AlphaZero use knowledge of the rules of the game in three places: (1) state transitions in the search tree, (2) actions available at each node of the search tree, (3) episode termination within the search tree. In MuZero, all of these have been replaced with the use of a single implicit model learned by a neural network (see Figure <ref. fig:recurrent_search> B): \n- State transitions. AlphaZero had access to a perfect simulator of the true dynamics process. In contrast, MuZero employs a learned dynamics model within its search. Under this model, each node in the tree is represented by a corresponding hidden state; by providing a hidden state s_k-1 and an action a_k to the model the search algorithm can transition to a new node s_k = g(s_k-1, a_k). \n\n- Actions available. AlphaZero used the set of legal actions obtained from the simulator to mask the prior produced by the network everywhere in the search tree. MuZero only masks legal actions at the root of the search tree where the environment can be queried, but does not perform any masking within the search tree. This is possible because the network rapidly learns not to predict actions that never occur in the trajectories it is trained on. \n\n- Terminal nodes. AlphaZero stopped the search at tree nodes representing terminal states and used the terminal value provided by the simulator instead of the value produced by the network. MuZero does not give special treatment to terminal nodes and always uses the value predicted by the network. Inside the tree, the search can proceed past a terminal node - in this case the network is expected to always predict the same value. This is achieved by treating terminal states as absorbing states during training. \n\nIn addition, MuZero is designed to operate in the general reinforcement learning setting: single-agent domains with discounted intermediate rewards of arbitrary magnitude. In contrast, AlphaGo Zero and AlphaZero were designed to operate in two-player games with undiscounted terminal rewards of ± 1. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Search",
      "content": "We now describe the search algorithm used by MuZero. Our approach is based upon Monte-Carlo tree search with upper confidence bounds, an approach to planning that converges asymptotically to the optimal policy in single agent domains and to the minimax value function in zero sum games <cit. kocsis-sepesvari>. Every node of the search tree is associated with an internal state s. For each action a from s there is an edge (s,a) that stores a set of statistics { N(s,a), Q(s,a), P(s,a), R(s,a), S(s,a) }, respectively representing visit counts N, mean value Q, policy P, reward R, and state transition S. Similar to AlphaZero, the search is divided into three stages, repeated for a number of simulations. Selection: Each simulation starts from the internal root state s^0, and finishes when the simulation reaches a leaf node s^l. For each hypothetical time-step k = 1 ... l of the simulation, an action a^k is selected according to the stored statistics for internal state s^k-1, by maximizing over an upper confidence bound <cit. rosin:puct><cit. Silver18AZ>, \n    <label. pUCT> a^k = max _a[ Q(s, a) + P(s, a) ·√(∑ _b N(s, b))/1 + N(s, a)(c_1 + log(∑ _b N(s, b) + c_2 + 1/c_2)) ]\nThe constants c_1 and c_2 are used to control the influence of the prior P(s, a) relative to the value Q(s, a) as nodes are visited more often. In our experiments, c_1 = 1.25 and c_2 = 19652. For k<l, the next state and reward are looked up in the state transition and reward table s^k = S(s^k-1, a^k), r^k = R(s^k-1, a^k). Expansion: At the final time-step l of the simulation, the reward and state are computed by the dynamics function, r^l, s^l = g_θ (s^l-1, a^l), and stored in the corresponding tables, R(s^l-1, a^l) = r^l, S(s^l-1, a^l) = s^l. The policy and value are computed by the prediction function, 𝐩^l, v^l = f_θ (s^l). A new node, corresponding to state s^l is added to the search tree. Each edge (s^l, a) from the newly expanded node is initialized to {  N(s^l, a)=0, Q(s^l, a)=0, P(s^l, a)=𝐩^l }. Note that the search algorithm makes at most one call to the dynamics function and prediction function respectively per simulation; the computational cost is of the same order as in AlphaZero. Backup: At the end of the simulation, the statistics along the trajectory are updated. The backup is generalized to the case where the environment can emit intermediate rewards, have a discount γ different from 1, and the value estimates are unbounded In board games the discount is assumed to be 1 and there are no intermediate rewards.. For k= l ... 0, we form an l-k-step estimate of the cumulative discounted reward, bootstrapping from the value function v^l, \n    G^k = ∑ _τ =0^l-1-kγ ^τ r_k+ 1 +τ + γ ^l - k v^l\nFor k= l ... 1, we update the statistics for each edge (s^k-1, a^k) in the simulation path as follows, \n    Q(s^k-1,a^k)      := N(s^k-1,a^k) · Q(s^k-1,a^k) + G^k/N(s^k-1,a^k) + 1\n     N(s^k-1,a^k)      := N(s^k-1,a^k) + 1\nIn two-player zero sum games the value functions are assumed to be bounded within the [0, 1] interval. This choice allows us to combine value estimates with probabilities using the pUCT rule (Eqn <ref. pUCT>). However, since in many environments the value is unbounded, it is necessary to adjust the pUCT rule. A simple solution would be to use the maximum score that can be observed in the environment to either re-scale the value or set the pUCT constants appropriately <cit. MCTS_single_agent>. However, both solutions are game specific and require adding prior knowledge to the MuZero algorithm. To avoid this, MuZero computes normalized Q value estimates Q∈ [0, 1] by using the minimum-maximum values observed in the search tree up to that point. When a node is reached during the selection stage, the algorithm computes the normalized Q values of its edges to be used in the pUCT rule using the equation below: \n    Q(s^k-1, a^k) = Q(s^k-1, a^k) - min _s, a ∈ TreeQ(s, a)/max _s,a ∈ Tree Q(s, a) - min _s,a ∈ Tree Q(s, a)\n",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Hyperparameters",
      "content": "For simplicity we preferentially use the same architectural choices and hyperparameters as in previous work. Specifically, we started with the network architecture and search choices of AlphaZero <cit. Silver18AZ>. For board games, we use the same UCB constants, dirichlet exploration noise and the same 800 simulations per search as in AlphaZero. Due to the much smaller branching factor and simpler policies in Atari, we only used 50 simulations per search to speed up experiments. As shown in Figure <ref. fig:ablations>B, the algorithm is not very sensitive to this choice. We also use the same discount (0.997) and value transformation (see Network Architecture section) as R2D2 <cit. r2d2>. For parameter values not mentioned in the text, please refer to the pseudocode. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Data Generation",
      "content": "To generate training data, the latest checkpoint of the network (updated every 1000 training steps) is used to play games with MCTS. In the board games Go, chess and shogi the search is run for 800 simulations per move to pick an action; in Atari due to the much smaller action space 50 simulations per move are sufficient. For board games, games are sent to the training job as soon as they finish. Due to the much larger length of Atari games (up to 30 minutes or 108,000 frames), intermediate sequences are sent every 200 moves. In board games, the training job keeps an in-memory replay buffer of the most recent 1 million games received; in Atari, where the visual observations are larger, the most recent 125 thousand sequences of length 200 are kept. During the generation of experience in the board game domains, the same exploration scheme as the one described in AlphaZero <cit. Silver18AZ> is used. Using a variation of this scheme, in the Atari domain actions are sampled from the visit count distribution throughout the duration of each game, instead of just the first k moves. Moreover, the visit count distribution is parametrized using a temperature parameter T: \n    p_α = N(α )^1 / T/∑ _b N(b)^1 / T\nT is decayed as a function of the number of training steps of the network. Specifically, for the first 500k training steps a temperature of 1 is used, for the next 250k steps a temperature of 0.5 and for the remaining 250k a temperature of 0.25. This ensures that the action selection becomes greedier as training progresses. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Network Input",
      "content": "\n\nRepresentation Function\n-----------------------\nThe history over board states used as input to the representation function for Go, chess and shogi is represented similarly to AlphaZero <cit. Silver18AZ>. In Go and shogi we encode the last 8 board states as in AlphaZero; in chess we increased the history to the last 100 board states to allow correct prediction of draws. For Atari, the input of the representation function includes the last 32 RGB frames at resolution 96x96 along with the last 32 actions that led to each of those frames. We encode the historical actions because unlike board games, an action in Atari does not necessarily have a visible effect on the observation. RGB frames are encoded as one plane per color, rescaled to the range [0, 1], for red, green and blue respectively. We perform no other normalization, whitening or other preprocessing of the RGB input. Historical actions are encoded as simple bias planes, scaled as a / 18 (there are 18 total actions in Atari). \n\nDynamics Function\n-----------------\nThe input to the dynamics function is the hidden state produced by the representation function or previous application of the dynamics function, concatenated with a representation of the action for the transition. Actions are encoded spatially in planes of the same resolution as the hidden state. In Atari, this resolution is 6x6 (see description of downsampling in Network Architecture section), in board games this is the same as the board size (19x19 for Go, 8x8 for chess, 9x9 for shogi). In Go, a normal action (playing a stone on the board) is encoded as an all zero plane, with a single one in the position of the played stone. A pass is encoded as an all zero plane. In chess, 8 planes are used to encode the action. The first one-hot plane encodes which position the piece was moved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the target position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or not. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions, not all of which are legal, and we use the same action space for policy prediction and to encode the dynamics function input. The remaining five binary planes are used to indicate the type of promotion, if any (queen, knight, bishop, rook, none). The encoding for shogi is similar, with a total of 11 planes. We use the first 8 planes to indicate where the piece moved from - either a board position (first one-hot plane) or the drop of one of the seven types of prisoner (remaining 7 binary planes). The next two planes are used to encode the target as in chess. The remaining binary plane indicates whether the move was a promotion or not. In Atari, an action is encoded as a one hot vector which is tiled appropriately into planes. ",
      "subsections": [
        {
          "title": "Representation Function",
          "content": "The history over board states used as input to the representation function for Go, chess and shogi is represented similarly to AlphaZero <cit. Silver18AZ>. In Go and shogi we encode the last 8 board states as in AlphaZero; in chess we increased the history to the last 100 board states to allow correct prediction of draws. For Atari, the input of the representation function includes the last 32 RGB frames at resolution 96x96 along with the last 32 actions that led to each of those frames. We encode the historical actions because unlike board games, an action in Atari does not necessarily have a visible effect on the observation. RGB frames are encoded as one plane per color, rescaled to the range [0, 1], for red, green and blue respectively. We perform no other normalization, whitening or other preprocessing of the RGB input. Historical actions are encoded as simple bias planes, scaled as a / 18 (there are 18 total actions in Atari). ",
          "subsections": [],
          "figures": {}
        },
        {
          "title": "Dynamics Function",
          "content": "The input to the dynamics function is the hidden state produced by the representation function or previous application of the dynamics function, concatenated with a representation of the action for the transition. Actions are encoded spatially in planes of the same resolution as the hidden state. In Atari, this resolution is 6x6 (see description of downsampling in Network Architecture section), in board games this is the same as the board size (19x19 for Go, 8x8 for chess, 9x9 for shogi). In Go, a normal action (playing a stone on the board) is encoded as an all zero plane, with a single one in the position of the played stone. A pass is encoded as an all zero plane. In chess, 8 planes are used to encode the action. The first one-hot plane encodes which position the piece was moved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the target position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or not. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions, not all of which are legal, and we use the same action space for policy prediction and to encode the dynamics function input. The remaining five binary planes are used to indicate the type of promotion, if any (queen, knight, bishop, rook, none). The encoding for shogi is similar, with a total of 11 planes. We use the first 8 planes to indicate where the piece moved from - either a board position (first one-hot plane) or the drop of one of the seven types of prisoner (remaining 7 binary planes). The next two planes are used to encode the target as in chess. The remaining binary plane indicates whether the move was a promotion or not. In Atari, an action is encoded as a one hot vector which is tiled appropriately into planes. ",
          "subsections": [],
          "figures": {}
        }
      ],
      "figures": {}
    },
    {
      "title": "Network Architecture",
      "content": "<label. sec:network_architecture> The prediction function 𝐩^k, v^k = f_θ (s^k) uses the same architecture as AlphaZero: one or two convolutional layers that preserve the resolution but reduce the number of planes, followed by a fully connected layer to the size of the output. For value and reward prediction in Atari we follow <cit. pohlen2018observe> in scaling targets using an invertible transform h(x) = sign(x)(√(|x| + 1) - 1 + ϵ x), where ϵ = 0.001 in all our experiments. We then apply a transformation ϕ to the scalar reward and value targets in order to obtain equivalent categorical representations. We use a discrete support set of size 601 with one support for every integer between -300 and 300. Under this transformation, each scalar is represented as the linear combination of its two adjacent supports, such that the original value can be recovered by x = x_low * p_low + x_high * p_high. As an example, a target of 3.7 would be represented as a weight of 0.3 on the support for 3 and a weight of 0.7 on the support for 4. The value and reward outputs of the network are also modeled using a softmax output of size 601. During inference the actual value and rewards are obtained by first computing their expected value under their respective softmax distribution and subsequently by inverting the scaling transformation. Scaling and transformation of the value and reward happens transparently on the network side and is not visible to the rest of the algorithm. Both the representation and dynamics function use the same architecture as AlphaZero, but with 16 instead of 20 residual blocks <cit. he:resnet>. We use 3x3 kernels and 256 hidden planes for each convolution. For Atari, where observations have large spatial resolution, the representation function starts with a sequence of convolutions with stride 2 to reduce the spatial resolution. Specifically, starting with an input observation of resolution 96x96 and 128 planes (32 history frames of 3 color channels each, concatenated with the corresponding 32 actions broadcast to planes), we downsample as follows: \n - 1 convolution with stride 2 and 128 output planes, output resolution 48x48. \n\n- 2 residual blocks with 128 planes \n\n- 1 convolution with stride 2 and 256 output planes, output resolution 24x24. \n\n- 3 residual blocks with 256 planes. \n\n- Average pooling with stride 2, output resolution 12x12. \n\n- 3 residual blocks with 256 planes. \n\n- Average pooling with stride 2, output resolution 6x6. \n\nThe kernel size is 3x3 for all operations. For the dynamics function (which always operates at the downsampled resolution of 6x6), the action is first encoded as an image, then stacked with the hidden state of the previous step along the plane dimension. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Training",
      "content": "During training, the MuZero network is unrolled for K hypothetical steps and aligned to sequences sampled from the trajectories generated by the MCTS actors. Sequences are selected by sampling a state from any game in the replay buffer, then unrolling for K steps from that state. In Atari, samples are drawn according to prioritized replay <cit. Schaul2016>, with priority P(i) = p_i^α/∑ _kp_k^α, where p_i = |ν _i - z_i|, ν is the search value and z the observed n-step return. To correct for sampling bias introduced by the prioritized sampling, we scale the loss using the importance sampling ratio w_i = (1/N·1/P(i))^β. In all our experiments, we set α = β = 1. For board games, states are sampled uniformly. Each observation o_t along the sequence also has a corresponding MCTS policy π _t, estimated value ν _t and environment reward u_t. At each unrolled step k the network has a loss to the value, policy and reward target for that step, summed to produce the total loss for the MuZero network (see Equation <ref. muzero_eqn>). Note that, in board games without intermediate rewards, we omit the reward prediction loss. For board games, we bootstrap directly to the end of the game, equivalent to predicting the final outcome; for Atari we bootstrap for n=10 steps into the future. To maintain roughly similar magnitude of gradient across different unroll steps, we scale the gradient in two separate locations: \n - We scale the loss of each head by 1/K, where K is the number of unroll steps. This ensures that the total gradient has similar magnitude irrespective of how many steps we unroll for. \n\n- We also scale the gradient at the start of the dynamics function by 1/2. This ensures that the total gradient applied to the dynamics function stays constant. \n\nIn the experiments reported in this paper, we always unroll for K=5 steps. For a detailed illustration, see Figure <ref. fig:recurrent_search>. To improve the learning process and bound the activations, we also scale the hidden state to the same range as the action input ([0, 1]): s_scaled = s - min (s)/max (s) - min (s). All experiments were run using third generation Google Cloud TPUs <cit. tpuv3>. For each board game, we used 16 TPUs for training and 1000 TPUs for selfplay. For each game in Atari, we used 8 TPUs for training and 32 TPUs for selfplay. The much smaller proportion of TPUs used for acting in Atari is due to the smaller number of simulations per move (50 instead of 800) and the smaller size of the dynamics function compared to the representation function. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Reanalyze",
      "content": " <label. reanalyze> To improve the sample efficiency of MuZero we introduced a second variant of the algorithm, MuZero Reanalyze. MuZero Reanalyze revisits its past time-steps and re-executes its search using the latest model parameters, potentially resulting in a better quality policy than the original search. This fresh policy is used as the policy target for 80%  of updates during MuZero training. Furthermore, a target network <cit. dqn> · , v^- = f_θ ^-(s^0), based on recent parameters θ ^-, is used to provide a fresher, stable n-step bootstrapped target for the value function, z_t = u_t+1 + γ u_t+2 + ... + γ ^n-1 u_t+n + γ ^n v^-_t+n. In addition, several other hyperparameters were adjusted – primarily to increase sample reuse and avoid overfitting of the value function. Specifically, 2.0 samples were drawn per state, instead of 0.1; the value target was weighted down to 0.25 compared to weights of 1.0 for policy and reward targets; and the n-step return was reduced to n=5 steps instead of n=10 steps. ",
      "subsections": [],
      "figures": {}
    },
    {
      "title": "Evaluation",
      "content": "We evaluated the relative strength of MuZero (Figure <ref. fig:results>) in board games by measuring the Elo rating of each player. We estimate the probability that player a will defeat player b by a logistic function p(a  defeats  b) = (1 + 10^(c_elo (e(b) - e(a))))^-1, and estimate the ratings e(· ) by Bayesian logistic regression, computed by the BayesElo program <cit. coulom:bayeselo> using the standard constant c_elo = 1/400. Elo ratings were computed from the results of a 800 simulations per move tournament between iterations of MuZero during training, and also a baseline player: either Stockfish, Elmo or AlphaZero respectively. Baseline players used an equivalent search time of 100ms per move. The Elo rating of the baseline players was anchored to publicly available values  <cit. Silver18AZ>. In Atari, we computed mean reward over 1000 episodes per game, limited to the standard 30 minutes or 108,000 frames per episode <cit. gorila>, using 50 simulations per move unless indicated otherwise. In order to mitigate the effects of the deterministic nature of the Atari simulator we employed two different evaluation strategies: 30 noop random starts and human starts. For the former, at the beginning of each episode a random number of between 0 and 30 noop actions are applied to the simulator before handing control to the agent. For the latter, start positions are sampled from human expert play to initialize the Atari simulator before handing the control to the agent <cit. gorila>.  <figure. fig:atari_repeatability -  <label. fig:atari_repeatability> Repeatability of MuZero in Atari for five games. Total reward is shown on the y-axis, millions of training steps on the x-axis. Dark line indicates median score across 10 separate training runs, light lines indicate individual training runs, and the shaded region indicates 25th to 75th percentile. > \n    Model\n       . [                   s^0 = h_θ (o_1, ..., o_t);              r^k, s^k    = g_θ (s^k-1, a^k);              𝐩^k, v^k           = f_θ (s^k) ]}  𝐩^k, v^k, r^k = μ _θ (o_1, ..., o_t, a^1, ..., a^k)\n\n\n\n    Search\n    ν _t, π _t     = MCTS(s^0_t, μ _θ ) \n     a_t    ∼π _t \n       Learning Rule\n    𝐩^k_t, v^k_t, r^k_t     = μ _θ (o_1, ..., o_t, a_t+1, ..., a_t+k) \n     z_t     = {[                                                u_T                                          for games; u_t+1 + γ u_t+2 + ... + γ ^n-1 u_t+n + γ ^n ν _t+n                                   for general MDPs ]. \n     l_t(θ )     = ∑ _k=0^K l^r (u_t+k, r_t^k) + l^v(z_t+k, v^k_t) + l^p(π _t+k, p^k_t) + c ||θ ||^2 \n       Losses\n     l^r(u, r)     = {[                0        for games;      ϕ(u)^T log𝐫 for general MDPs ]. \n     l^v(z, q)     = {[        (z - q)^2        for games;      ϕ(z)^T log𝐪 for general MDPs ]. \n     l^p(π , p)     = π^T log𝐩\n\n\n <label. fig:muzero_equations> Equations summarising the MuZero algorithm. Here, ϕ (x) refers to the representation of a real number x through a linear combination of its adjacent integers, as described in the Network Architecture section. \n\n<figure. fig:extended_ablations -  <label. fig:extended_ablations> Details of MuZero evaluations (A-B) and policy improvement ablations (C-D). (A-B) Distribution of evaluation depth in the search tree for the learned model for the evaluations in Figure <ref. fig:ablations>A-B. The network was trained over 5 hypothetical steps, as indicated by the red line. Dark blue line indicates median depth from the root, dark shaded region shows 25th to 75th percentile, light shaded region shows 5th to 95th percentile. (C) Policy improvement in Ms. Pacman - a single network was trained at 50 simulations per search and is evaluated at different numbers of simulations per search, including playing according to the argmax of the raw policy network. The policy improvement effect of the search over the raw policy network is clearly visible throughout training. This consistent gap between the performance with and without search highlights the policy improvement that MuZero exploits, by continually updating towards the improved policy, to efficiently progress towards the optimal policy. (D) Policy improvement in Go - a single network was trained at 800 simulations per search and is evaluated at different numbers of simulations per search. In Go, the playing strength improvement from longer searches is much larger than in Ms.  Pacman and persists throughout training, consistent with previous results in <cit. Silver17AG0>. This suggests, as might intuitively be expected, that the benefit of models is greatest in precision planning domains. >\n\n \n  Game \n\n     Random \n\n     Human \n\n     SimPLe <cit. kaiser:simple> \n\n     Ape-X <cit. apex> \n\n     R2D2 <cit. r2d2> \n\n     MuZero \n\n     MuZero normalized \n\n\n alien \n\n    227.75 \n\n     7,127.80 \n\n     616.90 \n\n     40,805.00 \n\n     229,496.90 \n\n     741,812.63 \n\n     10,747.5 % \n\n\n amidar \n\n    5.77 \n\n     1,719.53 \n\n     74.30 \n\n     8,659.00 \n\n     29,321.40 \n\n     28,634.39 \n\n     1,670.5 % \n\n\n assault \n\n    222.39 \n\n     742.00 \n\n     527.20 \n\n     24,559.00 \n\n     108,197.00 \n\n     143,972.03 \n\n     27,664.9 % \n\n\n asterix \n\n    210.00 \n\n     8,503.33 \n\n     1,128.30 \n\n     313,305.00 \n\n     999,153.30 \n\n     998,425.00 \n\n     12,036.4 % \n\n\n asteroids \n\n    719.10 \n\n     47,388.67 \n\n     793.60 \n\n     155,495.00 \n\n     357,867.70 \n\n     678,558.64 \n\n     1,452.4 % \n\n\n atlantis \n\n    12,850.00 \n\n     29,028.13 \n\n     20,992.50 \n\n     944,498.00 \n\n     1,620,764.00 \n\n     1,674,767.20 \n\n     10,272.6 % \n\n\n bank heist \n\n    14.20 \n\n     753.13 \n\n     34.20 \n\n     1,716.00 \n\n     24,235.90 \n\n     1,278.98 \n\n     171.2 % \n\n\n battle zone \n\n    2,360.00 \n\n     37,187.50 \n\n     4,031.20 \n\n     98,895.00 \n\n     751,880.00 \n\n     848,623.00 \n\n     2,429.9 % \n\n\n beam rider \n\n    363.88 \n\n     16,926.53 \n\n     621.60 \n\n     63,305.00 \n\n     188,257.40 \n\n     454,993.53 \n\n     2,744.9 % \n\n\n berzerk \n\n    123.65 \n\n     2,630.42 \n\n     - \n\n     57,197.00 \n\n     53,318.70 \n\n     85,932.60 \n\n     3,423.1 % \n\n\n bowling \n\n    23.11 \n\n     160.73 \n\n     30.00 \n\n     18.00 \n\n     219.50 \n\n     260.13 \n\n     172.2 % \n\n\n boxing \n\n    0.05 \n\n     12.06 \n\n     7.80 \n\n     100.00 \n\n     98.50 \n\n     100.00 \n\n     832.2 % \n\n\n breakout \n\n    1.72 \n\n     30.47 \n\n     16.40 \n\n     801.00 \n\n     837.70 \n\n     864.00 \n\n     2,999.2 % \n\n\n centipede \n\n    2,090.87 \n\n     12,017.04 \n\n     - \n\n     12,974.00 \n\n     599,140.30 \n\n     1,159,049.27 \n\n     11,655.6 % \n\n\n chopper command \n\n    811.00 \n\n     7,387.80 \n\n     979.40 \n\n     721,851.00 \n\n     986,652.00 \n\n     991,039.70 \n\n     15,056.4 % \n\n\n crazy climber \n\n    10,780.50 \n\n     35,829.41 \n\n     62,583.60 \n\n     320,426.00 \n\n     366,690.70 \n\n     458,315.40 \n\n     1,786.6 % \n\n\n defender \n\n    2,874.50 \n\n     18,688.89 \n\n     - \n\n     411,944.00 \n\n     665,792.00 \n\n     839,642.95 \n\n     5,291.2 % \n\n\n demon attack \n\n    152.07 \n\n     1,971.00 \n\n     208.10 \n\n     133,086.00 \n\n     140,002.30 \n\n     143,964.26 \n\n     7,906.4 % \n\n\n double dunk \n\n    -18.55 \n\n     -16.40 \n\n     - \n\n     24.00 \n\n     23.70 \n\n     23.94 \n\n     1,976.3 % \n\n\n enduro \n\n    0.00 \n\n     860.53 \n\n     - \n\n     2,177.00 \n\n     2,372.70 \n\n     2,382.44 \n\n     276.9 % \n\n\n fishing derby \n\n    -91.71 \n\n     -38.80 \n\n     -90.70 \n\n     44.00 \n\n     85.80 \n\n     91.16 \n\n     345.6 % \n\n\n freeway \n\n    0.01 \n\n     29.60 \n\n     16.70 \n\n     34.00 \n\n     32.50 \n\n     33.03 \n\n     111.6 % \n\n\n frostbite \n\n    65.20 \n\n     4,334.67 \n\n     236.90 \n\n     9,329.00 \n\n     315,456.40 \n\n     631,378.53 \n\n     14,786.7 % \n\n\n gopher \n\n    257.60 \n\n     2,412.50 \n\n     596.80 \n\n     120,501.00 \n\n     124,776.30 \n\n     130,345.58 \n\n     6,036.8 % \n\n\n gravitar \n\n    173.00 \n\n     3,351.43 \n\n     173.40 \n\n     1,599.00 \n\n     15,680.70 \n\n     6,682.70 \n\n     204.8 % \n\n\n hero \n\n    1,026.97 \n\n     30,826.38 \n\n     2,656.60 \n\n     31,656.00 \n\n     39,537.10 \n\n     49,244.11 \n\n     161.8 % \n\n\n ice hockey \n\n    -11.15 \n\n     0.88 \n\n     -11.60 \n\n     33.00 \n\n     79.30 \n\n     67.04 \n\n     650.0 % \n\n\n jamesbond \n\n    29.00 \n\n     302.80 \n\n     100.50 \n\n     21,323.00 \n\n     25,354.00 \n\n     41,063.25 \n\n     14,986.9 % \n\n\n kangaroo \n\n    52.00 \n\n     3,035.00 \n\n     51.20 \n\n     1,416.00 \n\n     14,130.70 \n\n     16,763.60 \n\n     560.2 % \n\n\n krull \n\n    1,598.05 \n\n     2,665.53 \n\n     2,204.80 \n\n     11,741.00 \n\n     218,448.10 \n\n     269,358.27 \n\n     25,083.4 % \n\n\n kung fu master \n\n    258.50 \n\n     22,736.25 \n\n     14,862.50 \n\n     97,830.00 \n\n     233,413.30 \n\n     204,824.00 \n\n     910.1 % \n\n\n montezuma revenge \n\n    0.00 \n\n     4,753.33 \n\n     - \n\n     2,500.00 \n\n     2,061.30 \n\n     0.00 \n\n     0.0 % \n\n\n ms pacman \n\n    307.30 \n\n     6,951.60 \n\n     1,480.00 \n\n     11,255.00 \n\n     42,281.70 \n\n     243,401.10 \n\n     3,658.7 % \n\n\n name this game \n\n    2,292.35 \n\n     8,049.00 \n\n     2,420.70 \n\n     25,783.00 \n\n     58,182.70 \n\n     157,177.85 \n\n     2,690.5 % \n\n\n phoenix \n\n    761.40 \n\n     7,242.60 \n\n     - \n\n     224,491.00 \n\n     864,020.00 \n\n     955,137.84 \n\n     14,725.3 % \n\n\n pitfall \n\n    -229.44 \n\n     6,463.69 \n\n     - \n\n     -1.00 \n\n     0.00 \n\n     0.00 \n\n     3.4 % \n\n\n pong \n\n    -20.71 \n\n     14.59 \n\n     12.80 \n\n     21.00 \n\n     21.00 \n\n     21.00 \n\n     118.2 % \n\n\n private eye \n\n    24.94 \n\n     69,571.27 \n\n     35.00 \n\n     50.00 \n\n     5,322.70 \n\n     15,299.98 \n\n     22.0 % \n\n\n qbert \n\n    163.88 \n\n     13,455.00 \n\n     1,288.80 \n\n     302,391.00 \n\n     408,850.00 \n\n     72,276.00 \n\n     542.6 % \n\n\n riverraid \n\n    1,338.50 \n\n     17,118.00 \n\n     1,957.80 \n\n     63,864.00 \n\n     45,632.10 \n\n     323,417.18 \n\n     2,041.1 % \n\n\n road runner \n\n    11.50 \n\n     7,845.00 \n\n     5,640.60 \n\n     222,235.00 \n\n     599,246.70 \n\n     613,411.80 \n\n     7,830.5 % \n\n\n robotank \n\n    2.16 \n\n     11.94 \n\n     - \n\n     74.00 \n\n     100.40 \n\n     131.13 \n\n     1,318.7 % \n\n\n seaquest \n\n    68.40 \n\n     42,054.71 \n\n     683.30 \n\n     392,952.00 \n\n     999,996.70 \n\n     999,976.52 \n\n     2,381.5 % \n\n\n skiing \n\n    -17,098.09 \n\n     -4,336.93 \n\n     - \n\n     -10,790.00 \n\n     -30,021.70 \n\n     -29,968.36 \n\n     -100.9 % \n\n\n solaris \n\n    1,236.30 \n\n     12,326.67 \n\n     - \n\n     2,893.00 \n\n     3,787.20 \n\n     56.62 \n\n     -10.6 % \n\n\n space invaders \n\n    148.03 \n\n     1,668.67 \n\n     - \n\n     54,681.00 \n\n     43,223.40 \n\n     74,335.30 \n\n     4,878.7 % \n\n\n star gunner \n\n    664.00 \n\n     10,250.00 \n\n     - \n\n     434,343.00 \n\n     717,344.00 \n\n     549,271.70 \n\n     5,723.0 % \n\n\n surround \n\n    -9.99 \n\n     6.53 \n\n     - \n\n     7.00 \n\n     9.90 \n\n     9.99 \n\n     120.9 % \n\n\n tennis \n\n    -23.84 \n\n     -8.27 \n\n     - \n\n     24.00 \n\n     -0.10 \n\n     0.00 \n\n     153.1 % \n\n\n time pilot \n\n    3,568.00 \n\n     5,229.10 \n\n     - \n\n     87,085.00 \n\n     445,377.30 \n\n     476,763.90 \n\n     28,486.9 % \n\n\n tutankham \n\n    11.43 \n\n     167.59 \n\n     - \n\n     273.00 \n\n     395.30 \n\n     491.48 \n\n     307.4 % \n\n\n up n down \n\n    533.40 \n\n     11,693.23 \n\n     3,350.30 \n\n     401,884.00 \n\n     589,226.90 \n\n     715,545.61 \n\n     6,407.0 % \n\n\n venture \n\n    0.00 \n\n     1,187.50 \n\n     - \n\n     1,813.00 \n\n     1,970.70 \n\n     0.40 \n\n     0.0 % \n\n\n video pinball \n\n    0.00 \n\n     17,667.90 \n\n     - \n\n     565,163.00 \n\n     999,383.20 \n\n     981,791.88 \n\n     5,556.9 % \n\n\n wizard of wor \n\n    563.50 \n\n     4,756.52 \n\n     - \n\n     46,204.00 \n\n     144,362.70 \n\n     197,126.00 \n\n     4,687.9 % \n\n\n yars revenge \n\n    3,092.91 \n\n     54,576.93 \n\n     5,664.30 \n\n     148,595.00 \n\n     995,048.40 \n\n     553,311.46 \n\n     1,068.7 % \n\n\n zaxxon \n\n    32.50 \n\n     9,173.30 \n\n     - \n\n     42,286.00 \n\n     224,910.70 \n\n     725,853.90 \n\n     7,940.5 % \n\n\n #  best \n\n     0 \n\n     5 \n\n     0 \n\n     5 \n\n     13 \n\n     37\n\n\n  \n\n\n <label. tab:atari-results-at30> Evaluation of MuZero in Atari for individual games with 30 random no-op starts. Best result for each game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames). SimPLe was only evaluated on 36 of the 57 games, unavailable results are indicated with `-'. Human normalized score is calculated as s_normalized = s_agent - s_random/s_human - s_random. \n\n\n\n\n\n \n  Game \n\n     Random \n\n     Human \n\n     Ape-X <cit. apex> \n\n     MuZero \n\n     MuZero normalized \n\n\n alien \n\n    128.30 \n\n     6,371.30 \n\n     17,732.00 \n\n     713,387.37 \n\n     11,424.9 % \n\n\n amidar \n\n    11.79 \n\n     1,540.43 \n\n     1,047.00 \n\n     26,638.80 \n\n     1,741.9 % \n\n\n assault \n\n    166.95 \n\n     628.89 \n\n     24,405.00 \n\n     143,900.58 \n\n     31,115.2 % \n\n\n asterix \n\n    164.50 \n\n     7,536.00 \n\n     283,180.00 \n\n     985,801.95 \n\n     13,370.9 % \n\n\n asteroids \n\n    877.10 \n\n     36,517.30 \n\n     117,303.00 \n\n     606,971.12 \n\n     1,700.6 % \n\n\n atlantis \n\n    13,463.00 \n\n     26,575.00 \n\n     918,715.00 \n\n     1,653,202.50 \n\n     12,505.6 % \n\n\n bank heist \n\n    21.70 \n\n     644.50 \n\n     1,201.00 \n\n     962.11 \n\n     151.0 % \n\n\n battle zone \n\n    3,560.00 \n\n     33,030.00 \n\n     92,275.00 \n\n     791,387.00 \n\n     2,673.3 % \n\n\n beam rider \n\n    254.56 \n\n     14,961.02 \n\n     72,234.00 \n\n     419,460.76 \n\n     2,850.5 % \n\n\n berzerk \n\n    196.10 \n\n     2,237.50 \n\n     55,599.00 \n\n     87,308.60 \n\n     4,267.3 % \n\n\n bowling \n\n    35.16 \n\n     146.46 \n\n     30.00 \n\n     194.03 \n\n     142.7 % \n\n\n boxing \n\n    -1.46 \n\n     9.61 \n\n     81.00 \n\n     56.60 \n\n     524.5 % \n\n\n breakout \n\n    1.77 \n\n     27.86 \n\n     757.00 \n\n     849.59 \n\n     3,249.6 % \n\n\n centipede \n\n    1,925.45 \n\n     10,321.89 \n\n     5,712.00 \n\n     1,138,294.60 \n\n     13,533.9 % \n\n\n chopper command \n\n    644.00 \n\n     8,930.00 \n\n     576,602.00 \n\n     932,370.10 \n\n     11,244.6 % \n\n\n crazy climber \n\n    9,337.00 \n\n     32,667.00 \n\n     263,954.00 \n\n     412,213.90 \n\n     1,726.9 % \n\n\n defender \n\n    1,965.50 \n\n     14,296.00 \n\n     399,865.00 \n\n     823,636.00 \n\n     6,663.7 % \n\n\n demon attack \n\n    208.25 \n\n     3,442.85 \n\n     133,002.00 \n\n     143,858.05 \n\n     4,441.0 % \n\n\n double dunk \n\n    -15.97 \n\n     -14.37 \n\n     22.00 \n\n     23.12 \n\n     2,443.1 % \n\n\n enduro \n\n    -81.84 \n\n     740.17 \n\n     2,042.00 \n\n     2,264.20 \n\n     285.4 % \n\n\n fishing derby \n\n    -77.11 \n\n     5.09 \n\n     22.00 \n\n     57.45 \n\n     163.7 % \n\n\n freeway \n\n    0.17 \n\n     25.61 \n\n     29.00 \n\n     28.38 \n\n     110.9 % \n\n\n frostbite \n\n    90.80 \n\n     4,202.80 \n\n     6,512.00 \n\n     613,944.04 \n\n     14,928.3 % \n\n\n gopher \n\n    250.00 \n\n     2,311.00 \n\n     121,168.00 \n\n     129,218.68 \n\n     6,257.6 % \n\n\n gravitar \n\n    245.50 \n\n     3,116.00 \n\n     662.00 \n\n     3,390.65 \n\n     109.6 % \n\n\n hero \n\n    1,580.30 \n\n     25,839.40 \n\n     26,345.00 \n\n     44,129.55 \n\n     175.4 % \n\n\n ice hockey \n\n    -9.67 \n\n     0.53 \n\n     24.00 \n\n     52.40 \n\n     608.5 % \n\n\n jamesbond \n\n    33.50 \n\n     368.50 \n\n     18,992.00 \n\n     39,107.20 \n\n     11,663.8 % \n\n\n kangaroo \n\n    100.00 \n\n     2,739.00 \n\n     578.00 \n\n     13,210.50 \n\n     496.8 % \n\n\n krull \n\n    1,151.90 \n\n     2,109.10 \n\n     8,592.00 \n\n     257,706.70 \n\n     26,802.6 % \n\n\n kung fu master \n\n    304.00 \n\n     20,786.80 \n\n     72,068.00 \n\n     174,623.60 \n\n     851.1 % \n\n\n montezuma revenge \n\n    25.00 \n\n     4,182.00 \n\n     1,079.00 \n\n     57.10 \n\n     0.8 % \n\n\n ms pacman \n\n    197.80 \n\n     15,375.05 \n\n     6,135.00 \n\n     230,650.24 \n\n     1,518.4 % \n\n\n name this game \n\n    1,747.80 \n\n     6,796.00 \n\n     23,830.00 \n\n     152,723.62 \n\n     2,990.7 % \n\n\n phoenix \n\n    1,134.40 \n\n     6,686.20 \n\n     188,789.00 \n\n     943,255.07 \n\n     16,969.6 % \n\n\n pitfall \n\n    -348.80 \n\n     5,998.91 \n\n     -273.00 \n\n     -801.10 \n\n     -7.1 % \n\n\n pong \n\n    -17.95 \n\n     15.46 \n\n     19.00 \n\n     19.20 \n\n     111.2 % \n\n\n private eye \n\n    662.78 \n\n     64,169.07 \n\n     865.00 \n\n     5,067.59 \n\n     6.9 % \n\n\n qbert \n\n    159.38 \n\n     12,085.00 \n\n     380,152.00 \n\n     39,302.10 \n\n     328.2 % \n\n\n riverraid \n\n    588.30 \n\n     14,382.20 \n\n     49,983.00 \n\n     315,353.33 \n\n     2,281.9 % \n\n\n road runner \n\n    200.00 \n\n     6,878.00 \n\n     127,112.00 \n\n     580,445.00 \n\n     8,688.9 % \n\n\n robotank \n\n    2.42 \n\n     8.94 \n\n     69.00 \n\n     128.80 \n\n     1,938.3 % \n\n\n seaquest \n\n    215.50 \n\n     40,425.80 \n\n     377,180.00 \n\n     997,601.01 \n\n     2,480.4 % \n\n\n skiing \n\n    -15,287.35 \n\n     -3,686.58 \n\n     -11,359.00 \n\n     -29,400.75 \n\n     -121.7 % \n\n\n solaris \n\n    2,047.20 \n\n     11,032.60 \n\n     3,116.00 \n\n     2,108.08 \n\n     0.7 % \n\n\n space invaders \n\n    182.55 \n\n     1,464.90 \n\n     50,699.00 \n\n     57,450.41 \n\n     4,465.9 % \n\n\n star gunner \n\n    697.00 \n\n     9,528.00 \n\n     432,958.00 \n\n     539,342.70 \n\n     6,099.5 % \n\n\n surround \n\n    -9.72 \n\n     5.37 \n\n     6.00 \n\n     8.46 \n\n     120.5 % \n\n\n tennis \n\n    -21.43 \n\n     -6.69 \n\n     23.00 \n\n     -2.30 \n\n     129.8 % \n\n\n time pilot \n\n    3,273.00 \n\n     5,650.00 \n\n     71,543.00 \n\n     405,829.30 \n\n     16,935.5 % \n\n\n tutankham \n\n    12.74 \n\n     138.30 \n\n     128.00 \n\n     351.76 \n\n     270.0 % \n\n\n up n down \n\n    707.20 \n\n     9,896.10 \n\n     347,912.00 \n\n     607,807.85 \n\n     6,606.9 % \n\n\n venture \n\n    18.00 \n\n     1,039.00 \n\n     936.00 \n\n     21.10 \n\n     0.3 % \n\n\n video pinball \n\n    0.00 \n\n     15,641.09 \n\n     873,989.00 \n\n     970,881.10 \n\n     6,207.2 % \n\n\n wizard of wor \n\n    804.00 \n\n     4,556.00 \n\n     46,897.00 \n\n     196,279.20 \n\n     5,209.9 % \n\n\n yars revenge \n\n    1,476.88 \n\n     47,135.17 \n\n     131,701.00 \n\n     888,633.84 \n\n     1,943.0 % \n\n\n zaxxon \n\n    475.00 \n\n     8,443.00 \n\n     37,672.00 \n\n     592,238.70 \n\n     7,426.8 % \n\n\n #  best \n\n     0 \n\n     6 \n\n     5 \n\n     46\n\n\n  \n\n\n <label. tab:atari-results-at30-rnd-starts> Evaluation of MuZero in Atari for individual games from human start positions. Best result for each game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames). \n\n\n\n<figure. fig:learning_curve_atari -  <label. fig:learning_curve_atari> Learning curves of MuZero in Atari for individual games. Total reward is shown on the y-axis, millions of training steps on the x-axis. Line indicates mean score across 1000 evaluation games, shaded region indicates standard deviation. >",
      "subsections": [],
      "figures": {
        "fig:atari_repeatability": {
          "label": "fig:atari_repeatability",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/atari_repeatability.png"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": null,
              "width": 607
            }
          ],
          "caption": " <label. fig:atari_repeatability> Repeatability of MuZero in Atari for five games. Total reward is shown on the y-axis, millions of training steps on the x-axis. Dark line indicates median score across 10 separate training runs, light lines indicate individual training runs, and the shaded region indicates 25th to 75th percentile. ",
          "section": "16. Evaluation"
        },
        "fig:extended_ablations": {
          "label": "fig:extended_ablations",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/go_scaling_depth.png",
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/atari_scaling_depth.png",
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/pacman_ablations_suppl.png",
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/go_policy_improvement.png"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": null,
              "width": 303
            },
            {
              "scale": 1.0,
              "height": null,
              "width": 303
            },
            {
              "scale": 1.0,
              "height": null,
              "width": 303
            },
            {
              "scale": 1.0,
              "height": null,
              "width": 303
            }
          ],
          "caption": " <label. fig:extended_ablations> Details of MuZero evaluations (A-B) and policy improvement ablations (C-D). (A-B) Distribution of evaluation depth in the search tree for the learned model for the evaluations in Figure <ref. fig:ablations>A-B. The network was trained over 5 hypothetical steps, as indicated by the red line. Dark blue line indicates median depth from the root, dark shaded region shows 25th to 75th percentile, light shaded region shows 5th to 95th percentile. (C) Policy improvement in Ms. Pacman - a single network was trained at 50 simulations per search and is evaluated at different numbers of simulations per search, including playing according to the argmax of the raw policy network. The policy improvement effect of the search over the raw policy network is clearly visible throughout training. This consistent gap between the performance with and without search highlights the policy improvement that MuZero exploits, by continually updating towards the improved policy, to efficiently progress towards the optimal policy. (D) Policy improvement in Go - a single network was trained at 800 simulations per search and is evaluated at different numbers of simulations per search. In Go, the playing strength improvement from longer searches is much larger than in Ms.  Pacman and persists throughout training, consistent with previous results in <cit. Silver17AG0>. This suggests, as might intuitively be expected, that the benefit of models is greatest in precision planning domains. ",
          "section": "16. Evaluation"
        },
        "fig:learning_curve_atari": {
          "label": "fig:learning_curve_atari",
          "path": [
            "/home/sara/Documents/semanticXplorer/xplorer-plugin/test/data/muzero/source/generated/learning_curve_at30.png"
          ],
          "size": [
            {
              "scale": 1.0,
              "height": null,
              "width": 577
            }
          ],
          "caption": " <label. fig:learning_curve_atari> Learning curves of MuZero in Atari for individual games. Total reward is shown on the y-axis, millions of training steps on the x-axis. Line indicates mean score across 1000 evaluation games, shaded region indicates standard deviation. ",
          "section": "16. Evaluation"
        }
      }
    }
  ],
  "figures": {}
}